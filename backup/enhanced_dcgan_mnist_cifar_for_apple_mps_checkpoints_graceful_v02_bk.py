#!/usr/bin/env python3
"""
Enhanced Multi-Dataset DCGAN with Apple Metal GPU Support
=========================================================

This version adds Apple Silicon (M1/M2/M3) GPU support via Metal Performance Shaders (MPS)
and includes fallback options for optimal performance across different hardware.

Key Features:
- Apple Metal GPU acceleration (MPS) for M1/M2/M3 Macs
- CUDA support for NVIDIA GPUs
- CPU fallback for compatibility
- Automatic device detection and optimization
- Memory management for Apple Silicon
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import ssl
import os
import time
import sys
import platform
from collections import deque
import threading
from matplotlib.animation import FuncAnimation

# Add these imports at the top of your script (after the existing imports)
import glob
import time
from pathlib import Path

import signal
import atexit
import sys
import traceback


# Add this near the top with other imports
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("‚ö†Ô∏è  psutil not available - CPU memory monitoring disabled")

# Add these imports at the top of your script (after existing imports)
class GracefulCheckpointManager:
    """Manages graceful checkpoint saving during interrupts and errors"""

    def __init__(self):
        self.checkpoint_data = None
        self.dataset_key = None
        self.current_epoch = 0
        self.models = {}
        self.optimizers = {}
        self.schedulers = {}
        self.ema_generator = None
        self.training_stats = {}
        self.emergency_save_enabled = False

        # Register signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        # Register exit handler
        atexit.register(self._cleanup)

    def register_training_components(self, dataset_key, generator, critic,
                                     optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                                     ema_generator):
        """Register all training components for emergency saving"""
        self.dataset_key = dataset_key
        self.models = {
            'generator': generator,
            'critic': critic
        }
        self.optimizers = {
            'optimizer_G': optimizer_G,
            'optimizer_D': optimizer_D
        }
        self.schedulers = {
            'scheduler_G': scheduler_G,
            'scheduler_D': scheduler_D
        }
        self.ema_generator = ema_generator
        self.emergency_save_enabled = True

        print("üõ°Ô∏è  Emergency checkpoint system activated")

    def update_current_state(self, epoch, training_stats):
        """Update current training state for emergency saves"""
        self.current_epoch = epoch
        self.training_stats = training_stats

    def _signal_handler(self, signum, frame):
        """Handle interrupt signals gracefully"""
        signal_names = {
            signal.SIGINT: "SIGINT (Ctrl+C)",
            signal.SIGTERM: "SIGTERM"
        }
        signal_name = signal_names.get(signum, f"Signal {signum}")

        print(f"\n\nüö® {signal_name} received - Initiating graceful shutdown...")
        print("=" * 80)

        if self.emergency_save_enabled:
            try:
                self._save_emergency_checkpoint("interrupt")
                print("‚úÖ Emergency checkpoint saved successfully!")
            except Exception as e:
                print(f"‚ùå Failed to save emergency checkpoint: {e}")

        print("üëã Graceful shutdown completed. Goodbye!")
        print("=" * 80)
        sys.exit(0)

    def _cleanup(self):
        """Cleanup function called on program exit"""
        if self.emergency_save_enabled and hasattr(self, '_abnormal_exit'):
            print("\nüö® Abnormal program termination detected")
            try:
                self._save_emergency_checkpoint("abnormal_exit")
                print("‚úÖ Emergency checkpoint saved on exit!")
            except:
                print("‚ùå Failed to save emergency checkpoint on exit")

    def _save_emergency_checkpoint(self, reason):
        """Enhanced emergency checkpoint saving with detailed progress and error handling - FIXED for DataLoader interrupts"""
        if not self.emergency_save_enabled or not self.dataset_key:
            print(f"   ‚ö†Ô∏è  Emergency save skipped: emergency_save_enabled={self.emergency_save_enabled}, dataset_key={self.dataset_key}")
            return None

        print(f"üíæ Saving emergency checkpoint (reason: {reason})...")

        # Step 1: Create emergency checkpoint directory
        print(f"   üìÅ Creating emergency directory...")
        emergency_dir = f'./models/{self.dataset_key}/emergency'
        try:
            os.makedirs(emergency_dir, exist_ok=True)
            print(f"   ‚úÖ Emergency directory ready: {emergency_dir}")
        except Exception as e:
            print(f"   ‚ùå Failed to create emergency directory: {e}")
            return None

        # Step 2: Generate filename with timestamp
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = f'{self.dataset_key}_emergency_{reason}_epoch_{self.current_epoch}_{timestamp}.pth'
        filepath = os.path.join(emergency_dir, filename)
        print(f"   üìù Emergency file: {filename}")

        try:
            # Step 3: Prepare checkpoint data with progress indication
            print(f"   üì¶ Preparing checkpoint data...")

            # Validate components before saving
            print(f"   üîç Validating model components...")
            if not self.models or 'generator' not in self.models or 'critic' not in self.models:
                print(f"   ‚ùå Models not properly registered!")
                return None

            if not self.optimizers or 'optimizer_G' not in self.optimizers or 'optimizer_D' not in self.optimizers:
                print(f"   ‚ùå Optimizers not properly registered!")
                return None

            if not self.schedulers or 'scheduler_G' not in self.schedulers or 'scheduler_D' not in self.schedulers:
                print(f"   ‚ùå Schedulers not properly registered!")
                return None

            if not self.ema_generator:
                print(f"   ‚ùå EMA Generator not properly registered!")
                return None

            print(f"   ‚úÖ All components validated successfully")

            # Step 4: Build checkpoint data with error handling for each component
            checkpoint_data = {}

            # Save generator state
            print(f"   üíæ Saving generator state...")
            try:
                checkpoint_data['generator'] = self.models['generator'].state_dict()
                print(f"   ‚úÖ Generator state saved")
            except Exception as e:
                print(f"   ‚ùå Failed to save generator: {e}")
                return None

            # Save critic state
            print(f"   üíæ Saving critic state...")
            try:
                checkpoint_data['critic'] = self.models['critic'].state_dict()
                print(f"   ‚úÖ Critic state saved")
            except Exception as e:
                print(f"   ‚ùå Failed to save critic: {e}")
                return None

            # Save EMA parameters
            print(f"   üíæ Saving EMA parameters...")
            try:
                checkpoint_data['ema_params'] = self.ema_generator.ema.shadow_params if self.ema_generator else None
                print(f"   ‚úÖ EMA parameters saved")
            except Exception as e:
                print(f"   ‚ùå Failed to save EMA parameters: {e}")
                # Don't return None here, EMA is not critical for emergency save
                checkpoint_data['ema_params'] = None

            # Save optimizer states
            print(f"   üíæ Saving optimizer states...")
            try:
                checkpoint_data['optimizer_G'] = self.optimizers['optimizer_G'].state_dict()
                checkpoint_data['optimizer_D'] = self.optimizers['optimizer_D'].state_dict()
                print(f"   ‚úÖ Optimizer states saved")
            except Exception as e:
                print(f"   ‚ùå Failed to save optimizers: {e}")
                return None

            # Save scheduler states
            print(f"   üíæ Saving scheduler states...")
            try:
                checkpoint_data['scheduler_G'] = self.schedulers['scheduler_G'].state_dict()
                checkpoint_data['scheduler_D'] = self.schedulers['scheduler_D'].state_dict()
                print(f"   ‚úÖ Scheduler states saved")
            except Exception as e:
                print(f"   ‚ùå Failed to save schedulers: {e}")
                return None

            # Add metadata
            print(f"   üìã Adding metadata...")
            checkpoint_data.update({
                'epoch': self.current_epoch,
                'dataset_key': self.dataset_key,
                'training_stats': self.training_stats,
                'emergency_save': True,
                'save_reason': reason,
                'timestamp': timestamp,
                'python_version': sys.version,
                'pytorch_version': torch.__version__,
                'device_info': str(torch.cuda.get_device_name(0)) if torch.cuda.is_available() else 'CPU',
                'enhancements': ['WGAN-GP', 'EMA', 'Enhanced-Arch', 'Spectral-Norm', 'Resume', 'Emergency-Save']
            })
            print(f"   ‚úÖ Metadata added")

            # Step 5: Write to disk with progress indication - FIXED SIZE ESTIMATION
            print(f"   üíæ Writing checkpoint to disk...")

            # FIX: Skip size estimation during emergency saves to avoid DataLoader tensor string conversion issues
            print(f"   üìä Emergency save - skipping size estimation for faster save")

            # Save with error handling
            try:
                torch.save(checkpoint_data, filepath)

                # Verify the file was actually created and has reasonable size
                if os.path.exists(filepath):
                    actual_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
                    print(f"   ‚úÖ File written successfully: {actual_size:.1f} MB")

                    # Quick verification - try to load the file header
                    try:
                        test_load = torch.load(filepath, map_location='cpu', weights_only=False)
                        if 'generator' in test_load and 'critic' in test_load:
                            print(f"   ‚úÖ File integrity verified")
                        else:
                            print(f"   ‚ö†Ô∏è  File may be corrupted - missing core components")
                    except Exception as verify_error:
                        print(f"   ‚ö†Ô∏è  File verification failed: {verify_error}")

                else:
                    print(f"   ‚ùå File was not created!")
                    return None

            except Exception as save_error:
                print(f"   ‚ùå Failed to write file: {save_error}")

                # Try alternative save location
                try:
                    alt_filepath = f'./emergency_backup_{filename}'
                    print(f"   üîÑ Attempting alternative save location: {alt_filepath}")
                    torch.save(checkpoint_data, alt_filepath)
                    print(f"   ‚úÖ Emergency backup saved to: {alt_filepath}")
                    return alt_filepath
                except Exception as alt_error:
                    print(f"   ‚ùå Alternative save also failed: {alt_error}")
                    return None

            # Step 6: Final success message with summary
            print(f"   üéâ EMERGENCY CHECKPOINT SAVED SUCCESSFULLY!")
            print(f"   üìÅ Location: {filepath}")
            print(f"   üìÖ Epoch: {self.current_epoch}")
            print(f"   üïí Timestamp: {timestamp}")
            print(f"   üîç Reason: {reason}")

            # Display what was saved
            saved_components = []
            if 'generator' in checkpoint_data: saved_components.append("Generator")
            if 'critic' in checkpoint_data: saved_components.append("Critic")
            if 'ema_params' in checkpoint_data and checkpoint_data['ema_params']: saved_components.append("EMA")
            if 'optimizer_G' in checkpoint_data: saved_components.append("Optimizers")
            if 'scheduler_G' in checkpoint_data: saved_components.append("Schedulers")

            print(f"   üíæ Saved components: {', '.join(saved_components)}")

            return filepath

        except Exception as e:
            print(f"   ‚ùå EMERGENCY SAVE FAILED: {e}")
            print(f"   üìç Error occurred at: {traceback.format_exc()}")

            # Last resort - try to save minimal checkpoint with just model weights
            try:
                print(f"   üÜò Attempting minimal emergency save...")
                minimal_data = {
                    'generator': self.models['generator'].state_dict(),
                    'critic': self.models['critic'].state_dict(),
                    'epoch': self.current_epoch,
                    'dataset_key': self.dataset_key,
                    'emergency_save': True,
                    'save_reason': f"{reason}_minimal",
                    'timestamp': timestamp,
                    'minimal_save': True
                }

                minimal_filepath = os.path.join(emergency_dir, f'MINIMAL_{filename}')
                torch.save(minimal_data, minimal_filepath)
                print(f"   ‚úÖ Minimal emergency checkpoint saved: {minimal_filepath}")
                return minimal_filepath

            except Exception as minimal_error:
                print(f"   ‚ùå Even minimal save failed: {minimal_error}")
                print(f"   üíî Emergency checkpoint completely failed!")
                return None

    # USAGE: Replace the existing _save_emergency_checkpoint method in your GracefulCheckpointManager class
    # with this enhanced version.

    # Additional helper method you can add to the GracefulCheckpointManager class:
    def get_emergency_checkpoint_info(self, dataset_key):
        """Get information about available emergency checkpoints - FIXED for PyTorch 2.6"""
        emergency_dir = f'./models/{dataset_key}/emergency'

        if not os.path.exists(emergency_dir):
            return []

        emergency_files = []
        for file in os.listdir(emergency_dir):
            if file.endswith('.pth') and 'emergency' in file:
                filepath = os.path.join(emergency_dir, file)
                try:
                    # Load just the metadata - FIX for PyTorch 2.6
                    checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
                    info = {
                        'filename': file,
                        'filepath': filepath,
                        'epoch': checkpoint.get('epoch', 'Unknown'),
                        'reason': checkpoint.get('save_reason', 'Unknown'),
                        'timestamp': checkpoint.get('timestamp', 'Unknown'),
                        'size_mb': os.path.getsize(filepath) / (1024 * 1024),
                        'minimal_save': checkpoint.get('minimal_save', False)
                    }
                    emergency_files.append(info)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Could not read emergency file {file}: {e}")

        # Sort by timestamp (newest first)
        emergency_files.sort(key=lambda x: x['timestamp'], reverse=True)
        return emergency_files

    def display_emergency_checkpoints(self, dataset_key):
        """Display available emergency checkpoints"""
        emergency_files = self.get_emergency_checkpoint_info(dataset_key)

        if not emergency_files:
            print(f"üìÅ No emergency checkpoints found for {dataset_key}")
            return

        print(f"\nüö® EMERGENCY CHECKPOINTS FOR {dataset_key.upper()}:")
        print("=" * 80)

        for i, info in enumerate(emergency_files, 1):
            print(f"\n{i}. {info['filename']}")
            print(f"   üìÖ Epoch: {info['epoch']}")
            print(f"   üîç Reason: {info['reason']}")
            print(f"   üïí Timestamp: {info['timestamp']}")
            print(f"   üìä Size: {info['size_mb']:.1f} MB")
            if info['minimal_save']:
                print(f"   ‚ö†Ô∏è  Minimal save (models only)")
            else:
                print(f"   ‚úÖ Complete save (all components)")

        print("=" * 80)

# Initialize the global checkpoint manager
checkpoint_manager = GracefulCheckpointManager()

def save_checkpoint_enhanced(dataset_key, epoch, generator, critic, ema_generator,
                             optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                             training_stats, checkpoint_type="regular"):
    """Enhanced checkpoint saving function"""

    # Create models directory if it doesn't exist
    os.makedirs('./models', exist_ok=True)

    checkpoint_data = {
        'generator': generator.state_dict(),
        'critic': critic.state_dict(),
        'ema_params': ema_generator.ema.shadow_params,
        'optimizer_G': optimizer_G.state_dict(),
        'optimizer_D': optimizer_D.state_dict(),
        'scheduler_G': scheduler_G.state_dict(),
        'scheduler_D': scheduler_D.state_dict(),
        'epoch': epoch,
        'dataset_key': dataset_key,
        'training_stats': training_stats,
        'checkpoint_type': checkpoint_type,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'enhancements': ['WGAN-GP', 'EMA', 'Enhanced-Arch', 'Spectral-Norm', 'Resume', 'Auto-Save']
    }

    if checkpoint_type == "regular":
        filename = f'./models/{dataset_key}_enhanced_epoch_{epoch}.pth'
        print(f"üíæ Saving checkpoint for epoch {epoch}...")
    elif checkpoint_type == "best":
        filename = f'./models/{dataset_key}_best_enhanced_model.pth'
        print(f"üèÜ Saving best model checkpoint...")
    elif checkpoint_type == "final":
        os.makedirs(f'./models/{dataset_key}/enhanced', exist_ok=True)
        filename = f'./models/{dataset_key}/enhanced/final_enhanced_model.pth'
        print(f"üéØ Saving final model checkpoint...")

    try:
        torch.save(checkpoint_data, filename)
        print(f"   ‚úÖ Checkpoint saved: {os.path.basename(filename)}")

        # Update checkpoint manager with current state
        checkpoint_manager.update_current_state(epoch, training_stats)

        return filename
    except Exception as e:
        print(f"   ‚ùå Failed to save checkpoint: {e}")
        return None

def enhanced_error_handler(func):
    """Decorator to handle errors with emergency checkpoint saving"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except KeyboardInterrupt:
            # This should be handled by signal handler, but just in case
            print("\nüö® KeyboardInterrupt caught in error handler")
            checkpoint_manager._signal_handler(signal.SIGINT, None)
        except Exception as e:
            print(f"\nüö® UNEXPECTED ERROR OCCURRED:")
            print(f"   Error: {str(e)}")
            print(f"   Type: {type(e).__name__}")
            print("\nüìç Traceback:")
            traceback.print_exc()

            # Mark abnormal exit for cleanup
            checkpoint_manager._abnormal_exit = True

            if checkpoint_manager.emergency_save_enabled:
                print("\nüíæ Attempting emergency checkpoint save...")
                try:
                    checkpoint_manager._save_emergency_checkpoint("error")
                    print("‚úÖ Emergency checkpoint saved successfully!")
                except Exception as save_error:
                    print(f"‚ùå Emergency save failed: {save_error}")

            print("\nüö® Program will now exit due to error")
            sys.exit(1)

    return wrapper

# =============================================================================
# CHECKPOINT RESUME FUNCTIONALITY
# =============================================================================

def find_available_checkpoints(dataset_key):
    """Find all available checkpoints for a dataset"""
    checkpoint_patterns = [
        f'./models/{dataset_key}_enhanced_epoch_*.pth',
        f'./models/{dataset_key}_best_enhanced_model.pth',
        f'./models/{dataset_key}/enhanced/final_enhanced_model.pth'
    ]

    all_checkpoints = []

    for pattern in checkpoint_patterns:
        checkpoints = glob.glob(pattern)
        all_checkpoints.extend(checkpoints)

    # Sort by modification time (newest first)
    all_checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)

    return all_checkpoints

# Optional: Also improve the display_checkpoint_options function to be clearer
def display_checkpoint_options(dataset_key):
    """Display available checkpoint options with details - Enhanced for clarity"""
    checkpoints = find_available_checkpoints(dataset_key)

    if not checkpoints:
        print(f"‚ùå No checkpoints found for {dataset_key}")
        return None

    print(f"\nüìÅ AVAILABLE CHECKPOINTS FOR {dataset_key.upper()}:")
    print("=" * 80)

    options = []

    for i, checkpoint_path in enumerate(checkpoints, 1):
        filename = os.path.basename(checkpoint_path)  # Define filename BEFORE try block
        try:
            # Load checkpoint info without loading full model - FIXED for PyTorch 2.6
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
            mod_time = os.path.getmtime(checkpoint_path)

            # Extract checkpoint info
            epoch = checkpoint.get('epoch', 'Unknown')
            enhancements = checkpoint.get('enhancements', [])
            training_stats = checkpoint.get('training_stats', {})

            print(f"\n{i}. üìÑ {filename}")
            print(f"   üìÖ Epoch: {epoch}")
            print(f"   üìä File Size: {file_size:.1f} MB")
            print(f"   üïí Modified: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mod_time))}")

            if enhancements:
                print(f"   ‚ú® Enhancements: {', '.join(enhancements[:3])}{'...' if len(enhancements) > 3 else ''}")

            if training_stats:
                if 'avg_d_loss' in training_stats:
                    print(f"   üìà Critic Loss: {training_stats['avg_d_loss']:.4f}")
                if 'avg_g_loss' in training_stats:
                    print(f"   üìà Generator Loss: {training_stats['avg_g_loss']:.4f}")
                if 'ema_quality' in training_stats:
                    print(f"   üíú EMA Quality: {training_stats['ema_quality']:.4f}")

            options.append((checkpoint_path, checkpoint))

        except Exception as e:
            print(f"\n{i}. üìÑ {filename} (‚ö†Ô∏è  Unable to read details: {e})")
            options.append((checkpoint_path, None))

    print("\n" + "=" * 80)
    return options

def get_checkpoint_choice(dataset_key):
    """Get user's choice of checkpoint to resume from - Enhanced with clear numbering"""
    options = display_checkpoint_options(dataset_key)

    # üÜï ADD EMERGENCY CHECKPOINT DISPLAY
    print(f"\nüö® EMERGENCY CHECKPOINTS:")
    checkpoint_manager.display_emergency_checkpoints(dataset_key)

    if not options:
        return None, None

    print(f"\n" + "="*80)
    print("üìã CHECKPOINT SELECTION OPTIONS:")
    print("="*80)

    # Show option 0 clearly
    print(f"0. üÜï Start fresh training (no checkpoint)")

    # Show each checkpoint option with clear numbering
    for i, (checkpoint_path, checkpoint_data) in enumerate(options, 1):
        filename = os.path.basename(checkpoint_path)

        # Get basic file info
        file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
        mod_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(checkpoint_path)))

        print(f"\n{i}. üìÑ {filename}")
        print(f"   üìä Size: {file_size:.1f} MB")
        print(f"   üïí Modified: {mod_time}")

        # Show checkpoint details if available
        if checkpoint_data:
            epoch = checkpoint_data.get('epoch', 'Unknown')
            print(f"   üìÖ Epoch: {epoch}")

            if 'training_stats' in checkpoint_data:
                stats = checkpoint_data['training_stats']
                if 'avg_d_loss' in stats:
                    print(f"   üî¥ Critic Loss: {stats['avg_d_loss']:.4f}")
                if 'avg_g_loss' in stats:
                    print(f"   üîµ Generator Loss: {stats['avg_g_loss']:.4f}")
                if 'ema_quality' in stats:
                    print(f"   üíú EMA Quality: {stats['ema_quality']:.4f}")

            if 'enhancements' in checkpoint_data:
                enhancements = checkpoint_data['enhancements']
                print(f"   ‚ú® Features: {', '.join(enhancements[:3])}{'...' if len(enhancements) > 3 else ''}")
        else:
            print(f"   ‚ö†Ô∏è  Unable to read checkpoint details")

    print("\n" + "="*80)
    print("üí° SELECTION GUIDE:")
    print("   ‚Ä¢ Option 0: Fresh training from scratch")
    for i, (checkpoint_path, checkpoint_data) in enumerate(options, 1):
        filename = os.path.basename(checkpoint_path)

        # Give helpful descriptions
        if 'final' in filename.lower():
            desc = "Complete training session"
        elif 'best' in filename.lower():
            desc = "Best performing model"
        elif 'epoch' in filename.lower():
            desc = "Regular training checkpoint"
        else:
            desc = "Training checkpoint"

        print(f"   ‚Ä¢ Option {i}: Resume from {desc}")

    print("="*80)

    while True:
        try:
            choice = input(f"\nüéØ Select checkpoint to resume from (0-{len(options)}): ").strip()

            if choice == '0':
                print(f"‚úÖ Selected: Start fresh training")
                return None, None

            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(options):
                checkpoint_path, checkpoint_data = options[choice_idx]
                filename = os.path.basename(checkpoint_path)

                # Show what was selected
                print(f"‚úÖ Selected: Option {choice} - {filename}")

                # Show confirmation details
                if checkpoint_data:
                    epoch = checkpoint_data.get('epoch', 'Unknown')
                    print(f"   üìÖ Will resume from epoch: {epoch}")
                    if 'training_stats' in checkpoint_data:
                        stats = checkpoint_data['training_stats']
                        if 'avg_wd' in stats:
                            print(f"   üìà Last Wasserstein Distance: {stats['avg_wd']:.4f}")

                return checkpoint_path, checkpoint_data
            else:
                print(f"‚ùå Invalid choice. Please enter a number between 0 and {len(options)}")

        except ValueError:
            print("‚ùå Invalid input. Please enter a valid number")
        except KeyboardInterrupt:
            print("\nüëã Exiting...")
            exit()

# 2. REPLACE the load_checkpoint_and_resume function with this enhanced version
def load_checkpoint_and_resume(checkpoint_path, checkpoint_data, generator, critic,
                               optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                               ema_generator, device):
    """Load checkpoint and restore all training state - FIXED for device consistency"""

    print(f"\nüîÑ LOADING CHECKPOINT: {os.path.basename(checkpoint_path)}")
    print("=" * 60)

    try:
        # FIX: Handle the case where checkpoint_data is None
        if checkpoint_data is None:
            print("üì¶ Loading checkpoint data...")
            checkpoint_data = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        # Load model states
        print("üì¶ Loading Generator state...")
        generator.load_state_dict(checkpoint_data['generator'])

        print("üì¶ Loading Critic state...")
        critic.load_state_dict(checkpoint_data['critic'])

        # Load EMA parameters if available - FIXED for device consistency
        if 'ema_params' in checkpoint_data and checkpoint_data['ema_params']:
            print("üíú Loading EMA parameters...")

            # FIX: Ensure EMA shadow params are moved to the correct device
            loaded_ema_params = checkpoint_data['ema_params']
            for name, param in loaded_ema_params.items():
                if name in ema_generator.ema.shadow_params:
                    # Move the loaded EMA param to the same device as the model
                    ema_generator.ema.shadow_params[name] = param.to(device)

            if 'ema_quality' in checkpoint_data.get('training_stats', {}):
                ema_generator.quality_score = checkpoint_data['training_stats']['ema_quality']

            print(f"   ‚úÖ EMA parameters moved to device: {device}")

        # Load optimizer states if available
        if 'optimizer_G' in checkpoint_data:
            print("‚öôÔ∏è  Loading Generator optimizer state...")
            optimizer_G.load_state_dict(checkpoint_data['optimizer_G'])

        if 'optimizer_D' in checkpoint_data:
            print("‚öôÔ∏è  Loading Critic optimizer state...")
            optimizer_D.load_state_dict(checkpoint_data['optimizer_D'])

        # Load scheduler states if available
        if 'scheduler_G' in checkpoint_data:
            print("üìà Loading Generator scheduler state...")
            scheduler_G.load_state_dict(checkpoint_data['scheduler_G'])

        if 'scheduler_D' in checkpoint_data:
            print("üìà Loading Critic scheduler state...")
            scheduler_D.load_state_dict(checkpoint_data['scheduler_D'])

        # Get starting epoch
        start_epoch = checkpoint_data.get('epoch', 0)

        # Display loaded information
        print(f"\n‚úÖ CHECKPOINT LOADED SUCCESSFULLY!")
        print(f"   üìÖ Resuming from epoch: {start_epoch}")
        print(f"   üéØ Dataset: {checkpoint_data.get('dataset_key', 'Unknown')}")
        print(f"   üñ•Ô∏è  Device consistency: All parameters on {device}")

        if 'training_stats' in checkpoint_data:
            stats = checkpoint_data['training_stats']
            print(f"   üìä Last Training Stats:")
            if 'avg_d_loss' in stats:
                print(f"      üî¥ Critic Loss: {stats['avg_d_loss']:.6f}")
            if 'avg_g_loss' in stats:
                print(f"      üîµ Generator Loss: {stats['avg_g_loss']:.6f}")
            if 'avg_wd' in stats:
                print(f"      üü¢ Wasserstein Distance: {stats['avg_wd']:.6f}")
            if 'ema_quality' in stats:
                print(f"      üíú EMA Quality: {stats['ema_quality']:.4f}")

        if 'enhancements' in checkpoint_data:
            print(f"   ‚ú® Active Enhancements: {', '.join(checkpoint_data['enhancements'])}")

        print("=" * 60)

        return start_epoch

    except Exception as e:
        print(f"‚ùå ERROR loading checkpoint: {e}")
        print("üí° Starting fresh training instead...")
        return 0

def quick_resume_latest(dataset_key):
    """Quickly resume from the latest checkpoint"""
    checkpoints = find_available_checkpoints(dataset_key)
    if checkpoints:
        latest_checkpoint = checkpoints[0]  # Already sorted by newest first
        print(f"üöÄ Auto-resuming from latest checkpoint: {os.path.basename(latest_checkpoint)}")
        return latest_checkpoint
    else:
        print(f"‚ùå No checkpoints found for {dataset_key}")
        return None

def list_all_checkpoints():
    """List all available checkpoints for all datasets - Enhanced with emergency checkpoints"""
    datasets = ['mnist', 'cifar10']

    print("\nüìÅ ALL AVAILABLE CHECKPOINTS:")
    print("=" * 80)

    for dataset in datasets:
        print(f"\nüéØ {dataset.upper()}:")

        # Regular checkpoints
        checkpoints = find_available_checkpoints(dataset)
        if checkpoints:
            print(f"\n   üì¶ REGULAR CHECKPOINTS:")
            for checkpoint in checkpoints:
                filename = os.path.basename(checkpoint)
                file_size = os.path.getsize(checkpoint) / (1024 * 1024)
                mod_time = time.strftime('%Y-%m-%d %H:%M:%S',
                                         time.localtime(os.path.getmtime(checkpoint)))
                print(f"      üìÑ {filename} ({file_size:.1f}MB) - {mod_time}")
        else:
            print(f"      ‚ùå No regular checkpoints found")

        # üÜï ADD EMERGENCY CHECKPOINTS
        print(f"\n   üö® EMERGENCY CHECKPOINTS:")
        emergency_info = checkpoint_manager.get_emergency_checkpoint_info(dataset)
        if emergency_info:
            for info in emergency_info:
                status = "‚ö†Ô∏è  MINIMAL" if info['minimal_save'] else "‚úÖ COMPLETE"
                print(f"      üìÑ {info['filename']} ({info['size_mb']:.1f}MB) - {info['timestamp']} [{status}]")
                print(f"         üîç Reason: {info['reason']} | Epoch: {info['epoch']}")
        else:
            print(f"      ‚ùå No emergency checkpoints found")

    print("=" * 80)

# Enhanced Device Detection and Setup
def detect_and_setup_device():
    """
    Detect and setup the best available device with Apple Metal support
    """
    print("üîç Detecting available compute devices...")

    # Check for Apple Silicon with MPS support
    if torch.backends.mps.is_available():
        if torch.backends.mps.is_built():
            device = torch.device("mps")
            device_name = "Apple Silicon GPU (Metal Performance Shaders)"
            device_memory = "Unified Memory Architecture"
            print(f"‚úÖ Apple Metal GPU detected: {platform.processor()}")
            print(f"   üöÄ Using Metal Performance Shaders (MPS)")
            print(f"   üíæ Memory: {device_memory}")

            # Set MPS memory management
            if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                torch.mps.set_per_process_memory_fraction(0.8)  # Use 80% of available memory
                print(f"   ‚öôÔ∏è  Set MPS memory limit to 80%")

            return device, device_name, "mps"
        else:
            print("‚ö†Ô∏è  MPS is available but not built. Falling back to CPU.")

    # Check for CUDA (NVIDIA GPU)
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name(0)
        device_memory = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        print(f"‚úÖ NVIDIA GPU detected: {device_name}")
        print(f"   üíæ VRAM: {device_memory}")
        return device, device_name, "cuda"

    # Fallback to CPU
    else:
        device = torch.device("cpu")
        device_name = f"{platform.processor()} CPU"
        device_memory = "System RAM"
        print(f"üíª Using CPU: {device_name}")
        print(f"   üíæ Memory: {device_memory}")
        return device, device_name, "cpu"

def setup_device_optimizations(device_type):
    """
    Setup device-specific optimizations
    """
    print(f"\n‚öôÔ∏è  Configuring {device_type.upper()} optimizations...")

    if device_type == "mps":
        # Apple Metal optimizations
        print("   üçé Apple Metal optimizations:")
        print("   ‚úì Unified memory architecture enabled")
        print("   ‚úì Metal shader compilation optimized")
        print("   ‚úì Memory management configured")

        # Set environment variables for Metal
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

        # Configure batch sizes for Apple Silicon
        recommended_batch_size = 64  # Smaller batch size for MPS
        print(f"   üì¶ Recommended batch size: {recommended_batch_size}")

    elif device_type == "cuda":
        # NVIDIA CUDA optimizations
        print("   üü¢ NVIDIA CUDA optimizations:")
        print("   ‚úì cuDNN benchmark enabled")
        print("   ‚úì CUDA memory caching optimized")

        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        recommended_batch_size = 128
        print(f"   üì¶ Recommended batch size: {recommended_batch_size}")

    else:  # CPU
        # CPU optimizations
        print("   üíª CPU optimizations:")
        print("   ‚úì Threading optimized")
        print("   ‚úì Memory management configured")

        # Set optimal number of threads
        torch.set_num_threads(min(8, os.cpu_count()))

        recommended_batch_size = 32  # Smaller batch size for CPU
        print(f"   üì¶ Recommended batch size: {recommended_batch_size}")

    return recommended_batch_size

# Memory management for different devices
class DeviceMemoryManager:
    def __init__(self, device_type):
        self.device_type = device_type

    def clear_cache(self):
        """Clear device memory cache"""
        if self.device_type == "cuda":
            torch.cuda.empty_cache()
        elif self.device_type == "mps":
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()

    def get_memory_usage(self):
        """Get current memory usage"""
        if self.device_type == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            return f"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB"
        elif self.device_type == "mps":
            if hasattr(torch.mps, 'current_allocated_memory'):
                allocated = torch.mps.current_allocated_memory() / 1024**3
                return f"Allocated: {allocated:.2f}GB (Unified Memory)"
            else:
                return "MPS Memory tracking not available"
        else:
            if PSUTIL_AVAILABLE:
                import psutil
                memory = psutil.virtual_memory()
                return f"System RAM: {memory.percent}% used"
            else:
                return "CPU Memory monitoring not available"

    def optimize_for_device(self, model):
        """Apply device-specific model optimizations"""
        if self.device_type == "mps":
            # Apple Metal optimizations
            print("   üçé Applying Metal optimizations to model...")
            # MPS works best with float32
            model = model.float()

        elif self.device_type == "cuda":
            # NVIDIA optimizations
            print("   üü¢ Applying CUDA optimizations to model...")
            # Enable mixed precision if available
            if hasattr(torch.cuda.amp, 'autocast'):
                print("   ‚úì Mixed precision available")

        return model

# Initialize device
device, device_name, device_type = detect_and_setup_device()
recommended_batch_size = setup_device_optimizations(device_type)
memory_manager = DeviceMemoryManager(device_type)

print(f"\nüöÄ Enhanced DCGAN configured for: {device_name}")
print(f"üì± Device Type: {device_type.upper()}")

# Progress bars and terminal utilities
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
    print("‚úÖ tqdm available - Enhanced progress bars enabled")
except ImportError:
    TQDM_AVAILABLE = False
    print("‚ùå tqdm not available - Install with: pip install tqdm")
    # Fallback simple progress bar
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", leave=True,
                     ncols=None, unit="it", dynamic_ncols=True, colour=None):
            self.iterable = iterable
            self.total = total or (len(iterable) if iterable else 0)
            self.desc = desc
            self.n = 0
            self.leave = leave

        def __enter__(self):
            return self

        def __exit__(self, *args):
            if self.leave:
                print()

        def __iter__(self):
            for item in self.iterable:
                yield item
                self.update()

        def update(self, n=1):
            self.n += n
            if self.total > 0:
                progress = self.n / self.total
                bar_length = 40
                filled = int(bar_length * progress)
                bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
                percent = progress * 100
                print(f'\r{self.desc}: |{bar}| {percent:.1f}% ({self.n}/{self.total})', end='', flush=True)

        def set_description(self, desc):
            self.desc = desc

        def set_postfix(self, **kwargs):
            postfix = ', '.join([f'{k}={v}' for k, v in kwargs.items()])
            print(f' - {postfix}', end='', flush=True)

        def close(self):
            if self.leave:
                print()

# Enhanced Progress and Monitoring Utilities
class ProgressTracker:
    """Advanced progress tracking with device-specific optimizations"""
    def __init__(self, device_type):
        self.device_type = device_type
        self.start_time = time.time()
        self.epoch_start_time = None
        self.batch_times = deque(maxlen=100)
        self.loss_history = {'d_loss': [], 'g_loss': [], 'wd': [], 'gp': []}
        self.memory_usage = deque(maxlen=50)

    def start_epoch(self):
        self.epoch_start_time = time.time()
        memory_manager.clear_cache()  # Clear cache at epoch start

    def update_batch_time(self, batch_time):
        self.batch_times.append(batch_time)

        # Track memory usage
        memory_usage = memory_manager.get_memory_usage()
        self.memory_usage.append(memory_usage)

    def get_avg_batch_time(self):
        return np.mean(self.batch_times) if self.batch_times else 0

    def get_eta(self, current_batch, total_batches, current_epoch, total_epochs):
        if not self.batch_times:
            return "Calculating..."

        avg_batch_time = self.get_avg_batch_time()
        remaining_batches_this_epoch = total_batches - current_batch
        remaining_epochs = total_epochs - current_epoch

        eta_seconds = (remaining_batches_this_epoch * avg_batch_time +
                       remaining_epochs * total_batches * avg_batch_time)

        hours = int(eta_seconds // 3600)
        minutes = int((eta_seconds % 3600) // 60)
        seconds = int(eta_seconds % 60)

        if hours > 0:
            return f"{hours:02d}h:{minutes:02d}m:{seconds:02d}s"
        else:
            return f"{minutes:02d}m:{seconds:02d}s"

    def get_elapsed_time(self):
        elapsed = time.time() - self.start_time
        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        seconds = int(elapsed % 60)

        if hours > 0:
            return f"{hours:02d}h:{minutes:02d}m:{seconds:02d}s"
        else:
            return f"{minutes:02d}m:{seconds:02d}s"

    def get_current_memory_usage(self):
        return self.memory_usage[-1] if self.memory_usage else "N/A"

class LiveTerminalMonitor:
    """Live terminal monitoring with device-specific information"""
    def __init__(self, dataset_name, total_epochs, device_name, device_type):
        self.dataset_name = dataset_name
        self.total_epochs = total_epochs
        self.device_name = device_name
        self.device_type = device_type
        self.current_stats = {}

    def print_header(self, epoch, progress_tracker):
        print("=" * 100)
        print(f"üöÄ Enhanced DCGAN Training - {self.dataset_name} Dataset")
        print(f"üñ•Ô∏è  Device: {self.device_name} ({self.device_type.upper()})")
        print("=" * 100)
        print(f"üìÖ Epoch: {epoch}/{self.total_epochs}")
        print(f"‚è±Ô∏è  Elapsed: {progress_tracker.get_elapsed_time()}")
        print(f"üíæ Memory: {progress_tracker.get_current_memory_usage()}")
        print("-" * 100)

    def print_live_stats(self, stats):
        """Print live streaming statistics with device info"""
        self.current_stats.update(stats)

        print(f"üìä LIVE TRAINING METRICS ({self.device_type.upper()}):")
        print(f"   üî¥ Critic Loss:     {self.current_stats.get('d_loss', 0):.6f}")
        print(f"   üîµ Generator Loss:  {self.current_stats.get('g_loss', 0):.6f}")
        print(f"   üü¢ W-Distance:      {self.current_stats.get('wd', 0):.6f}")
        print(f"   üü° Grad Penalty:    {self.current_stats.get('gp', 0):.6f}")
        print(f"   üíú EMA Quality:     {self.current_stats.get('ema_quality', 0):.4f}")
        print(f"   üìà Batch Time:      {self.current_stats.get('batch_time', 0):.3f}s")
        print(f"   üîÑ Learning Rate G: {self.current_stats.get('lr_g', 0):.2e}")
        print(f"   üîÑ Learning Rate D: {self.current_stats.get('lr_d', 0):.2e}")
        print(f"   üíæ Memory Usage:    {self.current_stats.get('memory', 'N/A')}")

def print_step_details(step, total_steps, stats, progress_tracker, epoch, total_epochs):
    """Print detailed step information with device-specific metrics"""

    # Calculate progress
    step_progress = (step / total_steps) * 100
    eta = progress_tracker.get_eta(step, total_steps, epoch, total_epochs)

    # Create progress bar
    bar_length = 30
    filled = int(bar_length * (step / total_steps))
    bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)

    # Format stats
    d_loss = stats.get('d_loss', 0)
    g_loss = stats.get('g_loss', 0)
    wd = stats.get('wd', 0)
    gp = stats.get('gp', 0)
    batch_time = stats.get('batch_time', 0)

    # Print dynamic line with device info
    device_emoji = "üçé" if device_type == "mps" else "üü¢" if device_type == "cuda" else "üíª"
    info_line = (f"\r{device_emoji} Step {step:3d}/{total_steps} |{bar}| "
                 f"{step_progress:5.1f}% | "
                 f"D:{d_loss:7.4f} G:{g_loss:7.4f} | "
                 f"W:{wd:6.3f} GP:{gp:5.2f} | "
                 f"{batch_time:.3f}s/batch | "
                 f"ETA: {eta}")

    print(info_line, end='', flush=True)

# Optional TensorBoard support
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
    print("TensorBoard available - logging enabled")
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("TensorBoard not available - install with: pip install tensorboard")
    class DummyWriter:
        def add_scalar(self, *args, **kwargs): pass
        def close(self): pass
    SummaryWriter = lambda *args, **kwargs: DummyWriter()

# Fix SSL certificate issue on macOS
ssl._create_default_https_context = ssl._create_unverified_context

# =============================================================================
# DATASET CONFIGURATION WITH DEVICE OPTIMIZATION
# =============================================================================

class DatasetConfig:
    def __init__(self, name, image_size, channels, num_classes, description, preprocessing_info):
        self.name = name
        self.image_size = image_size
        self.channels = channels
        self.num_classes = num_classes
        self.description = description
        self.preprocessing_info = preprocessing_info

# Define dataset configurations with device-specific batch sizes
DATASETS = {
    'mnist': DatasetConfig(
        name='MNIST',
        image_size=32,
        channels=1,
        num_classes=10,
        description="Handwritten digits (0-9) in grayscale. 60,000 training images of 28x28 pixels.",
        preprocessing_info="Images resized to 32x32, normalized to [-1, 1] range"
    ),
    'cifar10': DatasetConfig(
        name='CIFAR-10',
        image_size=32,
        channels=3,
        num_classes=10,
        description="Color images in 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. 50,000 training images of 32x32 pixels.",
        preprocessing_info="Images normalized to [-1, 1] range, data augmentation with random horizontal flips"
    )
}

def display_dataset_options():
    """Display available datasets with device-specific recommendations"""
    print("\n" + "="*80)
    print("ENHANCED MULTI-DATASET DCGAN - DATASET SELECTION")
    print(f"üñ•Ô∏è  Optimized for: {device_name} ({device_type.upper()})")
    print("="*80)

    for idx, (key, config) in enumerate(DATASETS.items(), 1):
        print(f"\n{idx}. {config.name}")
        print(f"   Description: {config.description}")
        print(f"   Image Size: {config.image_size}x{config.image_size}")
        print(f"   Channels: {config.channels} ({'Grayscale' if config.channels == 1 else 'RGB'})")
        print(f"   Classes: {config.num_classes}")
        print(f"   Preprocessing: {config.preprocessing_info}")

        # Device-specific recommendations
        if device_type == "mps":
            print(f"   üçé Apple Metal: Optimized batch size, efficient memory usage")
        elif device_type == "cuda":
            print(f"   üü¢ NVIDIA CUDA: High performance training, large batch sizes")
        else:
            print(f"   üíª CPU: Stable training, conservative memory usage")

    print(f"\nüì¶ Recommended batch size for {device_type.upper()}: {recommended_batch_size}")
    print("\n" + "="*80)

def get_dataset_choice():
    """Get user's dataset choice"""
    display_dataset_options()

    while True:
        try:
            choice = input("\nSelect dataset (1 for MNIST, 2 for CIFAR-10): ").strip()
            if choice == '1':
                return 'mnist'
            elif choice == '2':
                return 'cifar10'
            else:
                print("Please enter 1 or 2")
        except KeyboardInterrupt:
            print("\nExiting...")
            exit()

def display_enhancement_details(dataset_key):
    """Display detailed enhancement information with device-specific optimizations"""
    config = DATASETS[dataset_key]

    print(f"\n" + "="*80)
    print(f"ENHANCED WORKFLOW - {config.name} DATASET")
    print(f"üñ•Ô∏è  Device: {device_name} ({device_type.upper()})")
    print("="*80)

    print("üöÄ ADVANCED ENHANCEMENTS:")
    print("\n1. DEVICE-SPECIFIC OPTIMIZATIONS:")
    if device_type == "mps":
        print("   ‚úì Apple Metal Performance Shaders (MPS)")
        print("   ‚úì Unified memory architecture utilization")
        print("   ‚úì Metal shader compilation optimization")
        print("   ‚úì M1/M2/M3 specific memory management")
    elif device_type == "cuda":
        print("   ‚úì NVIDIA CUDA acceleration")
        print("   ‚úì cuDNN optimized operations")
        print("   ‚úì GPU memory management")
        print("   ‚úì Mixed precision training support")
    else:
        print("   ‚úì CPU multi-threading optimization")
        print("   ‚úì Memory-efficient operations")
        print("   ‚úì SIMD instruction utilization")

    print("\n2. WGAN-GP LOSS:")
    print("   ‚úì Wasserstein distance for better convergence")
    print("   ‚úì Gradient penalty for Lipschitz constraint")
    print("   ‚úì No mode collapse issues")
    print("   ‚úì More stable training dynamics")

    print("\n3. EMA (EXPONENTIAL MOVING AVERAGE):")
    print("   ‚úì Smoothed generator parameters")
    print("   ‚úì Better sample quality at inference")
    print("   ‚úì Reduced variance in generated images")
    print("   ‚úì Improved training stability")

    print("\n4. ENHANCED ARCHITECTURE:")
    print("   ‚úì Improved convolutional layers")
    print("   ‚úì Instance normalization in discriminator")
    print("   ‚úì Device-optimized operations")
    print("   ‚úì Better gradient flow")

    print("\n5. SPECTRAL NORMALIZATION:")
    print("   ‚úì Lipschitz constraint enforcement")
    print("   ‚úì Prevents discriminator overpowering")
    print("   ‚úì Stable training without careful tuning")
    print("   ‚úì Device-agnostic implementation")

    print(f"\nüìä EXPECTED PERFORMANCE ({device_type.upper()}):")
    if device_type == "mps":
        print("   ‚Ä¢ 3-5x faster than CPU on Apple Silicon")
        print("   ‚Ä¢ Efficient unified memory usage")
        print("   ‚Ä¢ Optimized for M1/M2/M3 architecture")
    elif device_type == "cuda":
        print("   ‚Ä¢ 10-20x faster than CPU")
        print("   ‚Ä¢ High throughput training")
        print("   ‚Ä¢ Large batch size support")
    else:
        print("   ‚Ä¢ Stable CPU-based training")
        print("   ‚Ä¢ Memory-efficient operations")
        print("   ‚Ä¢ Multi-core utilization")

    print("\n‚ö° DEVICE-SPECIFIC FEATURES:")
    print(f"   ‚Ä¢ Real-time {device_type.upper()} memory monitoring")
    print("   ‚Ä¢ Device-optimized batch sizes")
    print("   ‚Ä¢ Hardware-specific acceleration")
    print("   ‚Ä¢ Adaptive memory management")

    print("="*80)

    # Get user confirmation
    confirm = input(f"\nProceed with enhanced training on {device_name}? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Training cancelled.")
        exit()

# Continue with the rest of the functions (transforms, dataset loading, etc.)
# These remain largely the same but with device-specific optimizations

def get_transforms(dataset_key):
    """Get appropriate transforms for the dataset"""
    config = DATASETS[dataset_key]

    if dataset_key == 'mnist':
        return transforms.Compose([
            transforms.Resize(config.image_size),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])
    else:  # cifar10
        return transforms.Compose([
            transforms.Resize(config.image_size),
            transforms.RandomHorizontalFlip(0.5),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

def get_dataset(dataset_key, transform):
    """Load the appropriate dataset"""
    if dataset_key == 'mnist':
        return torchvision.datasets.MNIST(
            root='./data', train=True, transform=transform, download=True
        )
    else:  # cifar10
        return torchvision.datasets.CIFAR10(
            root='./data', train=True, transform=transform, download=True
        )

def get_class_names(dataset_key):
    """Get class names for the dataset"""
    if dataset_key == 'mnist':
        return [str(i) for i in range(10)]
    else:  # cifar10
        return ['airplane', 'automobile', 'bird', 'cat', 'deer',
                'dog', 'frog', 'horse', 'ship', 'truck']

# =============================================================================
# ENHANCED LIVE PLOTTING
# =============================================================================

plt.ion()
class EnhancedLivePlotter:
    def __init__(self, dataset_name):
        self.fig, ((self.ax1, self.ax2), (self.ax3, self.ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle(f'Enhanced GAN Training - {dataset_name} - Live Progress', fontsize=16)

        # Data storage
        self.epochs = deque(maxlen=200)
        self.d_losses = deque(maxlen=200)
        self.g_losses = deque(maxlen=200)
        self.wasserstein_dists = deque(maxlen=200)
        self.gradient_penalties = deque(maxlen=200)
        self.learning_rates_g = deque(maxlen=200)
        self.learning_rates_d = deque(maxlen=200)
        self.ema_scores = deque(maxlen=200)

        # Setup loss plot
        self.line_d_loss, = self.ax1.plot([], [], 'r-', label='Critic Loss', linewidth=2)
        self.line_g_loss, = self.ax1.plot([], [], 'b-', label='Generator Loss', linewidth=2)
        self.ax1.set_title('WGAN-GP Training Losses')
        self.ax1.set_xlabel('Epoch')
        self.ax1.set_ylabel('Loss')
        self.ax1.legend()
        self.ax1.grid(True, alpha=0.3)

        # Setup Wasserstein distance plot
        self.line_wd, = self.ax2.plot([], [], 'g-', label='Wasserstein Distance', linewidth=2)
        self.line_gp, = self.ax2.plot([], [], 'orange', label='Gradient Penalty', linewidth=2)
        self.ax2.set_title('WGAN-GP Metrics')
        self.ax2.set_xlabel('Epoch')
        self.ax2.set_ylabel('Value')
        self.ax2.legend()
        self.ax2.grid(True, alpha=0.3)

        # Setup learning rate plot
        self.line_lr_g, = self.ax3.plot([], [], 'b--', label='Generator LR', linewidth=2)
        self.line_lr_d, = self.ax3.plot([], [], 'r--', label='Critic LR', linewidth=2)
        self.ax3.set_title('Learning Rates')
        self.ax3.set_xlabel('Epoch')
        self.ax3.set_ylabel('Learning Rate')
        self.ax3.legend()
        self.ax3.grid(True, alpha=0.3)

        # Setup EMA quality plot
        self.line_ema, = self.ax4.plot([], [], 'm-', label='EMA Quality Score', linewidth=2)
        self.ax4.set_title('EMA Generator Quality')
        self.ax4.set_xlabel('Epoch')
        self.ax4.set_ylabel('Quality Score')
        self.ax4.legend()
        self.ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show(block=False)

    def update(self, epoch, d_loss, g_loss, wd, gp, lr_g, lr_d, ema_score=0.5):
        # Add new data
        self.epochs.append(epoch)
        self.d_losses.append(d_loss)
        self.g_losses.append(g_loss)
        self.wasserstein_dists.append(wd)
        self.gradient_penalties.append(gp)
        self.learning_rates_g.append(lr_g)
        self.learning_rates_d.append(lr_d)
        self.ema_scores.append(ema_score)

        # Update loss plot
        self.line_d_loss.set_data(list(self.epochs), list(self.d_losses))
        self.line_g_loss.set_data(list(self.epochs), list(self.g_losses))
        self.ax1.relim()
        self.ax1.autoscale_view()

        # Update WGAN metrics
        self.line_wd.set_data(list(self.epochs), list(self.wasserstein_dists))
        self.line_gp.set_data(list(self.epochs), list(self.gradient_penalties))
        self.ax2.relim()
        self.ax2.autoscale_view()

        # Update learning rate plot
        self.line_lr_g.set_data(list(self.epochs), list(self.learning_rates_g))
        self.line_lr_d.set_data(list(self.epochs), list(self.learning_rates_d))
        self.ax3.relim()
        self.ax3.autoscale_view()

        # Update EMA plot
        self.line_ema.set_data(list(self.epochs), list(self.ema_scores))
        self.ax4.relim()
        self.ax4.autoscale_view()

        # Health indicators
        if gp > 50:
            self.ax2.set_facecolor('#ffeeee')
            self.ax2.set_title('WGAN-GP Metrics - WARNING: HIGH GP!')
        elif abs(wd) < 0.1:
            self.ax2.set_facecolor('#fff5ee')
            self.ax2.set_title('WGAN-GP Metrics - WARNING: LOW W-DISTANCE!')
        else:
            self.ax2.set_facecolor('white')
            self.ax2.set_title('WGAN-GP Metrics - HEALTHY')

        # Refresh the plot
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()

# =============================================================================
# ENHANCED MODELS
# =============================================================================

def weights_init(m):
    """Initialize weights according to DCGAN paper"""
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# Enhanced Conditional Generator
class EnhancedConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes, channels, ngf=64):
        super(EnhancedConditionalGenerator, self).__init__()
        self.latent_dim = latent_dim
        self.num_classes = num_classes
        self.channels = channels
        self.ngf = ngf

        # Enhanced embedding for class labels
        self.label_emb = nn.Embedding(num_classes, latent_dim)
        self.label_fc = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Initial dense layer with improved architecture
        self.fc = nn.Sequential(
            nn.Linear(latent_dim * 2, ngf * 8 * 4 * 4),
            nn.BatchNorm1d(ngf * 8 * 4 * 4),
            nn.ReLU(True)
        )

        # Enhanced convolutional layers
        self.conv_blocks = nn.Sequential(
            # 4x4 -> 8x8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),

            # 8x8 -> 16x16
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),

            # 16x16 -> 32x32
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            # Final output with residual connection
            nn.Conv2d(ngf, channels, 3, 1, 1, bias=False),
            nn.Tanh()
        )

        self.apply(weights_init)

    def forward(self, noise, labels):
        # Enhanced label embedding
        label_emb = self.label_emb(labels)
        label_emb = self.label_fc(label_emb)

        # Concatenate noise and enhanced label embedding
        gen_input = torch.cat([noise, label_emb], dim=1)

        # Dense layer
        out = self.fc(gen_input)
        out = out.view(out.size(0), self.ngf * 8, 4, 4)

        # Convolutional layers
        img = self.conv_blocks(out)
        return img

# Enhanced Conditional Critic (for WGAN-GP)
class EnhancedConditionalCritic(nn.Module):
    def __init__(self, num_classes, channels, image_size=32, ndf=64):
        super(EnhancedConditionalCritic, self).__init__()
        self.ndf = ndf
        self.num_classes = num_classes
        self.channels = channels
        self.image_size = image_size

        # Enhanced embedding for class labels
        self.label_emb = nn.Embedding(num_classes, image_size * image_size)

        # Enhanced convolutional layers (NO SIGMOID for WGAN)
        self.conv_blocks = nn.Sequential(
            # 32x32 -> 16x16
            nn.Conv2d(channels + 1, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # 16x16 -> 8x8
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 2),  # Instance norm instead of batch norm
            nn.LeakyReLU(0.2, inplace=True),

            # 8x8 -> 4x4
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # 4x4 -> 2x2
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),

            # 2x2 -> 1x1 (NO SIGMOID for WGAN!)
            nn.Conv2d(ndf * 8, 1, 2, 1, 0, bias=False)
        )

        self.apply(weights_init)

    def forward(self, img, labels):
        # Embed labels and reshape to image dimensions
        label_emb = self.label_emb(labels)
        label_emb = label_emb.view(label_emb.size(0), 1, self.image_size, self.image_size)

        # Concatenate image and label embedding
        d_input = torch.cat([img, label_emb], dim=1)

        # Convolutional layers
        validity = self.conv_blocks(d_input)
        return validity.view(-1)

# =============================================================================
# WGAN-GP LOSS
# =============================================================================
class AdaptiveGradientPenaltyScheduler:
    """FIXED: Monitor gradient NORMS (not penalty values) for proper WGAN-GP training"""

    def __init__(self, initial_lambda=10.0, target_norm_range=(0.8, 1.2)):
        self.lambda_gp = initial_lambda
        self.target_min, self.target_max = target_norm_range  # TARGET GRADIENT NORMS, not penalty values
        self.norm_history = []  # Track gradient NORMS, not penalty values
        self.adjustment_factor = 1.05
        self.update_frequency = 100  # Check every 100 steps
        self.max_lambda = 100.0  # More reasonable maximum
        self.min_lambda = 1.0
        self.adjustment_count = 0
        self.max_adjustments_per_epoch = 2  # Limit adjustments

    def update(self, gradient_norm):
        """FIXED: Update lambda based on GRADIENT NORMS, not penalty values"""
        self.norm_history.append(gradient_norm)

        # Only adjust periodically and with limits
        if len(self.norm_history) >= self.update_frequency and self.adjustment_count < self.max_adjustments_per_epoch:
            avg_norm = np.mean(self.norm_history[-self.update_frequency:])

            # FIXED: Adjust based on gradient norms being close to 1.0
            if avg_norm < self.target_min:  # Gradient norms too small
                old_lambda = self.lambda_gp
                self.lambda_gp *= self.adjustment_factor
                self.lambda_gp = min(self.lambda_gp, self.max_lambda)
                self.adjustment_count += 1
                print(f"üîß Grad norm low ({avg_norm:.3f} < {self.target_min:.1f}) - Increasing lambda: {old_lambda:.1f} ‚Üí {self.lambda_gp:.1f}")

            elif avg_norm > self.target_max:  # Gradient norms too large
                old_lambda = self.lambda_gp
                self.lambda_gp /= self.adjustment_factor
                self.lambda_gp = max(self.lambda_gp, self.min_lambda)
                self.adjustment_count += 1
                print(f"üîß Grad norm high ({avg_norm:.3f} > {self.target_max:.1f}) - Decreasing lambda: {old_lambda:.1f} ‚Üí {self.lambda_gp:.1f}")

            # Clear history after adjustment
            self.norm_history = []

        return self.lambda_gp

    def reset_epoch_counter(self):
        """Reset adjustment counter for new epoch"""
        self.adjustment_count = 0

class WassersteinGPLoss:
    """FIXED: Enhanced Wasserstein Loss with PROPER gradient penalty monitoring"""
    def __init__(self, lambda_gp=10.0):  # FIXED: Use standard lambda=10
        self.lambda_gp = lambda_gp
        self._step_count = 0
        print(f"üîß WGAN-GP initialized with lambda_gp = {lambda_gp}")

    def gradient_penalty(self, critic, real_samples, fake_samples, labels, device):
        """FIXED: More robust gradient penalty calculation with proper monitoring"""
        batch_size = real_samples.size(0)

        # Generate random interpolation coefficients
        alpha = torch.rand(batch_size, 1, 1, 1).to(device)
        alpha = alpha.expand_as(real_samples)

        # Interpolation between real and fake samples
        interpolates = alpha * real_samples + (1 - alpha) * fake_samples
        interpolates = interpolates.to(device)
        interpolates.requires_grad_(True)

        # Forward pass through critic
        d_interpolates = critic(interpolates, labels)

        # Calculate gradients with respect to interpolates
        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones(d_interpolates.size()).to(device),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]

        # Reshape gradients for norm calculation
        gradients = gradients.view(gradients.size(0), -1)

        # Calculate gradient norms
        gradient_norms = gradients.norm(2, dim=1)

        # FIXED: Calculate gradient penalty
        gradient_penalty = ((gradient_norms - 1) ** 2).mean()

        # Clamp gradient penalty to prevent extreme values
        gradient_penalty = torch.clamp(gradient_penalty, 0.0, 100.0)

        # Return both penalty and average gradient norm for monitoring
        avg_gradient_norm = gradient_norms.mean().item()

        return gradient_penalty, avg_gradient_norm

    def critic_loss(self, critic, real_samples, fake_samples, real_labels, fake_labels, device):
        """FIXED: Enhanced critic loss with proper gradient norm monitoring"""
        # Real samples loss
        real_output = critic(real_samples, real_labels)
        real_loss = -torch.mean(real_output)

        # Fake samples loss
        fake_output = critic(fake_samples.detach(), fake_labels)
        fake_loss = torch.mean(fake_output)

        # Gradient penalty with gradient norm monitoring
        gp, avg_grad_norm = self.gradient_penalty(critic, real_samples, fake_samples, real_labels, device)

        # Total critic loss
        critic_loss = real_loss + fake_loss + self.lambda_gp * gp

        # Wasserstein distance estimate
        wasserstein_distance = -(real_loss.item() + fake_loss.item())

        # FIXED: Debug information with gradient norms
        self._step_count += 1
        if self._step_count % 50 == 0:
            print(f"üîç WGAN-GP Debug - GP: {gp.item():.6f}, Grad Norm: {avg_grad_norm:.6f} (target: ~1.0), Lambda: {self.lambda_gp:.1f}")

        return critic_loss, real_loss, fake_loss, gp, wasserstein_distance, avg_grad_norm

    def generator_loss(self, critic, fake_samples, fake_labels):
        """Calculate generator loss"""
        fake_output = critic(fake_samples, fake_labels)
        g_loss = -torch.mean(fake_output)
        return g_loss



# Add this enhanced step details function:
def print_enhanced_step_details(step, total_steps, stats, progress_tracker, epoch, total_epochs):
    """Enhanced step details with gradient norm monitoring"""

    # Calculate progress
    step_progress = (step / total_steps) * 100
    eta = progress_tracker.get_eta(step, total_steps, epoch, total_epochs)

    # Create progress bar
    bar_length = 30
    filled = int(bar_length * (step / total_steps))
    bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)

    # Format stats
    d_loss = stats.get('d_loss', 0)
    g_loss = stats.get('g_loss', 0)
    wd = stats.get('wd', 0)
    gp = stats.get('gp', 0)
    grad_norm = stats.get('grad_norm', 0)  # NEW: Gradient norm
    batch_time = stats.get('batch_time', 0)

    # FIXED: Show gradient norm (target ~1.0) instead of just penalty
    device_emoji = "üçé" if device_type == "mps" else "üü¢" if device_type == "cuda" else "üíª"
    info_line = (f"\r{device_emoji} Step {step:3d}/{total_steps} |{bar}| "
                 f"{step_progress:5.1f}% | "
                 f"D:{d_loss:7.4f} G:{g_loss:7.4f} | "
                 f"W:{wd:6.3f} GP:{gp:5.2f} | "
                 f"GradNorm:{grad_norm:.3f} | "  # NEW: Show gradient norm
                 f"{batch_time:.3f}s/batch | "
                 f"ETA: {eta}")

    print(info_line, end='', flush=True)


def print_enhanced_epoch_summary(epoch, training_stats):
    """Enhanced epoch summary with gradient norm health assessment"""

    avg_d_loss = training_stats['avg_d_loss']
    avg_g_loss = training_stats['avg_g_loss']
    avg_wd = training_stats['avg_wd']
    avg_gp = training_stats['avg_gp']
    avg_grad_norm = training_stats.get('avg_grad_norm', 0)  # NEW
    current_lambda = training_stats.get('current_lambda_gp', 10)

    print(f"\nüéâ EPOCH {epoch} COMPREHENSIVE SUMMARY:")
    print("=" * 80)
    print(f"üìä Training Metrics:")
    print(f"   üî¥ Average Critic Loss: {avg_d_loss:.6f}")
    print(f"   üîµ Average Generator Loss: {avg_g_loss:.6f}")
    print(f"   üü¢ Average Wasserstein Distance: {avg_wd:.6f}")
    print(f"   üü° Average Gradient Penalty: {avg_gp:.6f}")
    print(f"   üìè Average Gradient Norm: {avg_grad_norm:.6f} (target: ~1.0)")  # NEW
    print(f"   üîß Current GP Lambda: {current_lambda:.2f}")

    # FIXED: Health assessment based on gradient norms
    if 0.8 <= avg_grad_norm <= 1.2:
        print(f"   ‚úÖ Gradient Health: OPTIMAL (norm: {avg_grad_norm:.3f})")
    elif avg_grad_norm < 0.8:
        print(f"   üö® Gradient Health: NORMS TOO LOW (norm: {avg_grad_norm:.3f}) - Increase lambda")
    else:
        print(f"   ‚ö†Ô∏è  Gradient Health: NORMS TOO HIGH (norm: {avg_grad_norm:.3f}) - Decrease lambda")


# =============================================================================
# EMA GENERATOR
# =============================================================================

# 1. REPLACE the ExponentialMovingAverage.update method (around line 1560)
class ExponentialMovingAverage:
    """Enhanced EMA for model parameters"""
    def __init__(self, model, decay=0.999, device=None):
        self.decay = decay
        self.device = device if device is not None else next(model.parameters()).device
        self.shadow_params = {}
        self.backup_params = {}

        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow_params[name] = param.data.clone().to(self.device)

    def update(self, model):
        """Update EMA parameters - FIXED for device consistency"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.shadow_params:
                # FIX: Ensure both tensors are on the same device
                shadow_param = self.shadow_params[name].to(param.device)
                param_data = param.data.to(param.device)

                self.shadow_params[name] = (
                        self.decay * shadow_param + (1.0 - self.decay) * param_data
                ).to(param.device)

    def apply_shadow(self, model):
        """Apply EMA parameters - FIXED for device consistency"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.shadow_params:
                self.backup_params[name] = param.data.clone()
                # FIX: Ensure shadow param is on the same device as model param
                shadow_param = self.shadow_params[name].to(param.device)
                param.data.copy_(shadow_param)

    def restore(self, model):
        """Restore original parameters"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.backup_params:
                param.data.copy_(self.backup_params[name])
        self.backup_params.clear()


# 4. UPDATE the EMAGenerator initialization to ensure proper device handling
class EMAGenerator:
    """Enhanced wrapper for generator with EMA - FIXED for device consistency"""
    def __init__(self, generator, decay=0.999, device=None):
        self.generator = generator
        # FIX: Ensure EMA uses the same device as the generator
        if device is None:
            device = next(generator.parameters()).device
        self.ema = ExponentialMovingAverage(generator, decay, device)
        self.quality_score = 0.5

    def update(self):
        """Update EMA parameters and quality score"""
        self.ema.update(self.generator)
        # Simple quality score based on parameter stability
        self.quality_score = min(0.9, self.quality_score + 0.01)

    def forward_with_ema(self, *args, **kwargs):
        """Forward pass with EMA parameters"""
        # Set generator to eval mode to avoid batch norm issues with single samples
        was_training = self.generator.training
        self.generator.eval()

        self.ema.apply_shadow(self.generator)
        try:
            output = self.generator(*args, **kwargs)
        finally:
            self.ema.restore(self.generator)
            # Restore original training mode
            if was_training:
                self.generator.train()
        return output

    def __call__(self, *args, **kwargs):
        return self.generator(*args, **kwargs)

# =============================================================================
# ENHANCED TRAINING FUNCTION WITH CHECKPOINT RESUME
# =============================================================================
# =============================================================================
# COMPLETE UPDATED TRAINING FUNCTION WITH ALL FIXES
# =============================================================================

# First, add this helper function before the training function (around line 650-700)
# RECOMMENDED: Replace your verify_device_consistency function with this one
def verify_device_consistency(generator, critic, ema_generator, device):
    """Recommended device consistency check - handles MPS properly"""
    print(f"\nüîç VERIFYING DEVICE CONSISTENCY:")

    # Get actual devices
    gen_device = next(generator.parameters()).device
    critic_device = next(critic.parameters()).device

    print(f"   Generator: {gen_device}")
    print(f"   Critic: {critic_device}")

    # Check EMA shadow params
    ema_device = None
    if ema_generator.ema.shadow_params:
        first_ema_param = next(iter(ema_generator.ema.shadow_params.values()))
        ema_device = first_ema_param.device
        print(f"   EMA Shadow Params: {ema_device}")

    # Convert target to torch.device if needed
    if isinstance(device, str):
        target_device = torch.device(device)
    else:
        target_device = device

    print(f"   üéØ Target device: {target_device}")

    # Check for REAL mismatches (different device types)
    issues = []

    if gen_device.type != target_device.type:
        issues.append(f"Generator on {gen_device.type}, expected {target_device.type}")

    if critic_device.type != target_device.type:
        issues.append(f"Critic on {critic_device.type}, expected {target_device.type}")

    if ema_device and ema_device.type != target_device.type:
        issues.append(f"EMA on {ema_device.type}, expected {target_device.type}")

    if not issues:
        print(f"   ‚úÖ All components on correct device type: {target_device.type}")
        if gen_device.type == 'mps':
            print(f"   üçé Apple Metal GPU working correctly!")
        elif gen_device.type == 'cuda':
            print(f"   üü¢ NVIDIA GPU working correctly!")
        else:
            print(f"   üíª CPU working correctly!")
        return True
    else:
        print(f"   ‚ùå Real device mismatches found:")
        for issue in issues:
            print(f"      ‚Ä¢ {issue}")

        print(f"   üîß Fixing device placement...")

        # Fix the issues
        if gen_device.type != target_device.type:
            generator.to(target_device)
            print(f"   üì¶ Moved generator: {gen_device} ‚Üí {target_device}")

        if critic_device.type != target_device.type:
            critic.to(target_device)
            print(f"   üì¶ Moved critic: {critic_device} ‚Üí {target_device}")

        if ema_device and ema_device.type != target_device.type:
            for name, param in ema_generator.ema.shadow_params.items():
                ema_generator.ema.shadow_params[name] = param.to(target_device)
            print(f"   üì¶ Moved EMA: {ema_device} ‚Üí {target_device}")

        print(f"   ‚úÖ All device issues resolved!")
        return False

# Now, replace your existing train_enhanced_gan_with_resume_modified function with this:
# Replace the train_enhanced_gan_with_resume_modified function in your
# enhanced_dcgan_mnist_cifar_for_apple_mps_checkpoints_graceful.py
# with this corrected version:

# =============================================================================
# STEP 6: ADD GP DIAGNOSTIC FUNCTION (OPTIONAL BUT RECOMMENDED)
# =============================================================================

# Add this function anywhere in your code for debugging:

def diagnose_gradient_penalty_issue(critic, real_samples, fake_samples, labels, device):
    """Comprehensive GP diagnostic - call this if you have issues"""
    print("üîç GRADIENT PENALTY DIAGNOSTIC")
    print("=" * 50)

    batch_size = real_samples.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1).to(device)
    alpha = alpha.expand_as(real_samples)

    # Create interpolates
    interpolates = alpha * real_samples + (1 - alpha) * fake_samples
    interpolates = interpolates.to(device)
    interpolates.requires_grad_(True)

    # Forward pass
    d_interpolates = critic(interpolates, labels)
    print(f"   Interpolates shape: {interpolates.shape}")
    print(f"   Critic output shape: {d_interpolates.shape}")
    print(f"   Critic output range: [{d_interpolates.min().item():.3f}, {d_interpolates.max().item():.3f}]")

    # Calculate gradients
    gradients = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones(d_interpolates.size()).to(device),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients_flat = gradients.view(gradients.size(0), -1)
    grad_norms = gradients_flat.norm(2, dim=1)

    print(f"   Gradient norms mean: {grad_norms.mean().item():.6f}")
    print(f"   Gradient norms std: {grad_norms.std().item():.6f}")
    print(f"   Gradient norms range: [{grad_norms.min().item():.6f}, {grad_norms.max().item():.6f}]")

    # Calculate GP
    gp = ((grad_norms - 1) ** 2).mean()
    print(f"   Raw GP value: {gp.item():.6f}")

    # Test different lambda values
    lambdas = [10.0, 50.0, 100.0, 200.0]
    print(f"   GP with different lambdas:")
    for lam in lambdas:
        weighted_gp = lam * gp.item()
        print(f"      Lambda {lam:3.0f}: {weighted_gp:.6f}")

    print("=" * 50)

    return gp.item(), grad_norms.mean().item()


@enhanced_error_handler
def train_enhanced_gan_with_resume_modified(dataset_key, config, resume_from_checkpoint=True, num_epochs=100):
    """Enhanced training with FIXED WGAN-GP gradient penalty monitoring"""

    print(f"\nüöÄ Starting Enhanced Training for {config.name}")
    print("üõ°Ô∏è  Enhanced checkpoint system active:")
    print("   ‚úÖ Auto-save every 5 epochs")
    print("   ‚úÖ Graceful interrupt handling (Ctrl+C)")
    print("   ‚úÖ Emergency error recovery")
    print("   ‚úÖ Device consistency checks")
    print("   ‚úÖ üÜï FIXED WGAN-GP gradient norm monitoring")

    # Check for resume option
    start_epoch = 0
    checkpoint_path = None
    checkpoint_data = None

    if resume_from_checkpoint:
        print("\nüîç CHECKPOINT RESUME OPTIONS")
        checkpoint_path, checkpoint_data = get_checkpoint_choice(dataset_key)

    print("="*80)
    print("üéØ Active Enhancements:")
    print("   ‚úÖ 1. WGAN-GP Loss with FIXED Gradient Penalty")
    print("   ‚úÖ 2. EMA (Exponential Moving Average)")
    print("   ‚úÖ 3. Enhanced Generator/Critic Architecture")
    print("   ‚úÖ 4. Spectral Normalization")
    print("   ‚úÖ 5. Progressive Learning Rate Scheduling")
    print("   ‚úÖ 6. Advanced Training Monitoring")
    print("   ‚úÖ 7. Live Progress Tracking & Terminal Streaming")
    print("   ‚úÖ 8. Checkpoint Resume Capability")
    print("   ‚úÖ 9. üÜï AUTO-SAVE EVERY 5 EPOCHS")
    print("   ‚úÖ 10. üÜï GRACEFUL INTERRUPT HANDLING")
    print("   ‚úÖ 11. üÜï EMERGENCY ERROR RECOVERY")
    print("   ‚úÖ 12. üÜï DEVICE CONSISTENCY CHECKS")
    print("   ‚úÖ 13. üÜï FIXED GRADIENT NORM MONITORING")
    print("="*80)

    # Enhanced hyperparameters
    batch_size = 128
    if dataset_key == 'mnist':
        learning_rate_g = 0.0001
        learning_rate_d = 0.0004
    else:  # cifar10
        learning_rate_g = 0.0001
        learning_rate_d = 0.0004

    latent_dim = 100
    beta1 = 0.0
    beta2 = 0.9
    n_critic = 5
    lambda_gp = 10.0  # FIXED: Use standard lambda=10 instead of 50

    # Initialize enhanced networks
    print(f"üèóÔ∏è  Initializing models on device: {device}")
    generator = EnhancedConditionalGenerator(latent_dim, config.num_classes, config.channels).to(device)
    critic = EnhancedConditionalCritic(config.num_classes, config.channels, config.image_size).to(device)

    # Apply spectral normalization to critic
    for module in critic.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            nn.utils.spectral_norm(module)

    # Initialize EMA Generator with explicit device
    print(f"üíú Initializing EMA on device: {device}")
    ema_generator = EMAGenerator(generator, decay=0.999, device=device)

    print(f"üìä Enhanced Model Statistics:")
    print(f"   Generator parameters: {sum(p.numel() for p in generator.parameters()):,}")
    print(f"   Critic parameters: {sum(p.numel() for p in critic.parameters()):,}")
    print(f"   EMA decay: 0.999")
    print(f"   Spectral normalization: Applied to critic")
    print(f"   Target device: {device}")

    # Initialize optimizers and schedulers
    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_g, betas=(beta1, beta2))
    optimizer_D = optim.Adam(critic.parameters(), lr=learning_rate_d, betas=(beta1, beta2))
    scheduler_G = optim.lr_scheduler.ExponentialLR(optimizer_G, gamma=0.995)
    scheduler_D = optim.lr_scheduler.ExponentialLR(optimizer_D, gamma=0.995)

    # üÜï REGISTER COMPONENTS WITH CHECKPOINT MANAGER
    checkpoint_manager.register_training_components(
        dataset_key, generator, critic, optimizer_G, optimizer_D,
        scheduler_G, scheduler_D, ema_generator
    )

    # INITIAL DEVICE CONSISTENCY CHECK
    print(f"\nüîß INITIAL DEVICE CONSISTENCY CHECK:")
    verify_device_consistency(generator, critic, ema_generator, device)

    # Load checkpoint if selected
    if checkpoint_path and checkpoint_data:
        print(f"\nüì• Loading checkpoint: {os.path.basename(checkpoint_path)}")
        start_epoch = load_checkpoint_and_resume(
            checkpoint_path, checkpoint_data, generator, critic,
            optimizer_G, optimizer_D, scheduler_G, scheduler_D,
            ema_generator, device
        )

        # POST-CHECKPOINT DEVICE CONSISTENCY CHECK
        print(f"\nüîß POST-CHECKPOINT DEVICE CONSISTENCY CHECK:")
        verify_device_consistency(generator, critic, ema_generator, device)

        # Adjust num_epochs if resuming
        remaining_epochs = num_epochs - start_epoch
        if remaining_epochs <= 0:
            print(f"‚ö†Ô∏è  Training already completed! Starting additional 25 epochs...")
            num_epochs = start_epoch + 25
            remaining_epochs = 25

        print(f"üéØ Training will continue for {remaining_epochs} more epochs (until epoch {num_epochs})")
    else:
        print(f"üÜï Starting fresh training from epoch 1")

        # FRESH TRAINING DEVICE CONSISTENCY CHECK
        print(f"\nüîß FRESH TRAINING DEVICE CONSISTENCY CHECK:")
        verify_device_consistency(generator, critic, ema_generator, device)

    # Load dataset
    print(f"\nüì• Loading {config.name} dataset...")
    transform = get_transforms(dataset_key)
    train_dataset = get_dataset(dataset_key, transform)

    # CREATE train_loader here in the correct scope
    try:
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            persistent_workers=False,
            pin_memory=False if device_type == "mps" else True
        )
        print(f"‚úÖ DataLoader created with 2 workers for optimal performance")
    except Exception as e:
        print(f"‚ö†Ô∏è  Multiprocessing DataLoader failed: {e}")
        print(f"üîÑ Falling back to single-threaded DataLoader for stability")
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0
        )

    print(f"‚úÖ Dataset loaded successfully!")
    print(f"   üìä Training samples: {len(train_dataset):,}")
    print(f"   üì¶ Batch size: {batch_size}")
    print(f"   üîÑ Total batches per epoch: {len(train_loader):,}")

    # Initialize enhanced live plotter
    live_plotter = EnhancedLivePlotter(config.name)

    # Initialize progress tracking
    progress_tracker = ProgressTracker(device_type)
    terminal_monitor = LiveTerminalMonitor(config.name, num_epochs, device_name, device_type)

    # TensorBoard writer with resume support
    if TENSORBOARD_AVAILABLE:
        log_suffix = f"_resume_epoch_{start_epoch}" if start_epoch > 0 else ""
        writer = SummaryWriter(f'./runs/{dataset_key}_enhanced_gan{log_suffix}')
        print(f"üìä TensorBoard logging to: ./runs/{dataset_key}_enhanced_gan{log_suffix}")
    else:
        writer = SummaryWriter()

    # FIXED: Initialize WGAN-GP loss with proper parameters
    wgan_loss = WassersteinGPLoss(lambda_gp)
    gp_scheduler = AdaptiveGradientPenaltyScheduler(
        initial_lambda=lambda_gp,
        target_norm_range=(0.8, 1.2)  # FIXED: Target gradient norms, not penalty values
    )
    print(f"üîß FIXED: Adaptive GP scheduler targets gradient norms 0.8-1.2 (not penalty values)")

    generator.train()
    critic.train()

    # Fixed samples for evaluation
    fixed_noise = torch.randn(64, latent_dim).to(device)
    fixed_labels = torch.randint(0, config.num_classes, (64,)).to(device)

    print(f"\nüéØ Enhanced Training Configuration:")
    print(f"   Starting Epoch: {start_epoch + 1}")
    print(f"   Target Epochs: {num_epochs}")
    print(f"   Remaining Epochs: {num_epochs - start_epoch}")
    print(f"   Batch Size: {batch_size}")
    print(f"   Checkpoint Frequency: Every 5 epochs")
    print(f"   Current Generator LR: {scheduler_G.get_last_lr()[0]:.2e}")
    print(f"   Current Critic LR: {scheduler_D.get_last_lr()[0]:.2e}")
    print(f"   Critic Updates per Generator: {n_critic}")
    print(f"   Gradient Penalty Lambda: {lambda_gp}")
    print(f"   üÜï FIXED: Monitoring gradient norms (target: ~1.0)")
    print(f"   Device: {device}")
    print(f"   Total Batches per Epoch: {len(train_loader)}")

    if checkpoint_path:
        print(f"   üìÅ Resumed from: {os.path.basename(checkpoint_path)}")

    # FINAL PRE-TRAINING DEVICE CONSISTENCY CHECK
    print(f"\nüîß FINAL PRE-TRAINING DEVICE CONSISTENCY CHECK:")
    is_consistent = verify_device_consistency(generator, critic, ema_generator, device)

    if not is_consistent:
        print("üö® CRITICAL: Device inconsistency detected before training!")
        print("üîÑ Attempting emergency device fix...")

        generator = generator.to(device)
        critic = critic.to(device)

        if ema_generator.ema.shadow_params:
            for name, param in ema_generator.ema.shadow_params.items():
                ema_generator.ema.shadow_params[name] = param.to(device)

        print(f"üîß Verifying emergency fix:")
        verify_device_consistency(generator, critic, ema_generator, device)

    print(f"\nüöÄ STARTING/RESUMING ENHANCED TRAINING WITH FIXED WGAN-GP...")
    print("üí° Press Ctrl+C for graceful shutdown with checkpoint saving")
    print("="*80)

    start_time = time.time()
    best_wasserstein_dist = float('inf')

    # Create main progress bar for epochs
    epoch_pbar = tqdm(range(start_epoch, num_epochs), desc="üéØ Training Epochs",
                      leave=True, ncols=100, colour='green')

    try:
        for epoch in epoch_pbar:
            progress_tracker.start_epoch()
            epoch_start_time = time.time()

            # Reset GP adjustment counter for new epoch
            gp_scheduler.reset_epoch_counter()

            # Initialize epoch statistics arrays
            d_losses = []
            g_losses = []
            wasserstein_distances = []
            gradient_penalties = []
            gradient_norms = []  # NEW: Track gradient norms
            batch_times = []

            # Update epoch progress bar description
            if start_epoch > 0:
                epoch_pbar.set_description(f"üîÑ Resumed Epoch {epoch+1}/{num_epochs}")
            else:
                epoch_pbar.set_description(f"üéØ Epoch {epoch+1}/{num_epochs}")

            print(f"\n\nüìÖ EPOCH {epoch+1}/{num_epochs} - Enhanced Training with FIXED WGAN-GP")
            if start_epoch > 0 and epoch == start_epoch:
                print(f"üîÑ RESUMED from checkpoint at epoch {start_epoch}")
            print("=" * 80)

            # Create batch progress bar
            batch_pbar = tqdm(train_loader, desc="üîÑ Processing Batches",
                              leave=False, ncols=120, colour='blue')

            for i, (real_imgs, labels) in enumerate(batch_pbar):
                batch_start_time = time.time()
                current_batch_size = real_imgs.size(0)

                # Ensure all data is on the correct device
                real_imgs = real_imgs.to(device)
                labels = labels.to(device)

                # Train Critic multiple times
                critic_losses = []
                last_grad_norm = 0  # Initialize to track gradient norm from last critic iteration

                for critic_iter in range(n_critic):
                    optimizer_D.zero_grad()

                    # Generate fake images
                    z = torch.randn(current_batch_size, latent_dim).to(device)
                    fake_labels_gen = torch.randint(0, config.num_classes, (current_batch_size,)).to(device)
                    fake_imgs = generator(z, fake_labels_gen)

                    # FIXED: Calculate critic loss with gradient norm monitoring
                    d_loss, d_real, d_fake, gp, wd, avg_grad_norm = wgan_loss.critic_loss(
                        critic, real_imgs, fake_imgs, labels, fake_labels_gen, device
                    )

                    # FIXED: Update GP lambda based on gradient NORMS (not penalty values)
                    new_lambda = gp_scheduler.update(avg_grad_norm)  # Pass gradient norm, not penalty
                    wgan_loss.lambda_gp = new_lambda

                    # Store the gradient norm from the last iteration
                    last_grad_norm = avg_grad_norm

                    d_loss.backward()
                    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)
                    optimizer_D.step()

                    critic_losses.append(d_loss.item())

                # Use the last critic iteration's values for logging
                d_losses.append(d_loss.item())
                wasserstein_distances.append(wd)
                gradient_penalties.append(gp.item())
                gradient_norms.append(last_grad_norm)  # NEW: Track gradient norms

                # Train Generator
                optimizer_G.zero_grad()
                z = torch.randn(current_batch_size, latent_dim).to(device)
                fake_labels_gen = torch.randint(0, config.num_classes, (current_batch_size,)).to(device)
                fake_imgs = generator(z, fake_labels_gen)

                g_loss = wgan_loss.generator_loss(critic, fake_imgs, fake_labels_gen)
                g_loss.backward()
                torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)
                optimizer_G.step()

                # Update EMA
                try:
                    ema_generator.update()
                except RuntimeError as e:
                    if "Expected all tensors to be on the same device" in str(e):
                        print(f"\n‚ö†Ô∏è  Device mismatch in EMA update! Fixing...")
                        for name, param in ema_generator.ema.shadow_params.items():
                            ema_generator.ema.shadow_params[name] = param.to(device)
                        print(f"‚úÖ EMA device fixed mid-training")
                        ema_generator.update()
                    else:
                        raise e

                g_losses.append(g_loss.item())

                # Calculate batch time
                batch_time = time.time() - batch_start_time
                batch_times.append(batch_time)
                progress_tracker.update_batch_time(batch_time)

                # Update batch progress bar
                current_lr_g = scheduler_G.get_last_lr()[0]
                current_lr_d = scheduler_D.get_last_lr()[0]

                batch_pbar.set_postfix({
                    'D_Loss': f'{d_loss.item():.4f}',
                    'G_Loss': f'{g_loss.item():.4f}',
                    'W_Dist': f'{wd:.4f}',
                    'GP': f'{gp.item():.3f}',
                    'GradNorm': f'{last_grad_norm:.3f}',  # NEW: Show gradient norm
                    'EMA_Q': f'{ema_generator.quality_score:.3f}',
                    'Time': f'{batch_time:.2f}s'
                })

                # Live terminal streaming every 25 steps
                if i % 25 == 0 or i < 5 or i == len(train_loader) - 1:
                    stats = {
                        'd_loss': d_loss.item(),
                        'g_loss': g_loss.item(),
                        'wd': wd,
                        'gp': gp.item(),
                        'grad_norm': last_grad_norm,  # NEW: Include gradient norm
                        'batch_time': batch_time,
                        'ema_quality': ema_generator.quality_score,
                        'lr_g': current_lr_g,
                        'lr_d': current_lr_d
                    }

                    print_enhanced_step_details(i+1, len(train_loader), stats, progress_tracker,
                                                epoch+1, num_epochs)

            # Close batch progress bar
            batch_pbar.close()
            print()

            # Epoch summary
            epoch_time = time.time() - epoch_start_time
            avg_d_loss = np.mean(d_losses)
            avg_g_loss = np.mean(g_losses)
            avg_wd = np.mean(wasserstein_distances)
            avg_gp = np.mean(gradient_penalties)
            avg_grad_norm = np.mean(gradient_norms)  # NEW: Average gradient norm
            avg_batch_time = np.mean(batch_times)

            # Prepare training stats for checkpoint manager
            training_stats = {
                'avg_d_loss': avg_d_loss,
                'avg_g_loss': avg_g_loss,
                'avg_wd': avg_wd,
                'avg_gp': avg_gp,
                'avg_grad_norm': avg_grad_norm,  # NEW: Include gradient norm
                'ema_quality': ema_generator.quality_score,
                'epoch_time': epoch_time,
                'avg_batch_time': avg_batch_time,
                'lr_g': scheduler_G.get_last_lr()[0],
                'lr_d': scheduler_D.get_last_lr()[0],
                'current_lambda_gp': wgan_loss.lambda_gp,
            }

            # Use the enhanced epoch summary function
            print_enhanced_epoch_summary(epoch + 1, training_stats)

            # Update learning rates
            scheduler_G.step()
            scheduler_D.step()
            new_lr_g = scheduler_G.get_last_lr()[0]
            new_lr_d = scheduler_D.get_last_lr()[0]

            # Log to tensorboard
            writer.add_scalar('Loss/Critic', avg_d_loss, epoch)
            writer.add_scalar('Loss/Generator', avg_g_loss, epoch)
            writer.add_scalar('WGAN/Wasserstein_Distance', avg_wd, epoch)
            writer.add_scalar('WGAN/Gradient_Penalty', avg_gp, epoch)
            writer.add_scalar('WGAN/Gradient_Norm', avg_grad_norm, epoch)  # NEW: Log gradient norm
            writer.add_scalar('WGAN/GP_Lambda', wgan_loss.lambda_gp, epoch)
            writer.add_scalar('EMA/Quality_Score', ema_generator.quality_score, epoch)
            writer.add_scalar('Learning_Rate/Generator', new_lr_g, epoch)
            writer.add_scalar('Learning_Rate/Critic', new_lr_d, epoch)

            # Update live plot
            try:
                live_plotter.update(epoch + 1, avg_d_loss, avg_g_loss, avg_wd, avg_gp,
                                    new_lr_g, new_lr_d, ema_generator.quality_score)
            except Exception as e:
                print(f"‚ö†Ô∏è  Live plot update failed: {e}")

            # üÜï SAVE CHECKPOINT EVERY 5 EPOCHS
            if (epoch + 1) % 5 == 0:
                print(f"üíæ Auto-saving checkpoint for epoch {epoch+1} (5-epoch interval)...")
                save_checkpoint_enhanced(
                    dataset_key, epoch + 1, generator, critic, ema_generator,
                    optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                    training_stats, "regular"
                )

            # Generate and save images every 5 epochs
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"üé® Generating sample images for epoch {epoch+1}...")
                save_enhanced_generated_images(epoch + 1, fixed_noise, fixed_labels,
                                               ema_generator, config, dataset_key)

            # Track best model
            if abs(avg_wd) < best_wasserstein_dist:
                best_wasserstein_dist = abs(avg_wd)
                print(f"üèÜ New best model! Wasserstein Distance: {best_wasserstein_dist:.6f}")
                save_checkpoint_enhanced(
                    dataset_key, epoch + 1, generator, critic, ema_generator,
                    optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                    training_stats, "best"
                )

            # Update epoch progress bar
            epoch_pbar.set_postfix({
                'D_Loss': f'{avg_d_loss:.4f}',
                'G_Loss': f'{avg_g_loss:.4f}',
                'W_Dist': f'{avg_wd:.4f}',
                'GradNorm': f'{avg_grad_norm:.3f}',  # NEW: Show gradient norm
                'EMA_Q': f'{ema_generator.quality_score:.3f}'
            })

            print("=" * 80)

    except KeyboardInterrupt:
        # This will be handled by the signal handler
        pass
    except Exception as e:
        # This will be handled by the error handler decorator
        raise e

    # Close epoch progress bar
    epoch_pbar.close()

    # Save final checkpoint
    print(f"\nüíæ Saving final checkpoint...")
    save_checkpoint_enhanced(
        dataset_key, epoch + 1, generator, critic, ema_generator,
        optimizer_G, optimizer_D, scheduler_G, scheduler_D,
        training_stats, "final"
    )

    # Training completion
    total_time = time.time() - start_time
    print(f"\nüéâ ENHANCED TRAINING COMPLETED WITH FIXED WGAN-GP!")
    print("="*80)
    print(f"‚è±Ô∏è Total Training Time: {total_time/60:.1f} minutes")
    print(f"üèÜ Best Wasserstein Distance: {best_wasserstein_dist:.6f}")
    print(f"üíú Final EMA Quality Score: {ema_generator.quality_score:.4f}")
    print(f"üìè Final Average Gradient Norm: {avg_grad_norm:.6f} (target: ~1.0)")
    print(f"üìä Total Batches Processed: {(epoch + 1 - start_epoch) * len(train_loader):,}")
    print(f"‚ö° Average Speed: {progress_tracker.get_avg_batch_time():.3f}s/batch")
    print(f"üñ•Ô∏è  Training Device: {device}")
    if start_epoch > 0:
        print(f"üîÑ Successfully resumed from epoch {start_epoch}")
    print("="*80)

    writer.close()
    return ema_generator, critic

# Enhanced image generation and saving
def save_enhanced_generated_images(epoch, fixed_noise, fixed_labels, ema_generator, config, dataset_key):
    """Save enhanced generated images with EMA - Works for both MNIST and CIFAR-10"""
    class_names = get_class_names(dataset_key)

    with torch.no_grad():
        # Generate with both regular and EMA generator
        regular_imgs = ema_generator.generator(fixed_noise, fixed_labels)
        ema_imgs = ema_generator.forward_with_ema(fixed_noise, fixed_labels)

        # Denormalize from [-1, 1] to [0, 1]
        regular_imgs = regular_imgs.cpu() * 0.5 + 0.5
        ema_imgs = ema_imgs.cpu() * 0.5 + 0.5

        # Clamp values to ensure they're in [0, 1] range
        regular_imgs = torch.clamp(regular_imgs, 0, 1)
        ema_imgs = torch.clamp(ema_imgs, 0, 1)

        # Create comparison grid
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

        # Regular generator samples
        regular_grid = torchvision.utils.make_grid(regular_imgs[:16], nrow=4, padding=2, normalize=False)

        if config.channels == 1:  # MNIST (Grayscale)
            # Handle different possible shapes from make_grid
            if regular_grid.dim() == 3:
                if regular_grid.size(0) == 3:
                    # make_grid created 3-channel image, convert to grayscale
                    regular_grid = regular_grid.mean(dim=0)
                elif regular_grid.size(0) == 1:
                    # make_grid created 1-channel image, squeeze
                    regular_grid = regular_grid.squeeze(0)
            ax1.imshow(regular_grid, cmap='gray', vmin=0, vmax=1)
        else:  # CIFAR-10 (RGB)
            # Convert from CHW to HWC format for matplotlib
            if regular_grid.dim() == 3 and regular_grid.size(0) == 3:
                regular_grid = regular_grid.permute(1, 2, 0)
            ax1.imshow(regular_grid)

        ax1.set_title(f'Regular Generator - Epoch {epoch}', fontsize=14, fontweight='bold')
        ax1.axis('off')

        # EMA generator samples
        ema_grid = torchvision.utils.make_grid(ema_imgs[:16], nrow=4, padding=2, normalize=False)

        if config.channels == 1:  # MNIST (Grayscale)
            # Handle different possible shapes from make_grid
            if ema_grid.dim() == 3:
                if ema_grid.size(0) == 3:
                    # make_grid created 3-channel image, convert to grayscale
                    ema_grid = ema_grid.mean(dim=0)
                elif ema_grid.size(0) == 1:
                    # make_grid created 1-channel image, squeeze
                    ema_grid = ema_grid.squeeze(0)
            ax2.imshow(ema_grid, cmap='gray', vmin=0, vmax=1)
        else:  # CIFAR-10 (RGB)
            # Convert from CHW to HWC format for matplotlib
            if ema_grid.dim() == 3 and ema_grid.size(0) == 3:
                ema_grid = ema_grid.permute(1, 2, 0)
            ax2.imshow(ema_grid)

        ax2.set_title(f'EMA Generator - Epoch {epoch}', fontsize=14, fontweight='bold')
        ax2.axis('off')

        # Add dataset info to the title
        dataset_info = f"{config.name} Dataset - {config.channels} Channel{'s' if config.channels > 1 else ''}"
        plt.suptitle(f'{dataset_info} Enhanced GAN Comparison - Epoch {epoch}', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # Save the comparison image
        save_path = f'./outputs/{dataset_key}_enhanced_comparison_epoch_{epoch}.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"   üíæ Saved comparison: {save_path}")
        plt.close()

        # Save detailed EMA samples in a larger grid
        fig, axes = plt.subplots(8, 8, figsize=(16, 16))

        for i, ax in enumerate(axes.flat):
            if i < len(ema_imgs):
                img = ema_imgs[i]
                label_idx = fixed_labels[i].item()

                if config.channels == 1:  # MNIST (Grayscale)
                    ax.imshow(img.squeeze(), cmap='gray', vmin=0, vmax=1)
                else:  # CIFAR-10 (RGB)
                    # Convert from CHW to HWC format
                    img = img.permute(1, 2, 0)
                    ax.imshow(img)

                ax.set_title(f'{class_names[label_idx]}', fontsize=8, fontweight='bold')
                ax.axis('off')
            else:
                # Hide unused subplots
                ax.axis('off')

        plt.suptitle(f'{config.name} EMA Generated Images - Epoch {epoch}', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # Save the detailed EMA samples
        save_path = f'./outputs/{dataset_key}_enhanced_ema_epoch_{epoch}.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"   üíæ Saved EMA samples: {save_path}")
        plt.close()

        print(f"   ‚úÖ Successfully generated and saved images for epoch {epoch}")

def generate_enhanced_specific_classes(ema_generator, config, dataset_key, classes, num_each=8):
    """Generate specific classes with enhanced quality"""
    class_names = get_class_names(dataset_key)

    # Set generator to eval mode for inference
    ema_generator.generator.eval()

    with torch.no_grad():
        all_imgs = []
        all_labels = []

        for class_idx in classes:
            z = torch.randn(num_each, 100).to(device)  # latent_dim = 100
            labels = torch.full((num_each,), class_idx).to(device)
            fake_imgs = ema_generator.forward_with_ema(z, labels)
            all_imgs.append(fake_imgs.cpu())
            all_labels.extend([class_idx] * num_each)

        all_imgs = torch.cat(all_imgs)
        all_imgs = all_imgs * 0.5 + 0.5

        # Plot with enhanced layout
        rows = len(classes)
        fig, axes = plt.subplots(rows, num_each, figsize=(num_each*2.5, rows*2.5))
        if rows == 1:
            axes = axes.reshape(1, -1)

        idx = 0
        for i in range(rows):
            for j in range(num_each):
                if config.channels == 1:
                    axes[i, j].imshow(all_imgs[idx].squeeze(), cmap='gray')
                else:
                    axes[i, j].imshow(all_imgs[idx].permute(1, 2, 0))
                axes[i, j].set_title(f'{class_names[all_labels[idx]]}', fontsize=10)
                axes[i, j].axis('off')
                idx += 1

        plt.suptitle(f'{config.name} - Enhanced Generated Specific Classes (EMA)', fontsize=16)
        plt.tight_layout()
        plt.show()

    # Restore training mode
    ema_generator.generator.train()

def enhanced_interpolate_latent_space(ema_generator, config, dataset_key, class1, class2, steps=10):
    """Enhanced latent space interpolation with EMA"""
    class_names = get_class_names(dataset_key)

    # Set generator to eval mode for inference
    ema_generator.generator.eval()

    with torch.no_grad():
        z1 = torch.randn(1, 100).to(device)  # latent_dim = 100
        z2 = torch.randn(1, 100).to(device)

        interpolated_imgs = []
        for i in range(steps):
            alpha = i / (steps - 1)
            z_interp = (1 - alpha) * z1 + alpha * z2

            # Generate images for both classes using EMA
            img1 = ema_generator.forward_with_ema(z_interp, torch.tensor([class1]).to(device))
            img2 = ema_generator.forward_with_ema(z_interp, torch.tensor([class2]).to(device))

            interpolated_imgs.append((img1.cpu(), img2.cpu()))

        # Enhanced plot
        fig, axes = plt.subplots(2, steps, figsize=(steps*2.5, 6))
        for i, (img1, img2) in enumerate(interpolated_imgs):
            img1 = img1 * 0.5 + 0.5
            img2 = img2 * 0.5 + 0.5

            if config.channels == 1:
                axes[0, i].imshow(img1.squeeze(), cmap='gray')
                axes[1, i].imshow(img2.squeeze(), cmap='gray')
            else:
                axes[0, i].imshow(img1.squeeze().permute(1, 2, 0))
                axes[1, i].imshow(img2.squeeze().permute(1, 2, 0))

            axes[0, i].set_title(f'{class_names[class1]}', fontsize=10)
            axes[1, i].set_title(f'{class_names[class2]}', fontsize=10)
            axes[0, i].axis('off')
            axes[1, i].axis('off')

        plt.suptitle(f'{config.name} Enhanced Latent Interpolation: {class_names[class1]} ‚Üî {class_names[class2]}',
                     fontsize=16)
        plt.tight_layout()
        plt.show()

    # Restore training mode if it was in training mode
    ema_generator.generator.train()

# =============================================================================
# ENHANCED MAIN EXECUTION WITH CHECKPOINT SUPPORT
# =============================================================================

# Modified main function to integrate all changes
def main_with_checkpoint_support_enhanced():
    """Enhanced main function with complete checkpoint integration"""

    print("=" * 100)
    print("üé® ENHANCED MULTI-DATASET DCGAN WITH ADVANCED CHECKPOINTING")
    print("=" * 100)
    print("üöÄ Advanced Features:")
    print("1. ‚úÖ WGAN-GP Loss with Gradient Penalty")
    print("2. ‚úÖ EMA (Exponential Moving Average)")
    print("3. ‚úÖ Enhanced Generator/Critic Architecture")
    print("4. ‚úÖ Spectral Normalization")
    print("5. ‚úÖ Progressive Learning Rate Scheduling")
    print("6. ‚úÖ Real-time Enhanced Monitoring")
    print("7. ‚úÖ Live Progress Bars & Terminal Streaming")
    print("8. ‚úÖ Comprehensive Training Analytics")
    print("9. ‚úÖ Multi-Dataset Support (MNIST & CIFAR-10)")
    print("10. ‚úÖ Checkpoint Resume Capability")
    print("11. ‚úÖ üÜï AUTO-SAVE EVERY 5 EPOCHS")
    print("12. ‚úÖ üÜï GRACEFUL INTERRUPT HANDLING (Ctrl+C)")
    print("13. ‚úÖ üÜï EMERGENCY ERROR RECOVERY")
    print("=" * 100)

    # Check for tqdm availability
    if not TQDM_AVAILABLE:
        print("\n‚ö†Ô∏è  NOTICE: For the best experience with progress bars, install tqdm:")
        print("   pip install tqdm")
        print("   (Fallback progress bars will be used)")
        time.sleep(2)

    # Get user's dataset choice
    dataset_key = get_dataset_choice()
    config = DATASETS[dataset_key]

    # Ask if user wants to resume from checkpoint
    print(f"\nüîÑ CHECKPOINT OPTIONS FOR {config.name}:")
    print("1. üìã List all available checkpoints")
    print("2. üîÑ Resume from checkpoint")
    print("3. üÜï Start fresh training")

    while True:
        try:
            choice = input(f"\nSelect option (1-3): ").strip()
            if choice == '1':
                list_all_checkpoints()
                continue
            elif choice == '2':
                resume_from_checkpoint = True
                break
            elif choice == '3':
                resume_from_checkpoint = False
                break
            else:
                print("Please enter 1, 2, or 3")
        except KeyboardInterrupt:
            print("\nExiting...")
            exit()

    if not resume_from_checkpoint:
        print("üÜï Starting fresh training...")

    # Display enhancement details
    display_enhancement_details(dataset_key)

    print(f"\nüöÄ Starting enhanced automated workflow for {config.name}...")

    # Create enhanced directory structure
    directories = [
        './outputs', './models', f'./outputs/{dataset_key}', f'./models/{dataset_key}',
        f'./outputs/{dataset_key}/enhanced', f'./models/{dataset_key}/enhanced',
        f'./models/{dataset_key}/emergency'  # Add emergency directory
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)

    print(f"üìÅ Created enhanced directory structure for {config.name}")
    print(f"üö® Emergency checkpoint directory: ./models/{dataset_key}/emergency/")

    # Load dataset with appropriate transforms
    print(f"\nüì• Loading {config.name} dataset with progress tracking...")
    transform = get_transforms(dataset_key)

    # Show dataset loading progress
    with tqdm(total=1, desc="üì¶ Downloading Dataset", ncols=80, colour='cyan') as pbar:
        train_dataset = get_dataset(dataset_key, transform)
        pbar.update(1)
        pbar.set_description("‚úÖ Dataset Downloaded")

    # Create data loader with progress indication
    print(f"üîÑ Creating optimized data loader...")
    global train_loader
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=128, shuffle=True, num_workers=2
    )

    print(f"‚úÖ Dataset loaded successfully!")
    print(f"   üìä Training samples: {len(train_dataset):,}")
    print(f"   üì¶ Batch size: 128")
    print(f"   üîÑ Total batches per epoch: {len(train_loader):,}")

    # Display enhanced sample from dataset
    print(f"\nüìä Generating enhanced dataset preview...")
    sample_batch = next(iter(train_loader))
    sample_imgs, sample_labels = sample_batch
    class_names = get_class_names(dataset_key)

    # Create preview with progress bar
    with tqdm(total=10, desc="üñºÔ∏è  Creating Preview", ncols=80, colour='magenta') as pbar:
        fig, axes = plt.subplots(2, 5, figsize=(15, 8))
        for i in range(10):
            row, col = i // 5, i % 5
            img = sample_imgs[i]
            if config.channels == 1:
                axes[row, col].imshow(img.squeeze(), cmap='gray')
            else:
                # Denormalize for display
                img = img * 0.5 + 0.5
                axes[row, col].imshow(img.permute(1, 2, 0))
            axes[row, col].set_title(f'{class_names[sample_labels[i]]}', fontsize=12)
            axes[row, col].axis('off')
            pbar.update(1)

    plt.suptitle(f'{config.name} Dataset Sample - Enhanced Preview', fontsize=16)
    plt.tight_layout()
    plt.show()

    print(f"\nüèóÔ∏è  Initializing enhanced {config.name}-specific model architecture...")

    # Model initialization progress
    with tqdm(total=4, desc="ü§ñ Initializing Models", ncols=80, colour='yellow') as pbar:
        pbar.set_description("ü§ñ Creating Generator")
        time.sleep(0.5)
        pbar.update(1)

        pbar.set_description("ü§ñ Creating Critic")
        time.sleep(0.5)
        pbar.update(1)

        pbar.set_description("ü§ñ Applying Enhancements")
        time.sleep(0.5)
        pbar.update(1)

        pbar.set_description("‚úÖ Models Ready")
        pbar.update(1)

    print(f"\nüõ°Ô∏è  ENHANCED CHECKPOINT SYSTEM ACTIVE:")
    print(f"   üîÑ Auto-save every 5 epochs")
    print(f"   ‚ö° Graceful shutdown on Ctrl+C")
    print(f"   üö® Emergency checkpoint on errors")
    print(f"   üìÅ Emergency saves: ./models/{dataset_key}/emergency/")
    print(f"   üîí Signal handling for clean interrupts")

    print(f"\nüéØ Beginning enhanced automated training process...")
    print(f"‚ö†Ô∏è  Training will show live progress bars and streaming updates")
    print(f"üí° Press Ctrl+C anytime for graceful shutdown with checkpoint saving")

    # Countdown to training start
    for i in range(3, 0, -1):
        print(f"üöÄ Starting training in {i}...", end='\r', flush=True)
        time.sleep(1)
    print("üöÄ Starting training now! " + " " * 20)

    # Start enhanced training with all checkpoint features
    ema_generator, critic = train_enhanced_gan_with_resume_modified(dataset_key, config, resume_from_checkpoint)

    # Post-training activities (same as original)
    print(f"\nüé® Generating enhanced final examples for {config.name}...")

    # Generate samples for all classes
    print("üé≠ Creating class-specific samples...")
    all_classes = list(range(config.num_classes))

    with tqdm(total=len(all_classes), desc="üé® Generating Classes", ncols=80, colour='green') as pbar:
        for class_idx in all_classes:
            pbar.set_description(f"üé® Generating {get_class_names(dataset_key)[class_idx]}")
            time.sleep(0.1)
            pbar.update(1)

    generate_enhanced_specific_classes(ema_generator, config, dataset_key, all_classes, num_each=6)

    # Show enhanced interpolation
    print(f"\nüåà Creating enhanced latent space interpolations...")

    if dataset_key == 'mnist':
        interpolations = [(3, 8, "3‚Üí8"), (0, 6, "0‚Üí6"), (4, 9, "4‚Üí9")]
    else:  # cifar10
        interpolations = [(3, 5, "cat‚Üídog"), (0, 1, "plane‚Üícar"), (2, 7, "bird‚Üíhorse")]

    with tqdm(total=len(interpolations), desc="üåà Creating Interpolations", ncols=80, colour='magenta') as pbar:
        for class1, class2, desc in interpolations:
            pbar.set_description(f"üåà Interpolating {desc}")
            enhanced_interpolate_latent_space(ema_generator, config, dataset_key, class1, class2, steps=10)
            pbar.update(1)
            time.sleep(0.5)

    # Final comprehensive summary
    print(f"\n" + "="*100)
    print("üéâ ENHANCED AUTOMATED WORKFLOW WITH ADVANCED CHECKPOINTING COMPLETED!")
    print("="*100)

    summary_items = [
        "üéØ Dataset Processing",
        "üîÑ Advanced Checkpoint Management",
        "ü§ñ Model Training with Auto-Save",
        "üõ°Ô∏è  Graceful Interrupt Handling",
        "üé® Sample Generation",
        "üåà Interpolation Creation",
        "üìä Quality Assessment",
        "üíæ Comprehensive Model Saving"
    ]

    print("üìã WORKFLOW COMPLETION SUMMARY:")
    for item in summary_items:
        print(f"   ‚úÖ {item}")
        time.sleep(0.2)

    print(f"\nüìä FINAL TRAINING STATISTICS:")
    print(f"   üéØ Dataset: {config.name}")
    print(f"   üìà Final EMA Quality Score: {ema_generator.quality_score:.4f}")
    print(f"   üñ•Ô∏è  Training Device: {device}")
    print(f"   üî¢ Total Parameters: {sum(p.numel() for p in ema_generator.generator.parameters()):,}")

    print(f"\nüìÅ All enhanced outputs saved to:")
    print(f"   üñºÔ∏è  Enhanced Images: ./outputs/{dataset_key}/enhanced/")
    print(f"   ü§ñ Enhanced Models: ./models/{dataset_key}/enhanced/")
    print(f"   üì¶ Regular Checkpoints: ./models/{dataset_key}_enhanced_epoch_*.pth")
    print(f"   üö® Emergency Checkpoints: ./models/{dataset_key}/emergency/")
    if TENSORBOARD_AVAILABLE:
        print(f"   üìä TensorBoard Logs: ./runs/{dataset_key}_enhanced_gan*/")

    print(f"\nüöÄ Enhanced Features Successfully Implemented:")
    enhancements = [
        "WGAN-GP Loss - Better convergence and stability",
        "EMA Generator - Improved sample quality",
        "Enhanced Architecture - Better gradient flow",
        "Spectral Normalization - Stable discriminator training",
        "Progressive LR Scheduling - Adaptive learning",
        "Advanced Monitoring - Real-time health tracking",
        "Live Progress Bars - Enhanced user experience",
        "Terminal Streaming - Detailed step analysis",
        "Checkpoint Resume - Continue from any saved state",
        "üÜï Auto-Save Every 5 Epochs - Frequent progress saves",
        "üÜï Graceful Interrupt (Ctrl+C) - Safe shutdown",
        "üÜï Emergency Error Recovery - Crash-proof training"
    ]

    for i, enhancement in enumerate(enhancements, 1):
        print(f"   ‚úÖ {i:2d}. {enhancement}")

    print(f"\nüí° Enhanced Next Steps:")
    next_steps = [
        f"Load final model: ./models/{dataset_key}/enhanced/final_enhanced_model.pth",
        f"Resume from any checkpoint: Select from available saves",
        f"Emergency recovery: Check ./models/{dataset_key}/emergency/ if needed",
        "Generate with enhanced quality using EMA parameters",
        "Explore enhanced latent space interpolations"
    ]

    for step in next_steps:
        print(f"   üí° {step}")

    print(f"\nüõ°Ô∏è  CHECKPOINT MANAGEMENT COMMANDS:")
    print(f"   üìã List all checkpoints: python script.py list")
    print(f"   üöÄ Quick resume latest: python script.py resume {dataset_key}")
    print(f"   üîç Interactive mode: python script.py")
    print(f"   üö® Emergency recovery: Check emergency directory for crash saves")

    if TENSORBOARD_AVAILABLE:
        print(f"\nüìä View enhanced training metrics:")
        print(f"   tensorboard --logdir ./runs/")
        print(f"   Then open: http://localhost:6006")

    print("="*100)
    print("\nüñ•Ô∏è  Enhanced live training plot will remain open.")
    print("üí° All progress bars and monitoring are now complete!")
    print("üõ°Ô∏è  Emergency checkpoint system remains active!")
    print("üéä Thank you for using Enhanced Multi-Dataset DCGAN with Advanced Checkpointing!")
    print("Press Ctrl+C to exit the program gracefully.")

    try:
        print("\n‚è≥ Keeping plots alive... (Press Ctrl+C to exit)")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n\nüëã Graceful exit completed! Goodbye!")
        print("üéâ Training session completed successfully!")
        plt.close('all')

# =============================================================================
# COMMAND LINE INTERFACE WITH CHECKPOINT SUPPORT
# =============================================================================
#
# # Updated command line interface
# if __name__ == "__main__":
#     import sys
#
#     # Command line arguments for quick operations
#     if len(sys.argv) > 1:
#         command = sys.argv[1].lower()
#
#         if command == "list":
#             list_all_checkpoints()
#         elif command == "resume" and len(sys.argv) > 2:
#             dataset_key = sys.argv[2].lower()
#             if dataset_key in ['mnist', 'cifar10']:
#                 checkpoint_path = quick_resume_latest(dataset_key)
#                 if checkpoint_path:
#                     print(f"üöÄ Quick resuming from: {os.path.basename(checkpoint_path)}")
#                     main_with_checkpoint_support_enhanced()
#                 else:
#                     print("‚ùå No checkpoints found for quick resume")
#             else:
#                 print("‚ùå Invalid dataset. Use 'mnist' or 'cifar10'")
#         elif command == "help":
#             print("\nüé® Enhanced DCGAN with Advanced Checkpointing - Command Line Help")
#             print("=" * 70)
#             print("Usage:")
#             print("  python script.py                         # Interactive mode")
#             print("  python script.py list                    # List all checkpoints")
#             print("  python script.py resume mnist            # Quick resume latest MNIST")
#             print("  python script.py resume cifar10          # Quick resume latest CIFAR-10")
#             print("  python script.py help                    # Show this help")
#             print("\nüõ°Ô∏è  Advanced Checkpoint Features:")
#             print("  ‚Ä¢ Auto-save every 5 epochs (instead of 25)")
#             print("  ‚Ä¢ Graceful interrupt handling (Ctrl+C)")
#             print("  ‚Ä¢ Emergency checkpoint on unexpected errors")
#             print("  ‚Ä¢ Signal handling for clean shutdowns")
#             print("  ‚Ä¢ Comprehensive state preservation")
#             print("\nüîÑ Interactive Mode Features:")
#             print("  ‚Ä¢ Choose dataset (MNIST or CIFAR-10)")
#             print("  ‚Ä¢ List and select from available checkpoints")
#             print("  ‚Ä¢ Resume from any epoch or start fresh")
#             print("  ‚Ä¢ Live progress monitoring and health assessment")
#             print("  ‚Ä¢ Real-time interrupt handling")
#             print("\nüìÅ Checkpoint Types:")
#             print("  ‚Ä¢ Regular: ./models/{dataset}_enhanced_epoch_{epoch}.pth")
#             print("  ‚Ä¢ Best: ./models/{dataset}_best_enhanced_model.pth")
#             print("  ‚Ä¢ Final: ./models/{dataset}/enhanced/final_enhanced_model.pth")
#             print("  ‚Ä¢ Emergency: ./models/{dataset}/emergency/{timestamp}.pth")
#             print("\nüö® Emergency Features:")
#             print("  ‚Ä¢ Automatic save on Ctrl+C")
#             print("  ‚Ä¢ Crash recovery checkpoints")
#             print("  ‚Ä¢ Signal handling (SIGINT, SIGTERM)")
#             print("  ‚Ä¢ Error traceback with state preservation")
#             print("=" * 70)
#         else:
#             print("‚ùå Unknown command. Use 'help' for usage information.")
#     else:
#         # Interactive mode with enhanced features
#         main_with_checkpoint_support_enhanced()