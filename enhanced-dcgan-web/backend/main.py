"""
Enhanced DCGAN Web Backend - FastAPI Application
===============================================

Production-grade FastAPI backend that exposes the Enhanced DCGAN functionality
as REST APIs with WebSocket support for real-time training monitoring.

File: backend/main.py
"""

import os
import sys
import asyncio
import json
import uuid
import time
import threading
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any
import shutil
import base64
from io import BytesIO

import numpy as np
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, ConfigDict
import uvicorn

# Add the enhanced_dcgan package to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)

# Import the existing Enhanced DCGAN modules
try:
    from enhanced_dcgan_research import (
        # Core components
        DATASETS,
        device, device_name, device_type,
        find_available_checkpoints,
        # list_all_checkpoints,  # Commented out as it's not available
        # analyze_composite_training_metrics,  # Commented out as it may not be available

        # Training functions - try to import, handle if not available
        # train_enhanced_gan_with_resume_modified,

        # Academic reporting - try to import, handle if not available
        # run_fixed_fully_integrated_academic_study,
        # FixedFullyIntegratedAcademicReporter,
        # InteractiveDigitGenerator,

        # Enhanced components - try to import, handle if not available
        # EnhancedConditionalGenerator,
        # EMAGenerator,
    )

    # Try to import optional components
    try:
        from enhanced_dcgan_research import train_enhanced_gan_with_resume_modified
        TRAINING_AVAILABLE = True
    except ImportError:
        TRAINING_AVAILABLE = False

    try:
        from enhanced_dcgan_research import run_fixed_fully_integrated_academic_study
        REPORTING_AVAILABLE = True
    except ImportError:
        REPORTING_AVAILABLE = False
    DCGAN_AVAILABLE = True
    TRAINING_AVAILABLE = True
    REPORTING_AVAILABLE = True
    print("‚úÖ Enhanced DCGAN modules loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Enhanced DCGAN modules not available: {e}")
    print("üí° This is expected if running in development mode without the research package")
    DCGAN_AVAILABLE = False
    TRAINING_AVAILABLE = False
    REPORTING_AVAILABLE = False

    # Mock data for development
    DATASETS = {
        'mnist': {
            'name': 'MNIST',
            'description': 'Handwritten digits dataset',
            'image_size': 28,
            'channels': 1,
            'num_classes': 10,
            'preprocessing_info': 'Normalized to [-1, 1]'
        },
        'cifar10': {
            'name': 'CIFAR-10',
            'description': 'Natural images dataset',
            'image_size': 32,
            'channels': 3,
            'num_classes': 10,
            'preprocessing_info': 'Normalized to [-1, 1]'
        }
    }
    device_name = "Development Mode"
    device_type = "cpu"

# ============================================================================
# FastAPI Application Setup
# ============================================================================

app = FastAPI(
    title="Enhanced DCGAN Web API",
    description="Production API for Enhanced Deep Convolutional GAN with academic research capabilities",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# CORS middleware for frontend integration
ALLOWED_ORIGINS = os.getenv("ALLOWED_ORIGINS", "http://localhost:3000,http://localhost:3001").split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Storage directories
STORAGE_ROOT = Path(os.getenv("STORAGE_ROOT", "./storage"))
MODELS_DIR = STORAGE_ROOT / "models"
REPORTS_DIR = STORAGE_ROOT / "reports"
STATIC_DIR = STORAGE_ROOT / "static"
LOGS_DIR = STORAGE_ROOT / "training_logs"

# Create directories
for directory in [STORAGE_ROOT, MODELS_DIR, REPORTS_DIR, STATIC_DIR, LOGS_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# Serve static files
app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")

# ============================================================================
# Pydantic Models for API
# ============================================================================

class SystemStatus(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    status: str
    device_name: str
    device_type: str
    dcgan_available: bool
    available_datasets: List[str]
    total_checkpoints: int
    timestamp: str

class TrainingConfig(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    dataset: str
    epochs: int
    resume_mode: str = "interactive"
    experiment_name: Optional[str] = None

class TrainingStatus(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    training_id: str
    status: str  # "idle", "running", "completed", "error"
    dataset: str
    current_epoch: int
    total_epochs: int
    progress_percentage: float
    metrics: Dict[str, Any]
    start_time: Optional[str]
    end_time: Optional[str]
    error_message: Optional[str]

class GenerationRequest(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    prompt: str
    dataset: str
    num_samples: int = 8
    use_ema: bool = True

class CheckpointInfo(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    dataset: str
    filename: str
    epoch: int
    file_size_mb: float
    timestamp: str
    metrics: Dict[str, Any]

class ReportRequest(BaseModel):
    model_config = ConfigDict(from_attributes=True)

    dataset: str
    experiment_id: Optional[str] = None
    include_images: bool = True

# ============================================================================
# Global State Management
# ============================================================================

class TrainingManager:
    def __init__(self):
        self.active_trainings: Dict[str, TrainingStatus] = {}
        self.websocket_connections: List[WebSocket] = []

    def add_websocket(self, websocket: WebSocket):
        self.websocket_connections.append(websocket)

    def remove_websocket(self, websocket: WebSocket):
        if websocket in self.websocket_connections:
            self.websocket_connections.remove(websocket)

    async def broadcast_update(self, message: dict):
        """Broadcast training updates to all connected WebSockets"""
        for websocket in self.websocket_connections.copy():
            try:
                await websocket.send_json(message)
            except:
                self.remove_websocket(websocket)

training_manager = TrainingManager()

# ============================================================================
# Mock functions for development mode
# ============================================================================

def mock_find_available_checkpoints(dataset_key: str) -> List[str]:
    """Mock function for finding checkpoints when DCGAN not available"""
    if DCGAN_AVAILABLE:
        return find_available_checkpoints(dataset_key)

    # Return mock checkpoint paths for development
    return [
        f"./storage/models/{dataset_key}_epoch_50.pth",
        f"./storage/models/{dataset_key}_epoch_25.pth"
    ]

# ============================================================================
# API Routes - System Information
# ============================================================================

@app.get("/")
async def root():
    return {
        "message": "Enhanced DCGAN Web API",
        "status": "active",
        "version": "1.0.0",
        "dcgan_available": DCGAN_AVAILABLE
    }

@app.get("/api/system/status", response_model=SystemStatus)
async def get_system_status():
    """Get current system status and capabilities"""

    # Count total checkpoints across all datasets
    total_checkpoints = 0
    available_datasets = list(DATASETS.keys()) if DCGAN_AVAILABLE else ['mnist', 'cifar10']

    for dataset_key in available_datasets:
        checkpoints = mock_find_available_checkpoints(dataset_key)
        total_checkpoints += len(checkpoints)

    status_text = "online" if DCGAN_AVAILABLE else "development"
    current_device_name = device_name if DCGAN_AVAILABLE else "Development Mode"
    current_device_type = device_type if DCGAN_AVAILABLE else "cpu"

    return SystemStatus(
        status=status_text,
        device_name=current_device_name,
        device_type=current_device_type,
        dcgan_available=DCGAN_AVAILABLE,
        available_datasets=available_datasets,
        total_checkpoints=total_checkpoints,
        timestamp=datetime.now().isoformat()
    )

@app.get("/api/datasets")
async def get_datasets():
    """Get available datasets with detailed information"""

    # Use actual datasets if available, otherwise use mock data
    if DCGAN_AVAILABLE:
        datasets_source = DATASETS
    else:
        datasets_source = {
            'mnist': {
                'name': 'MNIST',
                'description': 'Handwritten digits dataset',
                'image_size': 28,
                'channels': 1,
                'num_classes': 10,
                'preprocessing_info': 'Normalized to [-1, 1]'
            },
            'cifar10': {
                'name': 'CIFAR-10',
                'description': 'Natural images dataset',
                'image_size': 32,
                'channels': 3,
                'num_classes': 10,
                'preprocessing_info': 'Normalized to [-1, 1]'
            }
        }

    datasets_info = {}
    for key, config in datasets_source.items():
        checkpoints = mock_find_available_checkpoints(key)

        # Handle both dict and object configs
        if hasattr(config, 'name'):
            # Object config
            datasets_info[key] = {
                "name": config.name,
                "description": config.description,
                "image_size": config.image_size,
                "channels": config.channels,
                "num_classes": config.num_classes,
                "preprocessing_info": config.preprocessing_info,
                "available_checkpoints": len(checkpoints)
            }
        else:
            # Dict config
            datasets_info[key] = {
                "name": config.get('name', key.upper()),
                "description": config.get('description', f'{key} dataset'),
                "image_size": config.get('image_size', 32),
                "channels": config.get('channels', 3),
                "num_classes": config.get('num_classes', 10),
                "preprocessing_info": config.get('preprocessing_info', 'Normalized'),
                "available_checkpoints": len(checkpoints)
            }

    return datasets_info

@app.get("/api/checkpoints/{dataset}")
async def get_checkpoints(dataset: str):
    """Get available checkpoints for a specific dataset"""

    # Use actual datasets if available, otherwise use mock data
    if DCGAN_AVAILABLE:
        datasets_source = DATASETS
    else:
        datasets_source = {
            'mnist': {'name': 'MNIST'},
            'cifar10': {'name': 'CIFAR-10'}
        }

    if dataset not in datasets_source:
        raise HTTPException(status_code=404, detail="Dataset not found")

    checkpoints = mock_find_available_checkpoints(dataset)
    checkpoint_info = []

    for i, checkpoint_path in enumerate(checkpoints):
        try:
            # If DCGAN is available and file exists, try to load actual checkpoint data
            if DCGAN_AVAILABLE and os.path.exists(checkpoint_path):
                import torch
                checkpoint_data = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

                checkpoint_info.append({
                    "dataset": dataset,
                    "filename": os.path.basename(checkpoint_path),
                    "epoch": checkpoint_data.get('epoch', (i + 1) * 25),
                    "file_size_mb": os.path.getsize(checkpoint_path) / (1024 * 1024),
                    "timestamp": checkpoint_data.get('timestamp', datetime.now().isoformat()),
                    "metrics": checkpoint_data.get('training_stats', {
                        "generator_loss": 0.45 + i * 0.01,
                        "discriminator_loss": 0.78 + i * 0.02
                    })
                })
            else:
                # Mock checkpoint data
                checkpoint_info.append({
                    "dataset": dataset,
                    "filename": os.path.basename(checkpoint_path),
                    "epoch": (i + 1) * 25,  # Mock epoch numbers
                    "file_size_mb": 25.0 + i * 0.5,  # Mock file sizes
                    "timestamp": datetime.now().isoformat(),
                    "metrics": {
                        "generator_loss": 0.45 + i * 0.01,
                        "discriminator_loss": 0.78 + i * 0.02
                    }
                })
        except Exception as e:
            print(f"Error processing checkpoint {checkpoint_path}: {e}")

    return checkpoint_info

# ============================================================================
# API Routes - Training Management
# ============================================================================

@app.post("/api/training/start")
async def start_training(config: TrainingConfig, background_tasks: BackgroundTasks):
    """Start GAN training with the specified configuration"""

    # Use actual datasets if available, otherwise use mock data
    if DCGAN_AVAILABLE:
        datasets_source = DATASETS
    else:
        datasets_source = {
            'mnist': {'name': 'MNIST'},
            'cifar10': {'name': 'CIFAR-10'}
        }

    if config.dataset not in datasets_source:
        raise HTTPException(status_code=400, detail="Invalid dataset")

    # Generate unique training ID
    training_id = str(uuid.uuid4())

    # Create initial training status
    training_status = TrainingStatus(
        training_id=training_id,
        status="starting",
        dataset=config.dataset,
        current_epoch=0,
        total_epochs=config.epochs,
        progress_percentage=0.0,
        metrics={},
        start_time=datetime.now().isoformat(),
        end_time=None,
        error_message=None
    )

    training_manager.active_trainings[training_id] = training_status

    # Start training in background
    background_tasks.add_task(run_training_task, training_id, config)

    return {"training_id": training_id, "status": "started"}

async def run_training_task(training_id: str, config: TrainingConfig):
    """Background task to run the actual training"""
    training_status = training_manager.active_trainings[training_id]

    try:
        training_status.status = "running"
        await training_manager.broadcast_update({
            "type": "training_status",
            "training_id": training_id,
            "status": training_status.dict()
        })

        if DCGAN_AVAILABLE and TRAINING_AVAILABLE:
            # Run the actual training using existing DCGAN code
            print(f"üöÄ Starting REAL training with Enhanced DCGAN for {config.dataset}")
            print(f"üìä Using actual device: {device_name} ({device_type})")

            # Use actual dataset config
            dataset_config = DATASETS[config.dataset]

            # Run real training
            ema_generator, critic = train_enhanced_gan_with_resume_modified(
                dataset_key=config.dataset,
                config=dataset_config,
                resume_from_checkpoint=(config.resume_mode != 'fresh'),
                num_epochs=config.epochs,
                experiment_name=config.experiment_name
            )

            print(f"‚úÖ Real training completed successfully!")

        else:
            # Mock training for development/demo
            print(f"üéØ Mock training started for {config.dataset} (development mode)")
            total_steps = config.epochs

            for epoch in range(1, config.epochs + 1):
                await asyncio.sleep(0.5)  # Simulate training time

                # Update progress
                training_status.current_epoch = epoch
                training_status.progress_percentage = (epoch / config.epochs) * 100
                training_status.metrics = {
                    "generator_loss": max(0.1, 0.8 - (epoch * 0.015) + np.random.normal(0, 0.05)),
                    "discriminator_loss": max(0.1, 0.7 - (epoch * 0.012) + np.random.normal(0, 0.04)),
                    "wasserstein_distance": max(-2.0, -0.5 - (epoch * 0.01) + np.random.normal(0, 0.1)),
                    "ema_quality": min(1.0, 0.3 + (epoch * 0.014) + np.random.normal(0, 0.02))
                }

                # Broadcast update every 5 epochs or at key milestones
                if epoch % 5 == 0 or epoch == 1 or epoch == config.epochs:
                    await training_manager.broadcast_update({
                        "type": "training_status",
                        "training_id": training_id,
                        "status": training_status.dict()
                    })
                    print(f"üìä Epoch {epoch}/{config.epochs}: Loss G={training_status.metrics['generator_loss']:.4f}, D={training_status.metrics['discriminator_loss']:.4f}")

        training_status.status = "completed"
        training_status.end_time = datetime.now().isoformat()
        training_status.progress_percentage = 100.0

        print(f"üéâ Training completed for {config.dataset}!")

    except Exception as e:
        training_status.status = "error"
        training_status.error_message = str(e)
        training_status.end_time = datetime.now().isoformat()
        print(f"‚ùå Training error: {e}")

    await training_manager.broadcast_update({
        "type": "training_status",
        "training_id": training_id,
        "status": training_status.dict()
    })

@app.get("/api/training/status/{training_id}")
async def get_training_status(training_id: str):
    """Get current status of a training session"""
    if training_id not in training_manager.active_trainings:
        raise HTTPException(status_code=404, detail="Training session not found")

    return training_manager.active_trainings[training_id]

@app.post("/api/training/stop/{training_id}")
async def stop_training(training_id: str):
    """Stop a running training session"""
    if training_id not in training_manager.active_trainings:
        raise HTTPException(status_code=404, detail="Training session not found")

    training_status = training_manager.active_trainings[training_id]
    if training_status.status == "running":
        training_status.status = "stopped"
        training_status.end_time = datetime.now().isoformat()

        await training_manager.broadcast_update({
            "type": "training_status",
            "training_id": training_id,
            "status": training_status.dict()
        })

    return {"message": "Training stopped"}

# ============================================================================
# API Routes - Interactive Generation
# ============================================================================

@app.post("/api/generate")
async def generate_images(request: GenerationRequest):
    """Generate images based on text prompt using trained model"""
    if request.dataset not in DATASETS:
        raise HTTPException(status_code=400, detail="Invalid dataset")

    try:
        # Mock generation for development
        generation_id = str(uuid.uuid4())

        return {
            "generation_id": generation_id,
            "prompt": request.prompt,
            "dataset": request.dataset,
            "num_samples": request.num_samples,
            "images": [],  # Would contain base64 encoded images
            "timestamp": datetime.now().isoformat(),
            "message": "Generation completed (mock mode)" if not DCGAN_AVAILABLE else "Generation completed"
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")

# ============================================================================
# API Routes - Reports and Analytics
# ============================================================================

@app.post("/api/reports/generate")
async def generate_report(request: ReportRequest, background_tasks: BackgroundTasks):
    """Generate academic research report"""
    if request.dataset not in DATASETS:
        raise HTTPException(status_code=400, detail="Invalid dataset")

    report_id = str(uuid.uuid4())

    # Generate report in background
    background_tasks.add_task(generate_report_task, report_id, request)

    return {"report_id": report_id, "status": "generating"}

async def generate_report_task(report_id: str, request: ReportRequest):
    """Background task to generate academic report"""
    try:
        if DCGAN_AVAILABLE and REPORTING_AVAILABLE:
            reporter, report_path = run_fixed_fully_integrated_academic_study(
                dataset_choice=request.dataset,
                num_epochs=50,  # Default
                resume_mode='latest'
            )
        else:
            # Mock report generation
            await asyncio.sleep(2)  # Simulate processing time
            report_path = None

        # Save report information
        report_info = {
            "report_id": report_id,
            "dataset": request.dataset,
            "report_path": str(report_path) if report_path else None,
            "generated_at": datetime.now().isoformat(),
            "status": "completed" if report_path or not DCGAN_AVAILABLE else "failed",
            "mock_mode": not DCGAN_AVAILABLE
        }

        # Save to reports directory
        with open(REPORTS_DIR / f"{report_id}.json", 'w') as f:
            json.dump(report_info, f, indent=2)

    except Exception as e:
        print(f"Report generation failed: {e}")

@app.get("/api/reports/{report_id}")
async def get_report(report_id: str):
    """Get generated report information"""
    report_file = REPORTS_DIR / f"{report_id}.json"

    if not report_file.exists():
        raise HTTPException(status_code=404, detail="Report not found")

    with open(report_file, 'r') as f:
        report_info = json.load(f)

    return report_info

@app.get("/api/logs/{dataset}")
async def get_training_logs(dataset: str):
    """Get training logs for a specific dataset"""
    if dataset not in DATASETS:
        raise HTTPException(status_code=400, detail="Invalid dataset")

    # Mock logs for development
    logs = [
        {
            "id": 1,
            "timestamp": datetime.now().isoformat(),
            "level": "info",
            "message": f"Training started for {dataset} dataset",
            "dataset": dataset,
            "source": "training"
        },
        {
            "id": 2,
            "timestamp": datetime.now().isoformat(),
            "level": "debug",
            "message": "Generator loss: 0.4521, Discriminator loss: 0.7892",
            "dataset": dataset,
            "source": "training"
        }
    ]

    return logs

# ============================================================================
# WebSocket for Real-time Updates
# ============================================================================

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time training updates"""
    await websocket.accept()
    training_manager.add_websocket(websocket)

    print(f"üîå WebSocket client connected. Total connections: {len(training_manager.websocket_connections)}")

    try:
        while True:
            # Keep connection alive and handle incoming messages
            data = await websocket.receive_text()
            # Handle any client messages if needed
            print(f"üì® Received WebSocket message: {data}")

    except WebSocketDisconnect:
        training_manager.remove_websocket(websocket)
        print(f"üîå WebSocket client disconnected. Remaining connections: {len(training_manager.websocket_connections)}")
    except Exception as e:
        print(f"üîå WebSocket error: {e}")
        training_manager.remove_websocket(websocket)

# ============================================================================
# Health Check
# ============================================================================

@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "dcgan_available": DCGAN_AVAILABLE,
        "device": device_name,
        "mode": "production" if DCGAN_AVAILABLE else "development"
    }

# ============================================================================
# Application Events
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """Application startup event"""
    print("üöÄ Enhanced DCGAN Web API Starting...")
    print(f"üìä DCGAN Available: {DCGAN_AVAILABLE}")
    print(f"üñ•Ô∏è  Device: {device_name} ({device_type})")
    print(f"üìÅ Storage Root: {STORAGE_ROOT}")
    print(f"üåê CORS Origins: {ALLOWED_ORIGINS}")

    if not DCGAN_AVAILABLE:
        print("üí° Running in development mode with mock data")

    print("‚úÖ API Ready!")

@app.on_event("shutdown")
async def shutdown_event():
    """Application shutdown event"""
    print("üõë Enhanced DCGAN Web API Shutting down...")

# ============================================================================
# Application Startup
# ============================================================================

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    host = os.getenv("HOST", "0.0.0.0")
    reload = os.getenv("ENVIRONMENT", "production") == "development"

    print(f"üåê Starting server on {host}:{port}")
    print(f"üîÑ Reload mode: {reload}")

    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=reload,
        log_level="info"
    )