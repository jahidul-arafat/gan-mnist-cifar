#!/usr/bin/env python3
"""
Enhanced Multi-Dataset DCGAN with Apple Metal GPU Support
=========================================================

This version adds Apple Silicon (M1/M2/M3) GPU support via Metal Performance Shaders (MPS)
and includes fallback options for optimal performance across different hardware.

Key Features:
- Apple Metal GPU acceleration (MPS) for M1/M2/M3 Macs
- CUDA support for NVIDIA GPUs
- CPU fallback for compatibility
- Automatic device detection and optimization
- Memory management for Apple Silicon
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import ssl
import os
import time
import sys
import platform
from collections import deque
import threading
from matplotlib.animation import FuncAnimation

# Add these imports at the top of your script (after the existing imports)
import glob
import time
from pathlib import Path

import signal
import atexit
import sys
import traceback

from enhanced_dcgan_research.composite_enhanced_metrics_logger import create_composite_metrics_logger, \
    CompositeEnhancedMetricsLogger, analyze_composite_training_metrics

# Add this near the top with other imports
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("âš ï¸  psutil not available - CPU memory monitoring disabled")

# Add these imports at the top of your script (after existing imports)
class GracefulCheckpointManager:
    """Manages graceful checkpoint saving during interrupts and errors - FIXED for metrics finalization"""

    def __init__(self):
        self.checkpoint_data = None
        self.dataset_key = None
        self.current_epoch = 0
        self.models = {}
        self.optimizers = {}
        self.schedulers = {}
        self.ema_generator = None
        self.training_stats = {}
        self.emergency_save_enabled = False

        # ğŸ†• ADD: Metrics logger integration
        self.metrics_logger = None

        # Register signal handlers
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        atexit.register(self._cleanup)

    def register_training_components(self, dataset_key, generator, critic,
                                     optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                                     ema_generator, metrics_logger=None):
        """Register all training components for emergency saving - ENHANCED with metrics logger"""
        self.dataset_key = dataset_key
        self.models = {
            'generator': generator,
            'critic': critic
        }
        self.optimizers = {
            'optimizer_G': optimizer_G,
            'optimizer_D': optimizer_D
        }
        self.schedulers = {
            'scheduler_G': scheduler_G,
            'scheduler_D': scheduler_D
        }
        self.ema_generator = ema_generator

        # ğŸ†• CRITICAL: Register metrics logger
        self.metrics_logger = metrics_logger

        self.emergency_save_enabled = True

        print("ğŸ›¡ï¸  Emergency checkpoint system activated")
        if metrics_logger:
            print("ğŸ“Š Composite metrics logger integrated for graceful shutdown")

    def update_current_state(self, epoch, training_stats):
        """Update current training state for emergency saves"""
        self.current_epoch = epoch
        self.training_stats = training_stats

    def _signal_handler(self, signum, frame):
        """Handle interrupt signals gracefully - FIXED to finalize all data"""
        signal_names = {
            signal.SIGINT: "SIGINT (Ctrl+C)",
            signal.SIGTERM: "SIGTERM"
        }
        signal_name = signal_names.get(signum, f"Signal {signum}")

        print(f"\n\nğŸš¨ {signal_name} received - Initiating graceful shutdown...")
        print("=" * 80)

        if self.emergency_save_enabled:
            try:
                # ğŸ†• STEP 1: Finalize metrics logger FIRST
                if self.metrics_logger:
                    print("ğŸ“Š FINALIZING COMPOSITE METRICS LOGGER...")
                    print("   ğŸ“‹ This ensures all epoch and step data is saved to JSON files")

                    try:
                        # Save any pending step metrics
                        print("   ğŸ’¾ Saving pending step metrics...")
                        self.metrics_logger._save_step_metrics_incremental()

                        # Save epoch summaries
                        print("   ğŸ“Š Saving epoch summaries...")
                        self.metrics_logger._save_epoch_summaries()

                        # Finalize complete training log
                        print("   ğŸ“– Finalizing complete training log...")
                        self.metrics_logger.finalize_training()

                        print("   âœ… Composite metrics fully saved!")

                        # Display what was saved
                        print(f"   ğŸ“ Files updated:")
                        print(f"      â€¢ {self.metrics_logger.step_metrics_file.name}")
                        print(f"      â€¢ {self.metrics_logger.epoch_summaries_file.name}")
                        print(f"      â€¢ {self.metrics_logger.full_training_log.name}")

                    except Exception as metrics_error:
                        print(f"   âŒ Metrics finalization failed: {metrics_error}")
                        print(f"   âš ï¸  Continuing with emergency checkpoint save...")
                else:
                    print("   âš ï¸  No metrics logger registered - skipping metrics finalization")

                # ğŸ†• STEP 2: NOW save emergency checkpoint
                print("\nğŸ’¾ SAVING EMERGENCY CHECKPOINT...")
                checkpoint_path = self._save_emergency_checkpoint("interrupt")

                if checkpoint_path:
                    print("âœ… Emergency checkpoint saved successfully!")
                    print(f"   ğŸ“ Location: {checkpoint_path}")
                else:
                    print("âŒ Emergency checkpoint save failed!")

            except Exception as e:
                print(f"âŒ Critical error during graceful shutdown: {e}")
                print(f"ğŸ“ Error traceback:")
                import traceback
                traceback.print_exc()

        print("\nğŸ“‹ GRACEFUL SHUTDOWN SUMMARY:")
        print("   âœ… Signal received and handled")
        if self.metrics_logger:
            print("   âœ… Composite metrics finalized and saved")
        if self.emergency_save_enabled:
            print("   âœ… Emergency checkpoint attempted")
        print("   âœ… Clean program termination")

        print("\nğŸ‘‹ Graceful shutdown completed. All data preserved!")
        print("=" * 80)
        sys.exit(0)

    def _cleanup(self):
        """Cleanup function called on program exit - ENHANCED"""
        if self.emergency_save_enabled and hasattr(self, '_abnormal_exit'):
            print("\nğŸš¨ Abnormal program termination detected")

            # Try to finalize metrics first
            if self.metrics_logger:
                try:
                    print("ğŸ“Š Attempting to finalize metrics on abnormal exit...")
                    self.metrics_logger.finalize_training()
                    print("âœ… Metrics finalized on abnormal exit!")
                except:
                    print("âŒ Failed to finalize metrics on abnormal exit")

            # Then try emergency checkpoint
            try:
                self._save_emergency_checkpoint("abnormal_exit")
                print("âœ… Emergency checkpoint saved on exit!")
            except:
                print("âŒ Failed to save emergency checkpoint on exit")

    def _save_emergency_checkpoint(self, reason):
        """Enhanced emergency checkpoint saving with detailed progress and error handling - FIXED for DataLoader interrupts"""
        if not self.emergency_save_enabled or not self.dataset_key:
            print(f"   âš ï¸  Emergency save skipped: emergency_save_enabled={self.emergency_save_enabled}, dataset_key={self.dataset_key}")
            return None

        print(f"ğŸ’¾ Saving emergency checkpoint (reason: {reason})...")

        # Step 1: Create emergency checkpoint directory
        print(f"   ğŸ“ Creating emergency directory...")
        emergency_dir = f'./models/{self.dataset_key}/emergency'
        try:
            os.makedirs(emergency_dir, exist_ok=True)
            print(f"   âœ… Emergency directory ready: {emergency_dir}")
        except Exception as e:
            print(f"   âŒ Failed to create emergency directory: {e}")
            return None

        # Step 2: Generate filename with timestamp
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = f'{self.dataset_key}_emergency_{reason}_epoch_{self.current_epoch}_{timestamp}.pth'
        filepath = os.path.join(emergency_dir, filename)
        print(f"   ğŸ“ Emergency file: {filename}")

        try:
            # Step 3: Prepare checkpoint data with progress indication
            print(f"   ğŸ“¦ Preparing checkpoint data...")

            # Validate components before saving
            print(f"   ğŸ” Validating model components...")
            if not self.models or 'generator' not in self.models or 'critic' not in self.models:
                print(f"   âŒ Models not properly registered!")
                return None

            if not self.optimizers or 'optimizer_G' not in self.optimizers or 'optimizer_D' not in self.optimizers:
                print(f"   âŒ Optimizers not properly registered!")
                return None

            if not self.schedulers or 'scheduler_G' not in self.schedulers or 'scheduler_D' not in self.schedulers:
                print(f"   âŒ Schedulers not properly registered!")
                return None

            if not self.ema_generator:
                print(f"   âŒ EMA Generator not properly registered!")
                return None

            print(f"   âœ… All components validated successfully")

            # Step 4: Build checkpoint data with error handling for each component
            checkpoint_data = {}

            # Save generator state
            print(f"   ğŸ’¾ Saving generator state...")
            try:
                checkpoint_data['generator'] = self.models['generator'].state_dict()
                print(f"   âœ… Generator state saved")
            except Exception as e:
                print(f"   âŒ Failed to save generator: {e}")
                return None

            # Save critic state
            print(f"   ğŸ’¾ Saving critic state...")
            try:
                checkpoint_data['critic'] = self.models['critic'].state_dict()
                print(f"   âœ… Critic state saved")
            except Exception as e:
                print(f"   âŒ Failed to save critic: {e}")
                return None

            # Save EMA parameters
            print(f"   ğŸ’¾ Saving EMA parameters...")
            try:
                checkpoint_data['ema_params'] = self.ema_generator.ema.shadow_params if self.ema_generator else None
                print(f"   âœ… EMA parameters saved")
            except Exception as e:
                print(f"   âŒ Failed to save EMA parameters: {e}")
                # Don't return None here, EMA is not critical for emergency save
                checkpoint_data['ema_params'] = None

            # Save optimizer states
            print(f"   ğŸ’¾ Saving optimizer states...")
            try:
                checkpoint_data['optimizer_G'] = self.optimizers['optimizer_G'].state_dict()
                checkpoint_data['optimizer_D'] = self.optimizers['optimizer_D'].state_dict()
                print(f"   âœ… Optimizer states saved")
            except Exception as e:
                print(f"   âŒ Failed to save optimizers: {e}")
                return None

            # Save scheduler states
            print(f"   ğŸ’¾ Saving scheduler states...")
            try:
                checkpoint_data['scheduler_G'] = self.schedulers['scheduler_G'].state_dict()
                checkpoint_data['scheduler_D'] = self.schedulers['scheduler_D'].state_dict()
                print(f"   âœ… Scheduler states saved")
            except Exception as e:
                print(f"   âŒ Failed to save schedulers: {e}")
                return None

            # Add metadata
            print(f"   ğŸ“‹ Adding metadata...")
            checkpoint_data.update({
                'epoch': self.current_epoch,
                'dataset_key': self.dataset_key,
                'training_stats': self.training_stats,
                'emergency_save': True,
                'save_reason': reason,
                'timestamp': timestamp,
                'python_version': sys.version,
                'pytorch_version': torch.__version__,
                'device_info': str(torch.cuda.get_device_name(0)) if torch.cuda.is_available() else 'CPU',
                'enhancements': ['WGAN-GP', 'EMA', 'Enhanced-Arch', 'Spectral-Norm', 'Resume', 'Emergency-Save']
            })
            print(f"   âœ… Metadata added")

            # Step 5: Write to disk with progress indication - FIXED SIZE ESTIMATION
            print(f"   ğŸ’¾ Writing checkpoint to disk...")

            # FIX: Skip size estimation during emergency saves to avoid DataLoader tensor string conversion issues
            print(f"   ğŸ“Š Emergency save - skipping size estimation for faster save")

            # Save with error handling
            try:
                torch.save(checkpoint_data, filepath)

                # Verify the file was actually created and has reasonable size
                if os.path.exists(filepath):
                    actual_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
                    print(f"   âœ… File written successfully: {actual_size:.1f} MB")

                    # Quick verification - try to load the file header
                    try:
                        test_load = torch.load(filepath, map_location='cpu', weights_only=False)
                        if 'generator' in test_load and 'critic' in test_load:
                            print(f"   âœ… File integrity verified")
                        else:
                            print(f"   âš ï¸  File may be corrupted - missing core components")
                    except Exception as verify_error:
                        print(f"   âš ï¸  File verification failed: {verify_error}")

                else:
                    print(f"   âŒ File was not created!")
                    return None

            except Exception as save_error:
                print(f"   âŒ Failed to write file: {save_error}")

                # Try alternative save location
                try:
                    alt_filepath = f'./emergency_backup_{filename}'
                    print(f"   ğŸ”„ Attempting alternative save location: {alt_filepath}")
                    torch.save(checkpoint_data, alt_filepath)
                    print(f"   âœ… Emergency backup saved to: {alt_filepath}")
                    return alt_filepath
                except Exception as alt_error:
                    print(f"   âŒ Alternative save also failed: {alt_error}")
                    return None

            # Step 6: Final success message with summary
            print(f"   ğŸ‰ EMERGENCY CHECKPOINT SAVED SUCCESSFULLY!")
            print(f"   ğŸ“ Location: {filepath}")
            print(f"   ğŸ“… Epoch: {self.current_epoch}")
            print(f"   ğŸ•’ Timestamp: {timestamp}")
            print(f"   ğŸ” Reason: {reason}")

            # Display what was saved
            saved_components = []
            if 'generator' in checkpoint_data: saved_components.append("Generator")
            if 'critic' in checkpoint_data: saved_components.append("Critic")
            if 'ema_params' in checkpoint_data and checkpoint_data['ema_params']: saved_components.append("EMA")
            if 'optimizer_G' in checkpoint_data: saved_components.append("Optimizers")
            if 'scheduler_G' in checkpoint_data: saved_components.append("Schedulers")

            print(f"   ğŸ’¾ Saved components: {', '.join(saved_components)}")

            return filepath

        except Exception as e:
            print(f"   âŒ EMERGENCY SAVE FAILED: {e}")
            print(f"   ğŸ“ Error occurred at: {traceback.format_exc()}")

            # Last resort - try to save minimal checkpoint with just model weights
            try:
                print(f"   ğŸ†˜ Attempting minimal emergency save...")
                minimal_data = {
                    'generator': self.models['generator'].state_dict(),
                    'critic': self.models['critic'].state_dict(),
                    'epoch': self.current_epoch,
                    'dataset_key': self.dataset_key,
                    'emergency_save': True,
                    'save_reason': f"{reason}_minimal",
                    'timestamp': timestamp,
                    'minimal_save': True
                }

                minimal_filepath = os.path.join(emergency_dir, f'MINIMAL_{filename}')
                torch.save(minimal_data, minimal_filepath)
                print(f"   âœ… Minimal emergency checkpoint saved: {minimal_filepath}")
                return minimal_filepath

            except Exception as minimal_error:
                print(f"   âŒ Even minimal save failed: {minimal_error}")
                print(f"   ğŸ’” Emergency checkpoint completely failed!")
                return None

    # USAGE: Replace the existing _save_emergency_checkpoint method in your GracefulCheckpointManager class
    # with this enhanced version.

    # Additional helper method you can add to the GracefulCheckpointManager class:
    def get_emergency_checkpoint_info(self, dataset_key):
        """Get information about available emergency checkpoints - FIXED for PyTorch 2.6"""
        emergency_dir = f'./models/{dataset_key}/emergency'

        if not os.path.exists(emergency_dir):
            return []

        emergency_files = []
        for file in os.listdir(emergency_dir):
            if file.endswith('.pth') and 'emergency' in file:
                filepath = os.path.join(emergency_dir, file)
                try:
                    # Load just the metadata - FIX for PyTorch 2.6
                    checkpoint = torch.load(filepath, map_location='cpu', weights_only=False)
                    info = {
                        'filename': file,
                        'filepath': filepath,
                        'epoch': checkpoint.get('epoch', 'Unknown'),
                        'reason': checkpoint.get('save_reason', 'Unknown'),
                        'timestamp': checkpoint.get('timestamp', 'Unknown'),
                        'size_mb': os.path.getsize(filepath) / (1024 * 1024),
                        'minimal_save': checkpoint.get('minimal_save', False)
                    }
                    emergency_files.append(info)
                except Exception as e:
                    print(f"âš ï¸  Could not read emergency file {file}: {e}")

        # Sort by timestamp (newest first)
        emergency_files.sort(key=lambda x: x['timestamp'], reverse=True)
        return emergency_files

    def display_emergency_checkpoints(self, dataset_key):
        """Display available emergency checkpoints"""
        emergency_files = self.get_emergency_checkpoint_info(dataset_key)

        if not emergency_files:
            print(f"ğŸ“ No emergency checkpoints found for {dataset_key}")
            return

        print(f"\nğŸš¨ EMERGENCY CHECKPOINTS FOR {dataset_key.upper()}:")
        print("=" * 80)

        for i, info in enumerate(emergency_files, 1):
            print(f"\n{i}. {info['filename']}")
            print(f"   ğŸ“… Epoch: {info['epoch']}")
            print(f"   ğŸ” Reason: {info['reason']}")
            print(f"   ğŸ•’ Timestamp: {info['timestamp']}")
            print(f"   ğŸ“Š Size: {info['size_mb']:.1f} MB")
            if info['minimal_save']:
                print(f"   âš ï¸  Minimal save (models only)")
            else:
                print(f"   âœ… Complete save (all components)")

        print("=" * 80)

# Initialize the global checkpoint manager
checkpoint_manager = GracefulCheckpointManager()

def save_checkpoint_enhanced(dataset_key, epoch, generator, critic, ema_generator,
                             optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                             training_stats, checkpoint_type="regular"):
    """Enhanced checkpoint saving function"""

    # Create models directory if it doesn't exist
    os.makedirs('./models', exist_ok=True)

    checkpoint_data = {
        'generator': generator.state_dict(),
        'critic': critic.state_dict(),
        'ema_params': ema_generator.ema.shadow_params,
        'optimizer_G': optimizer_G.state_dict(),
        'optimizer_D': optimizer_D.state_dict(),
        'scheduler_G': scheduler_G.state_dict(),
        'scheduler_D': scheduler_D.state_dict(),
        'epoch': epoch,
        'dataset_key': dataset_key,
        'training_stats': training_stats,
        'checkpoint_type': checkpoint_type,
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'enhancements': ['WGAN-GP', 'EMA', 'Enhanced-Arch', 'Spectral-Norm', 'Resume', 'Auto-Save']
    }

    if checkpoint_type == "regular":
        filename = f'./models/{dataset_key}_enhanced_epoch_{epoch}.pth'
        print(f"ğŸ’¾ Saving checkpoint for epoch {epoch}...")
    elif checkpoint_type == "best":
        filename = f'./models/{dataset_key}_best_enhanced_model.pth'
        print(f"ğŸ† Saving best model checkpoint...")
    elif checkpoint_type == "final":
        os.makedirs(f'./models/{dataset_key}/enhanced', exist_ok=True)
        filename = f'./models/{dataset_key}/enhanced/final_enhanced_model.pth'
        print(f"ğŸ¯ Saving final model checkpoint...")

    try:
        torch.save(checkpoint_data, filename)
        print(f"   âœ… Checkpoint saved: {os.path.basename(filename)}")

        # Update checkpoint manager with current state
        checkpoint_manager.update_current_state(epoch, training_stats)

        return filename
    except Exception as e:
        print(f"   âŒ Failed to save checkpoint: {e}")
        return None

def enhanced_error_handler(func):
    """Decorator to handle errors with emergency checkpoint saving"""
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except KeyboardInterrupt:
            # This should be handled by signal handler, but just in case
            print("\nğŸš¨ KeyboardInterrupt caught in error handler")
            checkpoint_manager._signal_handler(signal.SIGINT, None)
        except Exception as e:
            print(f"\nğŸš¨ UNEXPECTED ERROR OCCURRED:")
            print(f"   Error: {str(e)}")
            print(f"   Type: {type(e).__name__}")
            print("\nğŸ“ Traceback:")
            traceback.print_exc()

            # Mark abnormal exit for cleanup
            checkpoint_manager._abnormal_exit = True

            if checkpoint_manager.emergency_save_enabled:
                print("\nğŸ’¾ Attempting emergency checkpoint save...")
                try:
                    checkpoint_manager._save_emergency_checkpoint("error")
                    print("âœ… Emergency checkpoint saved successfully!")
                except Exception as save_error:
                    print(f"âŒ Emergency save failed: {save_error}")

            print("\nğŸš¨ Program will now exit due to error")
            sys.exit(1)

    return wrapper

# =============================================================================
# CHECKPOINT RESUME FUNCTIONALITY
# =============================================================================

def find_available_checkpoints(dataset_key):
    """Find all available checkpoints for a dataset"""
    checkpoint_patterns = [
        f'./models/{dataset_key}_enhanced_epoch_*.pth',
        f'./models/{dataset_key}_best_enhanced_model.pth',
        f'./models/{dataset_key}/enhanced/final_enhanced_model.pth'
    ]

    all_checkpoints = []

    for pattern in checkpoint_patterns:
        checkpoints = glob.glob(pattern)
        all_checkpoints.extend(checkpoints)

    # Sort by modification time (newest first)
    all_checkpoints.sort(key=lambda x: os.path.getmtime(x), reverse=True)

    return all_checkpoints

# Optional: Also improve the display_checkpoint_options function to be clearer
def display_checkpoint_options(dataset_key):
    """Display available checkpoint options with details - Enhanced for clarity"""
    checkpoints = find_available_checkpoints(dataset_key)

    if not checkpoints:
        print(f"âŒ No checkpoints found for {dataset_key}")
        return None

    print(f"\nğŸ“ AVAILABLE CHECKPOINTS FOR {dataset_key.upper()}:")
    print("=" * 80)

    options = []

    for i, checkpoint_path in enumerate(checkpoints, 1):
        filename = os.path.basename(checkpoint_path)  # Define filename BEFORE try block
        try:
            # Load checkpoint info without loading full model - FIXED for PyTorch 2.6
            checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

            file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
            mod_time = os.path.getmtime(checkpoint_path)

            # Extract checkpoint info
            epoch = checkpoint.get('epoch', 'Unknown')
            enhancements = checkpoint.get('enhancements', [])
            training_stats = checkpoint.get('training_stats', {})

            print(f"\n{i}. ğŸ“„ {filename}")
            print(f"   ğŸ“… Epoch: {epoch}")
            print(f"   ğŸ“Š File Size: {file_size:.1f} MB")
            print(f"   ğŸ•’ Modified: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mod_time))}")

            if enhancements:
                print(f"   âœ¨ Enhancements: {', '.join(enhancements[:3])}{'...' if len(enhancements) > 3 else ''}")

            if training_stats:
                if 'avg_d_loss' in training_stats:
                    print(f"   ğŸ“ˆ Critic Loss: {training_stats['avg_d_loss']:.4f}")
                if 'avg_g_loss' in training_stats:
                    print(f"   ğŸ“ˆ Generator Loss: {training_stats['avg_g_loss']:.4f}")
                if 'ema_quality' in training_stats:
                    print(f"   ğŸ’œ EMA Quality: {training_stats['ema_quality']:.4f}")

            options.append((checkpoint_path, checkpoint))

        except Exception as e:
            print(f"\n{i}. ğŸ“„ {filename} (âš ï¸  Unable to read details: {e})")
            options.append((checkpoint_path, None))

    print("\n" + "=" * 80)
    return options

def get_checkpoint_choice(dataset_key):
    """Get user's choice of checkpoint to resume from - Enhanced with clear numbering"""
    options = display_checkpoint_options(dataset_key)

    # ğŸ†• ADD EMERGENCY CHECKPOINT DISPLAY
    print(f"\nğŸš¨ EMERGENCY CHECKPOINTS:")
    checkpoint_manager.display_emergency_checkpoints(dataset_key)

    if not options:
        return None, None

    print(f"\n" + "="*80)
    print("ğŸ“‹ CHECKPOINT SELECTION OPTIONS:")
    print("="*80)

    # Show option 0 clearly
    print(f"0. ğŸ†• Start fresh training (no checkpoint)")

    # Show each checkpoint option with clear numbering
    for i, (checkpoint_path, checkpoint_data) in enumerate(options, 1):
        filename = os.path.basename(checkpoint_path)

        # Get basic file info
        file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)  # MB
        mod_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(os.path.getmtime(checkpoint_path)))

        print(f"\n{i}. ğŸ“„ {filename}")
        print(f"   ğŸ“Š Size: {file_size:.1f} MB")
        print(f"   ğŸ•’ Modified: {mod_time}")

        # Show checkpoint details if available
        if checkpoint_data:
            epoch = checkpoint_data.get('epoch', 'Unknown')
            print(f"   ğŸ“… Epoch: {epoch}")

            if 'training_stats' in checkpoint_data:
                stats = checkpoint_data['training_stats']
                if 'avg_d_loss' in stats:
                    print(f"   ğŸ”´ Critic Loss: {stats['avg_d_loss']:.4f}")
                if 'avg_g_loss' in stats:
                    print(f"   ğŸ”µ Generator Loss: {stats['avg_g_loss']:.4f}")
                if 'ema_quality' in stats:
                    print(f"   ğŸ’œ EMA Quality: {stats['ema_quality']:.4f}")

            if 'enhancements' in checkpoint_data:
                enhancements = checkpoint_data['enhancements']
                print(f"   âœ¨ Features: {', '.join(enhancements[:3])}{'...' if len(enhancements) > 3 else ''}")
        else:
            print(f"   âš ï¸  Unable to read checkpoint details")

    print("\n" + "="*80)
    print("ğŸ’¡ SELECTION GUIDE:")
    print("   â€¢ Option 0: Fresh training from scratch")
    for i, (checkpoint_path, checkpoint_data) in enumerate(options, 1):
        filename = os.path.basename(checkpoint_path)

        # Give helpful descriptions
        if 'final' in filename.lower():
            desc = "Complete training session"
        elif 'best' in filename.lower():
            desc = "Best performing model"
        elif 'epoch' in filename.lower():
            desc = "Regular training checkpoint"
        else:
            desc = "Training checkpoint"

        print(f"   â€¢ Option {i}: Resume from {desc}")

    print("="*80)

    while True:
        try:
            choice = input(f"\nğŸ¯ Select checkpoint to resume from (0-{len(options)}): ").strip()

            if choice == '0':
                print(f"âœ… Selected: Start fresh training")
                return None, None

            choice_idx = int(choice) - 1
            if 0 <= choice_idx < len(options):
                checkpoint_path, checkpoint_data = options[choice_idx]
                filename = os.path.basename(checkpoint_path)

                # Show what was selected
                print(f"âœ… Selected: Option {choice} - {filename}")

                # Show confirmation details
                if checkpoint_data:
                    epoch = checkpoint_data.get('epoch', 'Unknown')
                    print(f"   ğŸ“… Will resume from epoch: {epoch}")
                    if 'training_stats' in checkpoint_data:
                        stats = checkpoint_data['training_stats']
                        if 'avg_wd' in stats:
                            print(f"   ğŸ“ˆ Last Wasserstein Distance: {stats['avg_wd']:.4f}")

                return checkpoint_path, checkpoint_data
            else:
                print(f"âŒ Invalid choice. Please enter a number between 0 and {len(options)}")

        except ValueError:
            print("âŒ Invalid input. Please enter a valid number")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Exiting...")
            exit()

# 2. REPLACE the load_checkpoint_and_resume function with this enhanced version
def load_checkpoint_and_resume(checkpoint_path, checkpoint_data, generator, critic,
                               optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                               ema_generator, device):
    """Load checkpoint and restore all training state - FIXED for device consistency"""

    print(f"\nğŸ”„ LOADING CHECKPOINT: {os.path.basename(checkpoint_path)}")
    print("=" * 60)

    try:
        # FIX: Handle the case where checkpoint_data is None
        if checkpoint_data is None:
            print("ğŸ“¦ Loading checkpoint data...")
            checkpoint_data = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

        # Load model states
        print("ğŸ“¦ Loading Generator state...")
        generator.load_state_dict(checkpoint_data['generator'])

        print("ğŸ“¦ Loading Critic state...")
        critic.load_state_dict(checkpoint_data['critic'])

        # Load EMA parameters if available - FIXED for device consistency
        if 'ema_params' in checkpoint_data and checkpoint_data['ema_params']:
            print("ğŸ’œ Loading EMA parameters...")

            # FIX: Ensure EMA shadow params are moved to the correct device
            loaded_ema_params = checkpoint_data['ema_params']
            for name, param in loaded_ema_params.items():
                if name in ema_generator.ema.shadow_params:
                    # Move the loaded EMA param to the same device as the model
                    ema_generator.ema.shadow_params[name] = param.to(device)

            if 'ema_quality' in checkpoint_data.get('training_stats', {}):
                ema_generator.quality_score = checkpoint_data['training_stats']['ema_quality']

            print(f"   âœ… EMA parameters moved to device: {device}")

        # Load optimizer states if available
        if 'optimizer_G' in checkpoint_data:
            print("âš™ï¸  Loading Generator optimizer state...")
            optimizer_G.load_state_dict(checkpoint_data['optimizer_G'])

        if 'optimizer_D' in checkpoint_data:
            print("âš™ï¸  Loading Critic optimizer state...")
            optimizer_D.load_state_dict(checkpoint_data['optimizer_D'])

        # Load scheduler states if available
        if 'scheduler_G' in checkpoint_data:
            print("ğŸ“ˆ Loading Generator scheduler state...")
            scheduler_G.load_state_dict(checkpoint_data['scheduler_G'])

        if 'scheduler_D' in checkpoint_data:
            print("ğŸ“ˆ Loading Critic scheduler state...")
            scheduler_D.load_state_dict(checkpoint_data['scheduler_D'])

        # Get starting epoch
        start_epoch = checkpoint_data.get('epoch', 0)

        # Display loaded information
        print(f"\nâœ… CHECKPOINT LOADED SUCCESSFULLY!")
        print(f"   ğŸ“… Resuming from epoch: {start_epoch}")
        print(f"   ğŸ¯ Dataset: {checkpoint_data.get('dataset_key', 'Unknown')}")
        print(f"   ğŸ–¥ï¸  Device consistency: All parameters on {device}")

        if 'training_stats' in checkpoint_data:
            stats = checkpoint_data['training_stats']
            print(f"   ğŸ“Š Last Training Stats:")
            if 'avg_d_loss' in stats:
                print(f"      ğŸ”´ Critic Loss: {stats['avg_d_loss']:.6f}")
            if 'avg_g_loss' in stats:
                print(f"      ğŸ”µ Generator Loss: {stats['avg_g_loss']:.6f}")
            if 'avg_wd' in stats:
                print(f"      ğŸŸ¢ Wasserstein Distance: {stats['avg_wd']:.6f}")
            if 'ema_quality' in stats:
                print(f"      ğŸ’œ EMA Quality: {stats['ema_quality']:.4f}")

        if 'enhancements' in checkpoint_data:
            print(f"   âœ¨ Active Enhancements: {', '.join(checkpoint_data['enhancements'])}")

        print("=" * 60)

        return start_epoch

    except Exception as e:
        print(f"âŒ ERROR loading checkpoint: {e}")
        print("ğŸ’¡ Starting fresh training instead...")
        return 0

def quick_resume_latest(dataset_key):
    """Quickly resume from the latest checkpoint"""
    checkpoints = find_available_checkpoints(dataset_key)
    if checkpoints:
        latest_checkpoint = checkpoints[0]  # Already sorted by newest first
        print(f"ğŸš€ Auto-resuming from latest checkpoint: {os.path.basename(latest_checkpoint)}")
        return latest_checkpoint
    else:
        print(f"âŒ No checkpoints found for {dataset_key}")
        return None

def list_all_checkpoints():
    """List all available checkpoints for all datasets - Enhanced with emergency checkpoints"""
    datasets = ['mnist', 'cifar10']

    print("\nğŸ“ ALL AVAILABLE CHECKPOINTS:")
    print("=" * 80)

    for dataset in datasets:
        print(f"\nğŸ¯ {dataset.upper()}:")

        # Regular checkpoints
        checkpoints = find_available_checkpoints(dataset)
        if checkpoints:
            print(f"\n   ğŸ“¦ REGULAR CHECKPOINTS:")
            for checkpoint in checkpoints:
                filename = os.path.basename(checkpoint)
                file_size = os.path.getsize(checkpoint) / (1024 * 1024)
                mod_time = time.strftime('%Y-%m-%d %H:%M:%S',
                                         time.localtime(os.path.getmtime(checkpoint)))
                print(f"      ğŸ“„ {filename} ({file_size:.1f}MB) - {mod_time}")
        else:
            print(f"      âŒ No regular checkpoints found")

        # ğŸ†• ADD EMERGENCY CHECKPOINTS
        print(f"\n   ğŸš¨ EMERGENCY CHECKPOINTS:")
        emergency_info = checkpoint_manager.get_emergency_checkpoint_info(dataset)
        if emergency_info:
            for info in emergency_info:
                status = "âš ï¸  MINIMAL" if info['minimal_save'] else "âœ… COMPLETE"
                print(f"      ğŸ“„ {info['filename']} ({info['size_mb']:.1f}MB) - {info['timestamp']} [{status}]")
                print(f"         ğŸ” Reason: {info['reason']} | Epoch: {info['epoch']}")
        else:
            print(f"      âŒ No emergency checkpoints found")

    print("=" * 80)

# Enhanced Device Detection and Setup
def detect_and_setup_device():
    """
    Detect and setup the best available device with Apple Metal support
    """
    print("ğŸ” Detecting available compute devices...")

    # Check for Apple Silicon with MPS support
    if torch.backends.mps.is_available():
        if torch.backends.mps.is_built():
            device = torch.device("mps")
            device_name = "Apple Silicon GPU (Metal Performance Shaders)"
            device_memory = "Unified Memory Architecture"
            print(f"âœ… Apple Metal GPU detected: {platform.processor()}")
            print(f"   ğŸš€ Using Metal Performance Shaders (MPS)")
            print(f"   ğŸ’¾ Memory: {device_memory}")

            # Set MPS memory management
            if hasattr(torch.mps, 'set_per_process_memory_fraction'):
                torch.mps.set_per_process_memory_fraction(0.8)  # Use 80% of available memory
                print(f"   âš™ï¸  Set MPS memory limit to 80%")

            return device, device_name, "mps"
        else:
            print("âš ï¸  MPS is available but not built. Falling back to CPU.")

    # Check for CUDA (NVIDIA GPU)
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        device_name = torch.cuda.get_device_name(0)
        device_memory = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        print(f"âœ… NVIDIA GPU detected: {device_name}")
        print(f"   ğŸ’¾ VRAM: {device_memory}")
        return device, device_name, "cuda"

    # Fallback to CPU
    else:
        device = torch.device("cpu")
        device_name = f"{platform.processor()} CPU"
        device_memory = "System RAM"
        print(f"ğŸ’» Using CPU: {device_name}")
        print(f"   ğŸ’¾ Memory: {device_memory}")
        return device, device_name, "cpu"

def setup_device_optimizations(device_type):
    """
    Setup device-specific optimizations
    """
    print(f"\nâš™ï¸  Configuring {device_type.upper()} optimizations...")

    if device_type == "mps":
        # Apple Metal optimizations
        print("   ğŸ Apple Metal optimizations:")
        print("   âœ“ Unified memory architecture enabled")
        print("   âœ“ Metal shader compilation optimized")
        print("   âœ“ Memory management configured")

        # Set environment variables for Metal
        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

        # Configure batch sizes for Apple Silicon
        recommended_batch_size = 64  # Smaller batch size for MPS
        print(f"   ğŸ“¦ Recommended batch size: {recommended_batch_size}")

    elif device_type == "cuda":
        # NVIDIA CUDA optimizations
        print("   ğŸŸ¢ NVIDIA CUDA optimizations:")
        print("   âœ“ cuDNN benchmark enabled")
        print("   âœ“ CUDA memory caching optimized")

        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False

        recommended_batch_size = 128
        print(f"   ğŸ“¦ Recommended batch size: {recommended_batch_size}")

    else:  # CPU
        # CPU optimizations
        print("   ğŸ’» CPU optimizations:")
        print("   âœ“ Threading optimized")
        print("   âœ“ Memory management configured")

        # Set optimal number of threads
        torch.set_num_threads(min(8, os.cpu_count()))

        recommended_batch_size = 32  # Smaller batch size for CPU
        print(f"   ğŸ“¦ Recommended batch size: {recommended_batch_size}")

    return recommended_batch_size

# Memory management for different devices
class DeviceMemoryManager:
    def __init__(self, device_type):
        self.device_type = device_type

    def clear_cache(self):
        """Clear device memory cache"""
        if self.device_type == "cuda":
            torch.cuda.empty_cache()
        elif self.device_type == "mps":
            if hasattr(torch.mps, 'empty_cache'):
                torch.mps.empty_cache()

    def get_memory_usage(self):
        """Get current memory usage"""
        if self.device_type == "cuda":
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            return f"Allocated: {allocated:.2f}GB, Cached: {cached:.2f}GB"
        elif self.device_type == "mps":
            if hasattr(torch.mps, 'current_allocated_memory'):
                allocated = torch.mps.current_allocated_memory() / 1024**3
                return f"Allocated: {allocated:.2f}GB (Unified Memory)"
            else:
                return "MPS Memory tracking not available"
        else:
            if PSUTIL_AVAILABLE:
                import psutil
                memory = psutil.virtual_memory()
                return f"System RAM: {memory.percent}% used"
            else:
                return "CPU Memory monitoring not available"

    def optimize_for_device(self, model):
        """Apply device-specific model optimizations"""
        if self.device_type == "mps":
            # Apple Metal optimizations
            print("   ğŸ Applying Metal optimizations to model...")
            # MPS works best with float32
            model = model.float()

        elif self.device_type == "cuda":
            # NVIDIA optimizations
            print("   ğŸŸ¢ Applying CUDA optimizations to model...")
            # Enable mixed precision if available
            if hasattr(torch.cuda.amp, 'autocast'):
                print("   âœ“ Mixed precision available")

        return model

# Initialize device
device, device_name, device_type = detect_and_setup_device()
recommended_batch_size = setup_device_optimizations(device_type)
memory_manager = DeviceMemoryManager(device_type)

print(f"\nğŸš€ Enhanced DCGAN configured for: {device_name}")
print(f"ğŸ“± Device Type: {device_type.upper()}")

# Progress bars and terminal utilities
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
    print("âœ… tqdm available - Enhanced progress bars enabled")
except ImportError:
    TQDM_AVAILABLE = False
    print("âŒ tqdm not available - Install with: pip install tqdm")
    # Fallback simple progress bar
    class tqdm:
        def __init__(self, iterable=None, total=None, desc="", leave=True,
                     ncols=None, unit="it", dynamic_ncols=True, colour=None):
            self.iterable = iterable
            self.total = total or (len(iterable) if iterable else 0)
            self.desc = desc
            self.n = 0
            self.leave = leave

        def __enter__(self):
            return self

        def __exit__(self, *args):
            if self.leave:
                print()

        def __iter__(self):
            for item in self.iterable:
                yield item
                self.update()

        def update(self, n=1):
            self.n += n
            if self.total > 0:
                progress = self.n / self.total
                bar_length = 40
                filled = int(bar_length * progress)
                bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                percent = progress * 100
                print(f'\r{self.desc}: |{bar}| {percent:.1f}% ({self.n}/{self.total})', end='', flush=True)

        def set_description(self, desc):
            self.desc = desc

        def set_postfix(self, **kwargs):
            postfix = ', '.join([f'{k}={v}' for k, v in kwargs.items()])
            print(f' - {postfix}', end='', flush=True)

        def close(self):
            if self.leave:
                print()

# Enhanced Progress and Monitoring Utilities
class ProgressTracker:
    """Advanced progress tracking with device-specific optimizations"""
    def __init__(self, device_type):
        self.device_type = device_type
        self.start_time = time.time()
        self.epoch_start_time = None
        self.batch_times = deque(maxlen=100)
        self.loss_history = {'d_loss': [], 'g_loss': [], 'wd': [], 'gp': []}
        self.memory_usage = deque(maxlen=50)

    def start_epoch(self):
        self.epoch_start_time = time.time()
        memory_manager.clear_cache()  # Clear cache at epoch start

    def update_batch_time(self, batch_time):
        self.batch_times.append(batch_time)

        # Track memory usage
        memory_usage = memory_manager.get_memory_usage()
        self.memory_usage.append(memory_usage)

    def get_avg_batch_time(self):
        return np.mean(self.batch_times) if self.batch_times else 0

    def get_eta(self, current_batch, total_batches, current_epoch, total_epochs):
        if not self.batch_times:
            return "Calculating..."

        avg_batch_time = self.get_avg_batch_time()
        remaining_batches_this_epoch = total_batches - current_batch
        remaining_epochs = total_epochs - current_epoch

        eta_seconds = (remaining_batches_this_epoch * avg_batch_time +
                       remaining_epochs * total_batches * avg_batch_time)

        hours = int(eta_seconds // 3600)
        minutes = int((eta_seconds % 3600) // 60)
        seconds = int(eta_seconds % 60)

        if hours > 0:
            return f"{hours:02d}h:{minutes:02d}m:{seconds:02d}s"
        else:
            return f"{minutes:02d}m:{seconds:02d}s"

    def get_elapsed_time(self):
        elapsed = time.time() - self.start_time
        hours = int(elapsed // 3600)
        minutes = int((elapsed % 3600) // 60)
        seconds = int(elapsed % 60)

        if hours > 0:
            return f"{hours:02d}h:{minutes:02d}m:{seconds:02d}s"
        else:
            return f"{minutes:02d}m:{seconds:02d}s"

    def get_current_memory_usage(self):
        return self.memory_usage[-1] if self.memory_usage else "N/A"

class LiveTerminalMonitor:
    """Live terminal monitoring with device-specific information"""
    def __init__(self, dataset_name, total_epochs, device_name, device_type):
        self.dataset_name = dataset_name
        self.total_epochs = total_epochs
        self.device_name = device_name
        self.device_type = device_type
        self.current_stats = {}

    def print_header(self, epoch, progress_tracker):
        print("=" * 100)
        print(f"ğŸš€ Enhanced DCGAN Training - {self.dataset_name} Dataset")
        print(f"ğŸ–¥ï¸  Device: {self.device_name} ({self.device_type.upper()})")
        print("=" * 100)
        print(f"ğŸ“… Epoch: {epoch}/{self.total_epochs}")
        print(f"â±ï¸  Elapsed: {progress_tracker.get_elapsed_time()}")
        print(f"ğŸ’¾ Memory: {progress_tracker.get_current_memory_usage()}")
        print("-" * 100)

    def print_live_stats(self, stats):
        """Print live streaming statistics with device info"""
        self.current_stats.update(stats)

        print(f"ğŸ“Š LIVE TRAINING METRICS ({self.device_type.upper()}):")
        print(f"   ğŸ”´ Critic Loss:     {self.current_stats.get('d_loss', 0):.6f}")
        print(f"   ğŸ”µ Generator Loss:  {self.current_stats.get('g_loss', 0):.6f}")
        print(f"   ğŸŸ¢ W-Distance:      {self.current_stats.get('wd', 0):.6f}")
        print(f"   ğŸŸ¡ Grad Penalty:    {self.current_stats.get('gp', 0):.6f}")
        print(f"   ğŸ’œ EMA Quality:     {self.current_stats.get('ema_quality', 0):.4f}")
        print(f"   ğŸ“ˆ Batch Time:      {self.current_stats.get('batch_time', 0):.3f}s")
        print(f"   ğŸ”„ Learning Rate G: {self.current_stats.get('lr_g', 0):.2e}")
        print(f"   ğŸ”„ Learning Rate D: {self.current_stats.get('lr_d', 0):.2e}")
        print(f"   ğŸ’¾ Memory Usage:    {self.current_stats.get('memory', 'N/A')}")

def print_step_details(step, total_steps, stats, progress_tracker, epoch, total_epochs):
    """Print detailed step information with device-specific metrics"""

    # Calculate progress
    step_progress = (step / total_steps) * 100
    eta = progress_tracker.get_eta(step, total_steps, epoch, total_epochs)

    # Create progress bar
    bar_length = 30
    filled = int(bar_length * (step / total_steps))
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

    # Format stats
    d_loss = stats.get('d_loss', 0)
    g_loss = stats.get('g_loss', 0)
    wd = stats.get('wd', 0)
    gp = stats.get('gp', 0)
    batch_time = stats.get('batch_time', 0)

    # Print dynamic line with device info
    device_emoji = "ğŸ" if device_type == "mps" else "ğŸŸ¢" if device_type == "cuda" else "ğŸ’»"
    info_line = (f"\r{device_emoji} Step {step:3d}/{total_steps} |{bar}| "
                 f"{step_progress:5.1f}% | "
                 f"D:{d_loss:7.4f} G:{g_loss:7.4f} | "
                 f"W:{wd:6.3f} GP:{gp:5.2f} | "
                 f"{batch_time:.3f}s/batch | "
                 f"ETA: {eta}")

    print(info_line, end='', flush=True)

# Optional TensorBoard support
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
    print("TensorBoard available - logging enabled")
except ImportError:
    TENSORBOARD_AVAILABLE = False
    print("TensorBoard not available - install with: pip install tensorboard")
    class DummyWriter:
        def add_scalar(self, *args, **kwargs): pass
        def close(self): pass
    SummaryWriter = lambda *args, **kwargs: DummyWriter()

# Fix SSL certificate issue on macOS
ssl._create_default_https_context = ssl._create_unverified_context

# =============================================================================
# DATASET CONFIGURATION WITH DEVICE OPTIMIZATION
# =============================================================================

class DatasetConfig:
    def __init__(self, name, image_size, channels, num_classes, description, preprocessing_info):
        self.name = name
        self.image_size = image_size
        self.channels = channels
        self.num_classes = num_classes
        self.description = description
        self.preprocessing_info = preprocessing_info

# Define dataset configurations with device-specific batch sizes
DATASETS = {
    'mnist': DatasetConfig(
        name='MNIST',
        image_size=32,
        channels=1,
        num_classes=10,
        description="Handwritten digits (0-9) in grayscale. 60,000 training images of 28x28 pixels.",
        preprocessing_info="Images resized to 32x32, normalized to [-1, 1] range"
    ),
    'cifar10': DatasetConfig(
        name='CIFAR-10',
        image_size=32,
        channels=3,
        num_classes=10,
        description="Color images in 10 categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. 50,000 training images of 32x32 pixels.",
        preprocessing_info="Images normalized to [-1, 1] range, data augmentation with random horizontal flips"
    )
}

def display_dataset_options():
    """Display available datasets with device-specific recommendations"""
    print("\n" + "="*80)
    print("ENHANCED MULTI-DATASET DCGAN - DATASET SELECTION")
    print(f"ğŸ–¥ï¸  Optimized for: {device_name} ({device_type.upper()})")
    print("="*80)

    for idx, (key, config) in enumerate(DATASETS.items(), 1):
        print(f"\n{idx}. {config.name}")
        print(f"   Description: {config.description}")
        print(f"   Image Size: {config.image_size}x{config.image_size}")
        print(f"   Channels: {config.channels} ({'Grayscale' if config.channels == 1 else 'RGB'})")
        print(f"   Classes: {config.num_classes}")
        print(f"   Preprocessing: {config.preprocessing_info}")

        # Device-specific recommendations
        if device_type == "mps":
            print(f"   ğŸ Apple Metal: Optimized batch size, efficient memory usage")
        elif device_type == "cuda":
            print(f"   ğŸŸ¢ NVIDIA CUDA: High performance training, large batch sizes")
        else:
            print(f"   ğŸ’» CPU: Stable training, conservative memory usage")

    print(f"\nğŸ“¦ Recommended batch size for {device_type.upper()}: {recommended_batch_size}")
    print("\n" + "="*80)

def get_dataset_choice():
    """Get user's dataset choice"""
    display_dataset_options()

    while True:
        try:
            choice = input("\nSelect dataset (1 for MNIST, 2 for CIFAR-10): ").strip()
            if choice == '1':
                return 'mnist'
            elif choice == '2':
                return 'cifar10'
            else:
                print("Please enter 1 or 2")
        except KeyboardInterrupt:
            print("\nExiting...")
            exit()

def display_enhancement_details(dataset_key):
    """Display detailed enhancement information with device-specific optimizations"""
    config = DATASETS[dataset_key]

    print(f"\n" + "="*80)
    print(f"ENHANCED WORKFLOW - {config.name} DATASET")
    print(f"ğŸ–¥ï¸  Device: {device_name} ({device_type.upper()})")
    print("="*80)

    print("ğŸš€ ADVANCED ENHANCEMENTS:")
    print("\n1. DEVICE-SPECIFIC OPTIMIZATIONS:")
    if device_type == "mps":
        print("   âœ“ Apple Metal Performance Shaders (MPS)")
        print("   âœ“ Unified memory architecture utilization")
        print("   âœ“ Metal shader compilation optimization")
        print("   âœ“ M1/M2/M3 specific memory management")
    elif device_type == "cuda":
        print("   âœ“ NVIDIA CUDA acceleration")
        print("   âœ“ cuDNN optimized operations")
        print("   âœ“ GPU memory management")
        print("   âœ“ Mixed precision training support")
    else:
        print("   âœ“ CPU multi-threading optimization")
        print("   âœ“ Memory-efficient operations")
        print("   âœ“ SIMD instruction utilization")

    print("\n2. WGAN-GP LOSS:")
    print("   âœ“ Wasserstein distance for better convergence")
    print("   âœ“ Gradient penalty for Lipschitz constraint")
    print("   âœ“ No mode collapse issues")
    print("   âœ“ More stable training dynamics")

    print("\n3. EMA (EXPONENTIAL MOVING AVERAGE):")
    print("   âœ“ Smoothed generator parameters")
    print("   âœ“ Better sample quality at inference")
    print("   âœ“ Reduced variance in generated images")
    print("   âœ“ Improved training stability")

    print("\n4. ENHANCED ARCHITECTURE:")
    print("   âœ“ Improved convolutional layers")
    print("   âœ“ Instance normalization in discriminator")
    print("   âœ“ Device-optimized operations")
    print("   âœ“ Better gradient flow")

    print("\n5. SPECTRAL NORMALIZATION:")
    print("   âœ“ Lipschitz constraint enforcement")
    print("   âœ“ Prevents discriminator overpowering")
    print("   âœ“ Stable training without careful tuning")
    print("   âœ“ Device-agnostic implementation")

    print(f"\nğŸ“Š EXPECTED PERFORMANCE ({device_type.upper()}):")
    if device_type == "mps":
        print("   â€¢ 3-5x faster than CPU on Apple Silicon")
        print("   â€¢ Efficient unified memory usage")
        print("   â€¢ Optimized for M1/M2/M3 architecture")
    elif device_type == "cuda":
        print("   â€¢ 10-20x faster than CPU")
        print("   â€¢ High throughput training")
        print("   â€¢ Large batch size support")
    else:
        print("   â€¢ Stable CPU-based training")
        print("   â€¢ Memory-efficient operations")
        print("   â€¢ Multi-core utilization")

    print("\nâš¡ DEVICE-SPECIFIC FEATURES:")
    print(f"   â€¢ Real-time {device_type.upper()} memory monitoring")
    print("   â€¢ Device-optimized batch sizes")
    print("   â€¢ Hardware-specific acceleration")
    print("   â€¢ Adaptive memory management")

    print("="*80)

    # Get user confirmation
    confirm = input(f"\nProceed with enhanced training on {device_name}? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Training cancelled.")
        exit()

# Continue with the rest of the functions (transforms, dataset loading, etc.)
# These remain largely the same but with device-specific optimizations

def get_transforms(dataset_key):
    """Get appropriate transforms for the dataset"""
    config = DATASETS[dataset_key]

    if dataset_key == 'mnist':
        return transforms.Compose([
            transforms.Resize(config.image_size),
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])
    else:  # cifar10
        return transforms.Compose([
            transforms.Resize(config.image_size),
            transforms.RandomHorizontalFlip(0.5),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

def get_dataset(dataset_key, transform):
    """Load the appropriate dataset"""
    if dataset_key == 'mnist':
        return torchvision.datasets.MNIST(
            root='./data', train=True, transform=transform, download=True
        )
    else:  # cifar10
        return torchvision.datasets.CIFAR10(
            root='./data', train=True, transform=transform, download=True
        )

def get_class_names(dataset_key):
    """Get class names for the dataset"""
    if dataset_key == 'mnist':
        return [str(i) for i in range(10)]
    else:  # cifar10
        return ['airplane', 'automobile', 'bird', 'cat', 'deer',
                'dog', 'frog', 'horse', 'ship', 'truck']

# =============================================================================
# ENHANCED LIVE PLOTTING
# =============================================================================

plt.ion()
class EnhancedLivePlotter:
    def __init__(self, dataset_name):
        self.fig, ((self.ax1, self.ax2), (self.ax3, self.ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        self.fig.suptitle(f'Enhanced GAN Training - {dataset_name} - Live Progress', fontsize=16)

        # Data storage
        self.epochs = deque(maxlen=200)
        self.d_losses = deque(maxlen=200)
        self.g_losses = deque(maxlen=200)
        self.wasserstein_dists = deque(maxlen=200)
        self.gradient_penalties = deque(maxlen=200)
        self.learning_rates_g = deque(maxlen=200)
        self.learning_rates_d = deque(maxlen=200)
        self.ema_scores = deque(maxlen=200)

        # Setup loss plot
        self.line_d_loss, = self.ax1.plot([], [], 'r-', label='Critic Loss', linewidth=2)
        self.line_g_loss, = self.ax1.plot([], [], 'b-', label='Generator Loss', linewidth=2)
        self.ax1.set_title('WGAN-GP Training Losses')
        self.ax1.set_xlabel('Epoch')
        self.ax1.set_ylabel('Loss')
        self.ax1.legend()
        self.ax1.grid(True, alpha=0.3)

        # Setup Wasserstein distance plot
        self.line_wd, = self.ax2.plot([], [], 'g-', label='Wasserstein Distance', linewidth=2)
        self.line_gp, = self.ax2.plot([], [], 'orange', label='Gradient Penalty', linewidth=2)
        self.ax2.set_title('WGAN-GP Metrics')
        self.ax2.set_xlabel('Epoch')
        self.ax2.set_ylabel('Value')
        self.ax2.legend()
        self.ax2.grid(True, alpha=0.3)

        # Setup learning rate plot
        self.line_lr_g, = self.ax3.plot([], [], 'b--', label='Generator LR', linewidth=2)
        self.line_lr_d, = self.ax3.plot([], [], 'r--', label='Critic LR', linewidth=2)
        self.ax3.set_title('Learning Rates')
        self.ax3.set_xlabel('Epoch')
        self.ax3.set_ylabel('Learning Rate')
        self.ax3.legend()
        self.ax3.grid(True, alpha=0.3)

        # Setup EMA quality plot
        self.line_ema, = self.ax4.plot([], [], 'm-', label='EMA Quality Score', linewidth=2)
        self.ax4.set_title('EMA Generator Quality')
        self.ax4.set_xlabel('Epoch')
        self.ax4.set_ylabel('Quality Score')
        self.ax4.legend()
        self.ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show(block=False)

    def update(self, epoch, d_loss, g_loss, wd, gp, lr_g, lr_d, ema_score=0.5):
        # Add new data
        self.epochs.append(epoch)
        self.d_losses.append(d_loss)
        self.g_losses.append(g_loss)
        self.wasserstein_dists.append(wd)
        self.gradient_penalties.append(gp)
        self.learning_rates_g.append(lr_g)
        self.learning_rates_d.append(lr_d)
        self.ema_scores.append(ema_score)

        # Update loss plot
        self.line_d_loss.set_data(list(self.epochs), list(self.d_losses))
        self.line_g_loss.set_data(list(self.epochs), list(self.g_losses))
        self.ax1.relim()
        self.ax1.autoscale_view()

        # Update WGAN metrics
        self.line_wd.set_data(list(self.epochs), list(self.wasserstein_dists))
        self.line_gp.set_data(list(self.epochs), list(self.gradient_penalties))
        self.ax2.relim()
        self.ax2.autoscale_view()

        # Update learning rate plot
        self.line_lr_g.set_data(list(self.epochs), list(self.learning_rates_g))
        self.line_lr_d.set_data(list(self.epochs), list(self.learning_rates_d))
        self.ax3.relim()
        self.ax3.autoscale_view()

        # Update EMA plot
        self.line_ema.set_data(list(self.epochs), list(self.ema_scores))
        self.ax4.relim()
        self.ax4.autoscale_view()

        # Health indicators
        if gp > 50:
            self.ax2.set_facecolor('#ffeeee')
            self.ax2.set_title('WGAN-GP Metrics - WARNING: HIGH GP!')
        elif abs(wd) < 0.1:
            self.ax2.set_facecolor('#fff5ee')
            self.ax2.set_title('WGAN-GP Metrics - WARNING: LOW W-DISTANCE!')
        else:
            self.ax2.set_facecolor('white')
            self.ax2.set_title('WGAN-GP Metrics - HEALTHY')

        # Refresh the plot
        self.fig.canvas.draw()
        self.fig.canvas.flush_events()

# =============================================================================
# ENHANCED MODELS
# =============================================================================

def weights_init(m):
    """Initialize weights according to DCGAN paper"""
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# Enhanced Conditional Generator
class EnhancedConditionalGenerator(nn.Module):
    def __init__(self, latent_dim, num_classes, channels, ngf=64):
        super(EnhancedConditionalGenerator, self).__init__()
        self.latent_dim = latent_dim
        self.num_classes = num_classes
        self.channels = channels
        self.ngf = ngf

        # Enhanced embedding for class labels
        self.label_emb = nn.Embedding(num_classes, latent_dim)
        self.label_fc = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.LeakyReLU(0.2, inplace=True)
        )

        # Initial dense layer with improved architecture
        self.fc = nn.Sequential(
            nn.Linear(latent_dim * 2, ngf * 8 * 4 * 4),
            nn.BatchNorm1d(ngf * 8 * 4 * 4),
            nn.ReLU(True)
        )

        # Enhanced convolutional layers
        self.conv_blocks = nn.Sequential(
            # 4x4 -> 8x8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),

            # 8x8 -> 16x16
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),

            # 16x16 -> 32x32
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            # Final output with residual connection
            nn.Conv2d(ngf, channels, 3, 1, 1, bias=False),
            nn.Tanh()
        )

        self.apply(weights_init)

    def forward(self, noise, labels):
        # Enhanced label embedding
        label_emb = self.label_emb(labels)
        label_emb = self.label_fc(label_emb)

        # Concatenate noise and enhanced label embedding
        gen_input = torch.cat([noise, label_emb], dim=1)

        # Dense layer
        out = self.fc(gen_input)
        out = out.view(out.size(0), self.ngf * 8, 4, 4)

        # Convolutional layers
        img = self.conv_blocks(out)
        return img

# Enhanced Conditional Critic (for WGAN-GP)
class EnhancedConditionalCritic(nn.Module):
    def __init__(self, num_classes, channels, image_size=32, ndf=64):
        super(EnhancedConditionalCritic, self).__init__()
        self.ndf = ndf
        self.num_classes = num_classes
        self.channels = channels
        self.image_size = image_size

        # Enhanced embedding for class labels
        self.label_emb = nn.Embedding(num_classes, image_size * image_size)

        # Enhanced convolutional layers (NO SIGMOID for WGAN)
        self.conv_blocks = nn.Sequential(
            # 32x32 -> 16x16
            nn.Conv2d(channels + 1, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # 16x16 -> 8x8
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 2),  # Instance norm instead of batch norm
            nn.LeakyReLU(0.2, inplace=True),

            # 8x8 -> 4x4
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # 4x4 -> 2x2
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.InstanceNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),

            # 2x2 -> 1x1 (NO SIGMOID for WGAN!)
            nn.Conv2d(ndf * 8, 1, 2, 1, 0, bias=False)
        )

        self.apply(weights_init)

    def forward(self, img, labels):
        # Embed labels and reshape to image dimensions
        label_emb = self.label_emb(labels)
        label_emb = label_emb.view(label_emb.size(0), 1, self.image_size, self.image_size)

        # Concatenate image and label embedding
        d_input = torch.cat([img, label_emb], dim=1)

        # Convolutional layers
        validity = self.conv_blocks(d_input)
        return validity.view(-1)

# =============================================================================
# WGAN-GP LOSS
# =============================================================================
class AdaptiveGradientPenaltyScheduler:
    """FIXED: Monitor gradient NORMS (not penalty values) for proper WGAN-GP training"""

    def __init__(self, initial_lambda=10.0, target_norm_range=(0.8, 1.2)):
        self.lambda_gp = initial_lambda
        self.target_min, self.target_max = target_norm_range  # TARGET GRADIENT NORMS, not penalty values
        self.norm_history = []  # Track gradient NORMS, not penalty values
        self.adjustment_factor = 1.05
        self.update_frequency = 100  # Check every 100 steps
        self.max_lambda = 100.0  # More reasonable maximum
        self.min_lambda = 1.0
        self.adjustment_count = 0
        self.max_adjustments_per_epoch = 2  # Limit adjustments

    def update(self, gradient_norm):
        """FIXED: Update lambda based on GRADIENT NORMS, not penalty values"""
        self.norm_history.append(gradient_norm)

        # Only adjust periodically and with limits
        if len(self.norm_history) >= self.update_frequency and self.adjustment_count < self.max_adjustments_per_epoch:
            avg_norm = np.mean(self.norm_history[-self.update_frequency:])

            # FIXED: Adjust based on gradient norms being close to 1.0
            if avg_norm < self.target_min:  # Gradient norms too small
                old_lambda = self.lambda_gp
                self.lambda_gp *= self.adjustment_factor
                self.lambda_gp = min(self.lambda_gp, self.max_lambda)
                self.adjustment_count += 1
                print(f"ğŸ”§ Grad norm low ({avg_norm:.3f} < {self.target_min:.1f}) - Increasing lambda: {old_lambda:.1f} â†’ {self.lambda_gp:.1f}")

            elif avg_norm > self.target_max:  # Gradient norms too large
                old_lambda = self.lambda_gp
                self.lambda_gp /= self.adjustment_factor
                self.lambda_gp = max(self.lambda_gp, self.min_lambda)
                self.adjustment_count += 1
                print(f"ğŸ”§ Grad norm high ({avg_norm:.3f} > {self.target_max:.1f}) - Decreasing lambda: {old_lambda:.1f} â†’ {self.lambda_gp:.1f}")

            # Clear history after adjustment
            self.norm_history = []

        return self.lambda_gp

    def reset_epoch_counter(self):
        """Reset adjustment counter for new epoch"""
        self.adjustment_count = 0

class WassersteinGPLoss:
    """FIXED: Enhanced Wasserstein Loss with PROPER gradient penalty monitoring"""
    def __init__(self, lambda_gp=10.0):  # FIXED: Use standard lambda=10
        self.lambda_gp = lambda_gp
        self._step_count = 0
        print(f"ğŸ”§ WGAN-GP initialized with lambda_gp = {lambda_gp}")

    def gradient_penalty(self, critic, real_samples, fake_samples, labels, device):
        """FIXED: More robust gradient penalty calculation with proper monitoring"""
        batch_size = real_samples.size(0)

        # Generate random interpolation coefficients
        alpha = torch.rand(batch_size, 1, 1, 1).to(device)
        alpha = alpha.expand_as(real_samples)

        # Interpolation between real and fake samples
        interpolates = alpha * real_samples + (1 - alpha) * fake_samples
        interpolates = interpolates.to(device)
        interpolates.requires_grad_(True)

        # Forward pass through critic
        d_interpolates = critic(interpolates, labels)

        # Calculate gradients with respect to interpolates
        gradients = torch.autograd.grad(
            outputs=d_interpolates,
            inputs=interpolates,
            grad_outputs=torch.ones(d_interpolates.size()).to(device),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]

        # Reshape gradients for norm calculation
        gradients = gradients.view(gradients.size(0), -1)

        # Calculate gradient norms
        gradient_norms = gradients.norm(2, dim=1)

        # FIXED: Calculate gradient penalty
        gradient_penalty = ((gradient_norms - 1) ** 2).mean()

        # Clamp gradient penalty to prevent extreme values
        gradient_penalty = torch.clamp(gradient_penalty, 0.0, 100.0)

        # Return both penalty and average gradient norm for monitoring
        avg_gradient_norm = gradient_norms.mean().item()

        return gradient_penalty, avg_gradient_norm

    def critic_loss(self, critic, real_samples, fake_samples, real_labels, fake_labels, device):
        """FIXED: Enhanced critic loss with proper gradient norm monitoring"""
        # Real samples loss
        real_output = critic(real_samples, real_labels)
        real_loss = -torch.mean(real_output)

        # Fake samples loss
        fake_output = critic(fake_samples.detach(), fake_labels)
        fake_loss = torch.mean(fake_output)

        # Gradient penalty with gradient norm monitoring
        gp, avg_grad_norm = self.gradient_penalty(critic, real_samples, fake_samples, real_labels, device)

        # Total critic loss
        critic_loss = real_loss + fake_loss + self.lambda_gp * gp

        # Wasserstein distance estimate
        wasserstein_distance = -(real_loss.item() + fake_loss.item())

        # FIXED: Debug information with gradient norms
        self._step_count += 1
        if self._step_count % 50 == 0:
            print(f"ğŸ” WGAN-GP Debug - GP: {gp.item():.6f}, Grad Norm: {avg_grad_norm:.6f} (target: ~1.0), Lambda: {self.lambda_gp:.1f}")

        return critic_loss, real_loss, fake_loss, gp, wasserstein_distance, avg_grad_norm

    def generator_loss(self, critic, fake_samples, fake_labels):
        """Calculate generator loss"""
        fake_output = critic(fake_samples, fake_labels)
        g_loss = -torch.mean(fake_output)
        return g_loss



# Add this enhanced step details function:
def print_enhanced_step_details(step, total_steps, stats, progress_tracker, epoch, total_epochs):
    """Enhanced step details with gradient norm monitoring"""

    # Calculate progress
    step_progress = (step / total_steps) * 100
    eta = progress_tracker.get_eta(step, total_steps, epoch, total_epochs)

    # Create progress bar
    bar_length = 30
    filled = int(bar_length * (step / total_steps))
    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)

    # Format stats
    d_loss = stats.get('d_loss', 0)
    g_loss = stats.get('g_loss', 0)
    wd = stats.get('wd', 0)
    gp = stats.get('gp', 0)
    grad_norm = stats.get('grad_norm', 0)  # NEW: Gradient norm
    batch_time = stats.get('batch_time', 0)

    # FIXED: Show gradient norm (target ~1.0) instead of just penalty
    device_emoji = "ğŸ" if device_type == "mps" else "ğŸŸ¢" if device_type == "cuda" else "ğŸ’»"
    info_line = (f"\r{device_emoji} Step {step:3d}/{total_steps} |{bar}| "
                 f"{step_progress:5.1f}% | "
                 f"D:{d_loss:7.4f} G:{g_loss:7.4f} | "
                 f"W:{wd:6.3f} GP:{gp:5.2f} | "
                 f"GradNorm:{grad_norm:.3f} | "  # NEW: Show gradient norm
                 f"{batch_time:.3f}s/batch | "
                 f"ETA: {eta}")

    print(info_line, end='', flush=True)


def print_enhanced_epoch_summary(epoch, training_stats):
    """Enhanced epoch summary with gradient norm health assessment"""

    avg_d_loss = training_stats['avg_d_loss']
    avg_g_loss = training_stats['avg_g_loss']
    avg_wd = training_stats['avg_wd']
    avg_gp = training_stats['avg_gp']
    avg_grad_norm = training_stats.get('avg_grad_norm', 0)  # NEW
    current_lambda = training_stats.get('current_lambda_gp', 10)

    print(f"\nğŸ‰ EPOCH {epoch} COMPREHENSIVE SUMMARY:")
    print("=" * 80)
    print(f"ğŸ“Š Training Metrics:")
    print(f"   ğŸ”´ Average Critic Loss: {avg_d_loss:.6f}")
    print(f"   ğŸ”µ Average Generator Loss: {avg_g_loss:.6f}")
    print(f"   ğŸŸ¢ Average Wasserstein Distance: {avg_wd:.6f}")
    print(f"   ğŸŸ¡ Average Gradient Penalty: {avg_gp:.6f}")
    print(f"   ğŸ“ Average Gradient Norm: {avg_grad_norm:.6f} (target: ~1.0)")  # NEW
    print(f"   ğŸ”§ Current GP Lambda: {current_lambda:.2f}")

    # FIXED: Health assessment based on gradient norms
    if 0.8 <= avg_grad_norm <= 1.2:
        print(f"   âœ… Gradient Health: OPTIMAL (norm: {avg_grad_norm:.3f})")
    elif avg_grad_norm < 0.8:
        print(f"   ğŸš¨ Gradient Health: NORMS TOO LOW (norm: {avg_grad_norm:.3f}) - Increase lambda")
    else:
        print(f"   âš ï¸  Gradient Health: NORMS TOO HIGH (norm: {avg_grad_norm:.3f}) - Decrease lambda")


# =============================================================================
# EMA GENERATOR
# =============================================================================

# 1. REPLACE the ExponentialMovingAverage.update method (around line 1560)
class ExponentialMovingAverage:
    """Enhanced EMA for model parameters"""
    def __init__(self, model, decay=0.999, device=None):
        self.decay = decay
        self.device = device if device is not None else next(model.parameters()).device
        self.shadow_params = {}
        self.backup_params = {}

        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow_params[name] = param.data.clone().to(self.device)

    def update(self, model):
        """Update EMA parameters - FIXED for device consistency"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.shadow_params:
                # FIX: Ensure both tensors are on the same device
                shadow_param = self.shadow_params[name].to(param.device)
                param_data = param.data.to(param.device)

                self.shadow_params[name] = (
                        self.decay * shadow_param + (1.0 - self.decay) * param_data
                ).to(param.device)

    def apply_shadow(self, model):
        """Apply EMA parameters - FIXED for device consistency"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.shadow_params:
                self.backup_params[name] = param.data.clone()
                # FIX: Ensure shadow param is on the same device as model param
                shadow_param = self.shadow_params[name].to(param.device)
                param.data.copy_(shadow_param)

    def restore(self, model):
        """Restore original parameters"""
        for name, param in model.named_parameters():
            if param.requires_grad and name in self.backup_params:
                param.data.copy_(self.backup_params[name])
        self.backup_params.clear()


# 4. UPDATE the EMAGenerator initialization to ensure proper device handling
class EMAGenerator:
    """Enhanced wrapper for generator with EMA - FIXED for device consistency"""
    def __init__(self, generator, decay=0.999, device=None):
        self.generator = generator
        # FIX: Ensure EMA uses the same device as the generator
        if device is None:
            device = next(generator.parameters()).device
        self.ema = ExponentialMovingAverage(generator, decay, device)
        self.quality_score = 0.5

    def update(self):
        """Update EMA parameters and quality score"""
        self.ema.update(self.generator)
        # Simple quality score based on parameter stability
        self.quality_score = min(0.9, self.quality_score + 0.01)

    def forward_with_ema(self, *args, **kwargs):
        """Forward pass with EMA parameters"""
        # Set generator to eval mode to avoid batch norm issues with single samples
        was_training = self.generator.training
        self.generator.eval()

        self.ema.apply_shadow(self.generator)
        try:
            output = self.generator(*args, **kwargs)
        finally:
            self.ema.restore(self.generator)
            # Restore original training mode
            if was_training:
                self.generator.train()
        return output

    def __call__(self, *args, **kwargs):
        return self.generator(*args, **kwargs)

# =============================================================================
# ENHANCED TRAINING FUNCTION WITH CHECKPOINT RESUME
# =============================================================================
# =============================================================================
# COMPLETE UPDATED TRAINING FUNCTION WITH ALL FIXES
# =============================================================================

# First, add this helper function before the training function (around line 650-700)
# RECOMMENDED: Replace your verify_device_consistency function with this one
def verify_device_consistency(generator, critic, ema_generator, device):
    """Recommended device consistency check - handles MPS properly"""
    print(f"\nğŸ” VERIFYING DEVICE CONSISTENCY:")

    # Get actual devices
    gen_device = next(generator.parameters()).device
    critic_device = next(critic.parameters()).device

    print(f"   Generator: {gen_device}")
    print(f"   Critic: {critic_device}")

    # Check EMA shadow params
    ema_device = None
    if ema_generator.ema.shadow_params:
        first_ema_param = next(iter(ema_generator.ema.shadow_params.values()))
        ema_device = first_ema_param.device
        print(f"   EMA Shadow Params: {ema_device}")

    # Convert target to torch.device if needed
    if isinstance(device, str):
        target_device = torch.device(device)
    else:
        target_device = device

    print(f"   ğŸ¯ Target device: {target_device}")

    # Check for REAL mismatches (different device types)
    issues = []

    if gen_device.type != target_device.type:
        issues.append(f"Generator on {gen_device.type}, expected {target_device.type}")

    if critic_device.type != target_device.type:
        issues.append(f"Critic on {critic_device.type}, expected {target_device.type}")

    if ema_device and ema_device.type != target_device.type:
        issues.append(f"EMA on {ema_device.type}, expected {target_device.type}")

    if not issues:
        print(f"   âœ… All components on correct device type: {target_device.type}")
        if gen_device.type == 'mps':
            print(f"   ğŸ Apple Metal GPU working correctly!")
        elif gen_device.type == 'cuda':
            print(f"   ğŸŸ¢ NVIDIA GPU working correctly!")
        else:
            print(f"   ğŸ’» CPU working correctly!")
        return True
    else:
        print(f"   âŒ Real device mismatches found:")
        for issue in issues:
            print(f"      â€¢ {issue}")

        print(f"   ğŸ”§ Fixing device placement...")

        # Fix the issues
        if gen_device.type != target_device.type:
            generator.to(target_device)
            print(f"   ğŸ“¦ Moved generator: {gen_device} â†’ {target_device}")

        if critic_device.type != target_device.type:
            critic.to(target_device)
            print(f"   ğŸ“¦ Moved critic: {critic_device} â†’ {target_device}")

        if ema_device and ema_device.type != target_device.type:
            for name, param in ema_generator.ema.shadow_params.items():
                ema_generator.ema.shadow_params[name] = param.to(target_device)
            print(f"   ğŸ“¦ Moved EMA: {ema_device} â†’ {target_device}")

        print(f"   âœ… All device issues resolved!")
        return False

# Now, replace your existing train_enhanced_gan_with_resume_modified function with this:
# Replace the train_enhanced_gan_with_resume_modified function in your
# enhanced_dcgan_mnist_cifar_for_apple_mps_checkpoints_graceful.py
# with this corrected version:

# =============================================================================
# STEP 6: ADD GP DIAGNOSTIC FUNCTION (OPTIONAL BUT RECOMMENDED)
# =============================================================================

# Add this function anywhere in your code for debugging:

def diagnose_gradient_penalty_issue(critic, real_samples, fake_samples, labels, device):
    """Comprehensive GP diagnostic - call this if you have issues"""
    print("ğŸ” GRADIENT PENALTY DIAGNOSTIC")
    print("=" * 50)

    batch_size = real_samples.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1).to(device)
    alpha = alpha.expand_as(real_samples)

    # Create interpolates
    interpolates = alpha * real_samples + (1 - alpha) * fake_samples
    interpolates = interpolates.to(device)
    interpolates.requires_grad_(True)

    # Forward pass
    d_interpolates = critic(interpolates, labels)
    print(f"   Interpolates shape: {interpolates.shape}")
    print(f"   Critic output shape: {d_interpolates.shape}")
    print(f"   Critic output range: [{d_interpolates.min().item():.3f}, {d_interpolates.max().item():.3f}]")

    # Calculate gradients
    gradients = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones(d_interpolates.size()).to(device),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    gradients_flat = gradients.view(gradients.size(0), -1)
    grad_norms = gradients_flat.norm(2, dim=1)

    print(f"   Gradient norms mean: {grad_norms.mean().item():.6f}")
    print(f"   Gradient norms std: {grad_norms.std().item():.6f}")
    print(f"   Gradient norms range: [{grad_norms.min().item():.6f}, {grad_norms.max().item():.6f}]")

    # Calculate GP
    gp = ((grad_norms - 1) ** 2).mean()
    print(f"   Raw GP value: {gp.item():.6f}")

    # Test different lambda values
    lambdas = [10.0, 50.0, 100.0, 200.0]
    print(f"   GP with different lambdas:")
    for lam in lambdas:
        weighted_gp = lam * gp.item()
        print(f"      Lambda {lam:3.0f}: {weighted_gp:.6f}")

    print("=" * 50)

    return gp.item(), grad_norms.mean().item()


@enhanced_error_handler
def train_enhanced_gan_with_resume_modified(dataset_key, config, resume_from_checkpoint=True, num_epochs=100, experiment_name=None):
    """Enhanced training with composite metrics logging that handles multiple training sessions seamlessly"""

    print(f"\nğŸš€ Starting Enhanced Training for {config.name} with Composite Metrics Logging")
    print("ğŸ›¡ï¸  Enhanced features active:")
    print("   âœ… Auto-save every 5 epochs")
    print("   âœ… Graceful interrupt handling (Ctrl+C)")
    print("   âœ… Emergency error recovery")
    print("   âœ… Device consistency checks")
    print("   âœ… WGAN-GP gradient norm monitoring")
    print("   âœ… ğŸ†• COMPOSITE METRICS LOGGING (handles resume sessions)")

    # =============================================================================
    # CHECKPOINT HANDLING
    # =============================================================================
    start_epoch = 0
    checkpoint_path = None
    checkpoint_data = None

    if resume_from_checkpoint:
        print("\nğŸ” CHECKPOINT RESUME OPTIONS")
        checkpoint_path, checkpoint_data = get_checkpoint_choice(dataset_key)

    # =============================================================================
    # TRAINING HYPERPARAMETERS
    # =============================================================================
    batch_size = 128
    if dataset_key == 'mnist':
        learning_rate_g = 0.0001
        learning_rate_d = 0.0004
    else:  # cifar10
        learning_rate_g = 0.0001
        learning_rate_d = 0.0004

    latent_dim = 100
    beta1 = 0.0
    beta2 = 0.9
    n_critic = 5
    lambda_gp = 10.0

    # =============================================================================
    # COMPOSITE METRICS LOGGER INITIALIZATION
    # =============================================================================
    print(f"\nğŸ“Š INITIALIZING COMPOSITE METRICS LOGGER")
    print("=" * 60)

    # Determine resume epoch for logger
    resume_from_epoch = None
    if checkpoint_path and checkpoint_data:
        resume_from_epoch = checkpoint_data.get('epoch', 0)
        print(f"ğŸ”„ Detected resume from epoch: {resume_from_epoch}")

    # Prepare system information for logger
    device_info = {
        "device_type": device_type,
        "device_name": device_name,
        "recommended_batch_size": recommended_batch_size,
        "device_tensor": str(device),
        "memory_manager_type": type(memory_manager).__name__,
        "tqdm_available": TQDM_AVAILABLE,
        "tensorboard_available": TENSORBOARD_AVAILABLE
    }

    # Prepare training configuration for logger
    training_config = {
        "dataset": dataset_key,
        "batch_size": batch_size,
        "learning_rate_generator": learning_rate_g,
        "learning_rate_discriminator": learning_rate_d,
        "latent_dimension": latent_dim,
        "beta1": beta1,
        "beta2": beta2,
        "n_critic_updates": n_critic,
        "lambda_gp": lambda_gp,
        "num_epochs_planned": num_epochs,
        "ema_decay": 0.999,
        "gradient_clip_value": 0.5,
        "checkpoint_frequency": 5,
        "image_generation_frequency": 5,
        "resumed_from_checkpoint": resume_from_checkpoint,
        "checkpoint_path": checkpoint_path.replace('\\', '/') if checkpoint_path else None,
        "resume_from_epoch": resume_from_epoch
    }

    # Create composite metrics logger that handles resume scenarios
    metrics_logger = create_composite_metrics_logger(
        dataset_key=dataset_key,
        device_info=device_info,
        training_config=training_config,
        experiment_name=experiment_name,
        resume_from_epoch=resume_from_epoch
    )

    print(f"âœ… Composite metrics logger initialized successfully!")

    # Display session summary if resuming
    if resume_from_epoch and resume_from_epoch > 0:
        print(f"\nğŸ“‹ {metrics_logger.get_session_summary()}")

    # =============================================================================
    # MODEL INITIALIZATION
    # =============================================================================
    print(f"\nğŸ—ï¸  Initializing models on device: {device}")
    generator = EnhancedConditionalGenerator(latent_dim, config.num_classes, config.channels).to(device)
    critic = EnhancedConditionalCritic(config.num_classes, config.channels, config.image_size).to(device)

    # Apply spectral normalization to critic
    for module in critic.modules():
        if isinstance(module, (nn.Conv2d, nn.Linear)):
            nn.utils.spectral_norm(module)

    # Initialize EMA Generator
    print(f"ğŸ’œ Initializing EMA on device: {device}")
    ema_generator = EMAGenerator(generator, decay=0.999, device=device)

    # Add model information to logger
    model_info = {
        "generator_parameters": sum(p.numel() for p in generator.parameters()),
        "critic_parameters": sum(p.numel() for p in critic.parameters()),
        "generator_architecture": str(generator.__class__.__name__),
        "critic_architecture": str(critic.__class__.__name__),
        "spectral_normalization": True,
        "ema_enabled": True,
        "ema_decay": 0.999
    }
    metrics_logger.set_system_info(device_info, model_info)

    print(f"ğŸ“Š Enhanced Model Statistics:")
    print(f"   Generator parameters: {model_info['generator_parameters']:,}")
    print(f"   Critic parameters: {model_info['critic_parameters']:,}")

    # =============================================================================
    # OPTIMIZER AND SCHEDULER INITIALIZATION
    # =============================================================================
    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_g, betas=(beta1, beta2))
    optimizer_D = optim.Adam(critic.parameters(), lr=learning_rate_d, betas=(beta1, beta2))
    scheduler_G = optim.lr_scheduler.ExponentialLR(optimizer_G, gamma=0.995)
    scheduler_D = optim.lr_scheduler.ExponentialLR(optimizer_D, gamma=0.995)

    # Register components with checkpoint manager
    checkpoint_manager.register_training_components(
        dataset_key, generator, critic, optimizer_G, optimizer_D,
        scheduler_G, scheduler_D, ema_generator,
        metrics_logger=metrics_logger  # ğŸ†• PASS THE METRICS LOGGER
    )

    # =============================================================================
    # CHECKPOINT LOADING
    # =============================================================================
    verify_device_consistency(generator, critic, ema_generator, device)

    if checkpoint_path and checkpoint_data:
        print(f"\nğŸ“¥ Loading checkpoint: {os.path.basename(checkpoint_path)}")
        start_epoch = load_checkpoint_and_resume(
            checkpoint_path, checkpoint_data, generator, critic,
            optimizer_G, optimizer_D, scheduler_G, scheduler_D,
            ema_generator, device
        )

        # Log checkpoint resume event with composite logger
        metrics_logger.log_event("checkpoint_resumed", f"Resumed from {os.path.basename(checkpoint_path)}", {
            "checkpoint_epoch": start_epoch,
            "checkpoint_path": checkpoint_path,
            "previous_sessions": len(metrics_logger.training_data["training_sessions"]) - 1
        })

        verify_device_consistency(generator, critic, ema_generator, device)

        # Adjust num_epochs if resuming
        remaining_epochs = num_epochs - start_epoch
        if remaining_epochs <= 0:
            print(f"âš ï¸  Training already completed! Starting additional 25 epochs...")
            num_epochs = start_epoch + 25
            remaining_epochs = 25

        print(f"ğŸ¯ Training will continue for {remaining_epochs} more epochs (until epoch {num_epochs})")
    else:
        print(f"ğŸ†• Starting fresh training from epoch 1")
        verify_device_consistency(generator, critic, ema_generator, device)

    # =============================================================================
    # DATASET LOADING
    # =============================================================================
    print(f"\nğŸ“¥ Loading {config.name} dataset...")
    transform = get_transforms(dataset_key)
    train_dataset = get_dataset(dataset_key, transform)

    try:
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True,
            num_workers=2, persistent_workers=False,
            pin_memory=False if device_type == "mps" else True
        )
        print(f"âœ… DataLoader created with 2 workers for optimal performance")
    except Exception as e:
        print(f"âš ï¸  Multiprocessing DataLoader failed: {e}")
        print(f"ğŸ”„ Falling back to single-threaded DataLoader for stability")
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, num_workers=0
        )

    print(f"âœ… Dataset loaded successfully!")
    print(f"   ğŸ“Š Training samples: {len(train_dataset):,}")
    print(f"   ğŸ“¦ Batch size: {batch_size}")
    print(f"   ğŸ”„ Total batches per epoch: {len(train_loader):,}")

    # =============================================================================
    # MONITORING AND VISUALIZATION SETUP
    # =============================================================================
    live_plotter = EnhancedLivePlotter(config.name)
    progress_tracker = ProgressTracker(device_type)
    terminal_monitor = LiveTerminalMonitor(config.name, num_epochs, device_name, device_type)

    # TensorBoard writer
    if TENSORBOARD_AVAILABLE:
        log_suffix = f"_resume_epoch_{start_epoch}" if start_epoch > 0 else ""
        writer = SummaryWriter(f'./runs/{dataset_key}_enhanced_gan{log_suffix}')
        print(f"ğŸ“Š TensorBoard logging to: ./runs/{dataset_key}_enhanced_gan{log_suffix}")
    else:
        writer = SummaryWriter()

    # =============================================================================
    # WGAN-GP LOSS AND SCHEDULER INITIALIZATION
    # =============================================================================
    wgan_loss = WassersteinGPLoss(lambda_gp)
    gp_scheduler = AdaptiveGradientPenaltyScheduler(
        initial_lambda=lambda_gp,
        target_norm_range=(0.8, 1.2)
    )
    print(f"ğŸ”§ Adaptive GP scheduler targets gradient norms 0.8-1.2")

    generator.train()
    critic.train()

    # Fixed samples for evaluation
    fixed_noise = torch.randn(64, latent_dim).to(device)
    fixed_labels = torch.randint(0, config.num_classes, (64,)).to(device)

    # Final device consistency check
    print(f"\nğŸ”§ FINAL PRE-TRAINING DEVICE CONSISTENCY CHECK:")
    is_consistent = verify_device_consistency(generator, critic, ema_generator, device)
    if not is_consistent:
        print("ğŸš¨ CRITICAL: Device inconsistency detected before training!")
        generator = generator.to(device)
        critic = critic.to(device)
        if ema_generator.ema.shadow_params:
            for name, param in ema_generator.ema.shadow_params.items():
                ema_generator.ema.shadow_params[name] = param.to(device)

    print(f"\nğŸš€ STARTING ENHANCED TRAINING WITH COMPOSITE METRICS LOGGING...")
    print("ğŸ’¡ Press Ctrl+C for graceful shutdown with checkpoint saving")
    print("ğŸ“Š All training metrics will be appended to existing logs for seamless resume")
    print("="*80)

    # =============================================================================
    # MAIN TRAINING LOOP
    # =============================================================================
    start_time = time.time()
    best_wasserstein_dist = float('inf')

    # Create main progress bar for epochs
    epoch_pbar = tqdm(range(start_epoch, num_epochs), desc="ğŸ¯ Training Epochs",
                      leave=True, ncols=100, colour='green')

    try:
        for epoch in epoch_pbar:
            progress_tracker.start_epoch()
            epoch_start_time = time.time()

            # START EPOCH IN COMPOSITE METRICS LOGGER
            metrics_logger.start_epoch(epoch + 1, num_epochs, len(train_loader))
            metrics_logger.log_event("epoch_start", f"Started epoch {epoch + 1}/{num_epochs}")

            # Reset GP adjustment counter for new epoch
            gp_scheduler.reset_epoch_counter()

            # Initialize epoch statistics arrays
            d_losses = []
            g_losses = []
            wasserstein_distances = []
            gradient_penalties = []
            gradient_norms = []
            batch_times = []

            # Update epoch progress bar description
            if start_epoch > 0:
                epoch_pbar.set_description(f"ğŸ”„ Resumed Epoch {epoch+1}/{num_epochs}")
            else:
                epoch_pbar.set_description(f"ğŸ¯ Epoch {epoch+1}/{num_epochs}")

            print(f"\n\nğŸ“… EPOCH {epoch+1}/{num_epochs} - Enhanced Training with Composite Metrics Logging")
            if start_epoch > 0 and epoch == start_epoch:
                print(f"ğŸ”„ RESUMED from checkpoint at epoch {start_epoch}")
                print(f"ğŸ“Š Continuing existing log with {len(metrics_logger.training_data['step_metrics'])} previous steps")
            print("=" * 80)

            # Create batch progress bar
            batch_pbar = tqdm(train_loader, desc="ğŸ”„ Processing Batches",
                              leave=False, ncols=120, colour='blue')

            for i, (real_imgs, labels) in enumerate(batch_pbar):
                batch_start_time = time.time()
                current_batch_size = real_imgs.size(0)

                # Ensure all data is on the correct device
                real_imgs = real_imgs.to(device)
                labels = labels.to(device)

                # =============================================================================
                # TRAIN CRITIC
                # =============================================================================
                critic_losses = []
                last_grad_norm = 0

                for critic_iter in range(n_critic):
                    optimizer_D.zero_grad()

                    # Generate fake images
                    z = torch.randn(current_batch_size, latent_dim).to(device)
                    fake_labels_gen = torch.randint(0, config.num_classes, (current_batch_size,)).to(device)
                    fake_imgs = generator(z, fake_labels_gen)

                    # Calculate critic loss with gradient norm monitoring
                    d_loss, d_real, d_fake, gp, wd, avg_grad_norm = wgan_loss.critic_loss(
                        critic, real_imgs, fake_imgs, labels, fake_labels_gen, device
                    )

                    # Update GP lambda based on gradient norms
                    new_lambda = gp_scheduler.update(avg_grad_norm)
                    wgan_loss.lambda_gp = new_lambda
                    last_grad_norm = avg_grad_norm

                    d_loss.backward()
                    torch.nn.utils.clip_grad_norm_(critic.parameters(), 0.5)
                    optimizer_D.step()

                    critic_losses.append(d_loss.item())

                # Store metrics from last critic iteration
                d_losses.append(d_loss.item())
                wasserstein_distances.append(wd)
                gradient_penalties.append(gp.item())
                gradient_norms.append(last_grad_norm)

                # =============================================================================
                # TRAIN GENERATOR
                # =============================================================================
                optimizer_G.zero_grad()
                z = torch.randn(current_batch_size, latent_dim).to(device)
                fake_labels_gen = torch.randint(0, config.num_classes, (current_batch_size,)).to(device)
                fake_imgs = generator(z, fake_labels_gen)

                g_loss = wgan_loss.generator_loss(critic, fake_imgs, fake_labels_gen)
                g_loss.backward()
                torch.nn.utils.clip_grad_norm_(generator.parameters(), 0.5)
                optimizer_G.step()

                # Update EMA
                try:
                    ema_generator.update()
                except RuntimeError as e:
                    if "Expected all tensors to be on the same device" in str(e):
                        print(f"\nâš ï¸  Device mismatch in EMA update! Fixing...")
                        for name, param in ema_generator.ema.shadow_params.items():
                            ema_generator.ema.shadow_params[name] = param.to(device)
                        print(f"âœ… EMA device fixed mid-training")
                        ema_generator.update()
                    else:
                        raise e

                g_losses.append(g_loss.item())

                # Calculate batch time
                batch_time = time.time() - batch_start_time
                batch_times.append(batch_time)
                progress_tracker.update_batch_time(batch_time)

                # =============================================================================
                # LOG STEP METRICS TO COMPOSITE LOGGER
                # =============================================================================
                step_metrics = {
                    "step": i + 1,
                    "d_loss": d_loss.item(),
                    "g_loss": g_loss.item(),
                    "wd": wd,
                    "gp": gp.item(),
                    "grad_norm": last_grad_norm,
                    "ema_quality": ema_generator.quality_score,
                    "lr_g": scheduler_G.get_last_lr()[0],
                    "lr_d": scheduler_D.get_last_lr()[0],
                    "batch_time": batch_time,
                    "lambda_gp": wgan_loss.lambda_gp,
                    "memory_usage": memory_manager.get_memory_usage()
                }

                metrics_logger.log_step_metrics(step_metrics)

                # Update batch progress bar
                current_stats = metrics_logger.get_current_stats()
                total_steps = current_stats["total_steps_completed"]
                current_session = current_stats["current_session_id"]

                batch_pbar.set_postfix({
                    'D_Loss': f'{d_loss.item():.4f}',
                    'G_Loss': f'{g_loss.item():.4f}',
                    'W_Dist': f'{wd:.4f}',
                    'GP': f'{gp.item():.3f}',
                    'GradNorm': f'{last_grad_norm:.3f}',
                    'EMA_Q': f'{ema_generator.quality_score:.3f}',
                    'Session': f'{current_session}',
                    'TotalSteps': f'{total_steps}',
                    'Time': f'{batch_time:.2f}s'
                })

                # Live terminal streaming every 25 steps
                if i % 25 == 0 or i < 5 or i == len(train_loader) - 1:
                    stats = {
                        'd_loss': d_loss.item(),
                        'g_loss': g_loss.item(),
                        'wd': wd,
                        'gp': gp.item(),
                        'grad_norm': last_grad_norm,
                        'batch_time': batch_time,
                        'ema_quality': ema_generator.quality_score,
                        'lr_g': scheduler_G.get_last_lr()[0],
                        'lr_d': scheduler_D.get_last_lr()[0]
                    }

                    print_enhanced_step_details(i+1, len(train_loader), stats, progress_tracker,
                                                epoch+1, num_epochs)

            # Close batch progress bar
            batch_pbar.close()
            print()

            # =============================================================================
            # END EPOCH PROCESSING
            # =============================================================================
            # End epoch in composite metrics logger (calculates summary automatically)
            metrics_logger.end_epoch()

            # Calculate epoch summary statistics
            epoch_time = time.time() - epoch_start_time
            avg_d_loss = np.mean(d_losses)
            avg_g_loss = np.mean(g_losses)
            avg_wd = np.mean(wasserstein_distances)
            avg_gp = np.mean(gradient_penalties)
            avg_grad_norm = np.mean(gradient_norms)
            avg_batch_time = np.mean(batch_times)

            # Prepare training stats for checkpoint manager
            training_stats = {
                'avg_d_loss': avg_d_loss,
                'avg_g_loss': avg_g_loss,
                'avg_wd': avg_wd,
                'avg_gp': avg_gp,
                'avg_grad_norm': avg_grad_norm,
                'ema_quality': ema_generator.quality_score,
                'epoch_time': epoch_time,
                'avg_batch_time': avg_batch_time,
                'lr_g': scheduler_G.get_last_lr()[0],
                'lr_d': scheduler_D.get_last_lr()[0],
                'current_lambda_gp': wgan_loss.lambda_gp,
            }

            # Log epoch completion event with session info
            current_stats = metrics_logger.get_current_stats()
            metrics_logger.log_event("epoch_complete", f"Completed epoch {epoch + 1}", {
                "epoch_duration": epoch_time,
                "avg_d_loss": avg_d_loss,
                "avg_g_loss": avg_g_loss,
                "avg_wasserstein_distance": avg_wd,
                "avg_gradient_norm": avg_grad_norm,
                "session_id": current_stats["current_session_id"],
                "total_epochs_completed": current_stats["total_epochs_completed"],
                "total_steps_completed": current_stats["total_steps_completed"]
            })

            print_enhanced_epoch_summary(epoch + 1, training_stats)

            # Display composite metrics info
            print(f"ğŸ“Š Composite Metrics Info:")
            print(f"   ğŸ”„ Current Session: {current_stats['current_session_id']}")
            print(f"   ğŸ“ˆ Total Epochs Completed: {current_stats['total_epochs_completed']}")
            print(f"   ğŸ“Š Total Steps Logged: {current_stats['total_steps_completed']}")

            # =============================================================================
            # LEARNING RATE UPDATE
            # =============================================================================
            scheduler_G.step()
            scheduler_D.step()
            new_lr_g = scheduler_G.get_last_lr()[0]
            new_lr_d = scheduler_D.get_last_lr()[0]

            # =============================================================================
            # TENSORBOARD LOGGING
            # =============================================================================
            writer.add_scalar('Loss/Critic', avg_d_loss, epoch)
            writer.add_scalar('Loss/Generator', avg_g_loss, epoch)
            writer.add_scalar('WGAN/Wasserstein_Distance', avg_wd, epoch)
            writer.add_scalar('WGAN/Gradient_Penalty', avg_gp, epoch)
            writer.add_scalar('WGAN/Gradient_Norm', avg_grad_norm, epoch)
            writer.add_scalar('WGAN/GP_Lambda', wgan_loss.lambda_gp, epoch)
            writer.add_scalar('EMA/Quality_Score', ema_generator.quality_score, epoch)
            writer.add_scalar('Learning_Rate/Generator', new_lr_g, epoch)
            writer.add_scalar('Learning_Rate/Critic', new_lr_d, epoch)

            # =============================================================================
            # LIVE PLOT UPDATE
            # =============================================================================
            try:
                live_plotter.update(epoch + 1, avg_d_loss, avg_g_loss, avg_wd, avg_gp,
                                    new_lr_g, new_lr_d, ema_generator.quality_score)
            except Exception as e:
                print(f"âš ï¸  Live plot update failed: {e}")

            # =============================================================================
            # CHECKPOINT SAVING
            # =============================================================================
            if (epoch + 1) % 5 == 0:
                print(f"ğŸ’¾ Auto-saving checkpoint for epoch {epoch+1}...")
                checkpoint_saved = save_checkpoint_enhanced(
                    dataset_key, epoch + 1, generator, critic, ema_generator,
                    optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                    training_stats, "regular"
                )

                if checkpoint_saved:
                    metrics_logger.log_event("checkpoint_saved", f"Regular checkpoint saved", {
                        "epoch": epoch + 1,
                        "checkpoint_type": "regular",
                        "file_path": checkpoint_saved,
                        "session_id": current_stats["current_session_id"]
                    })

            # =============================================================================
            # IMAGE GENERATION
            # =============================================================================
            if (epoch + 1) % 5 == 0 or epoch == 0:
                print(f"ğŸ¨ Generating sample images for epoch {epoch+1}...")
                save_enhanced_generated_images(epoch + 1, fixed_noise, fixed_labels,
                                               ema_generator, config, dataset_key)

                metrics_logger.log_event("images_generated", f"Sample images generated", {
                    "epoch": epoch + 1,
                    "num_samples": 64,
                    "session_id": current_stats["current_session_id"]
                })

            # =============================================================================
            # BEST MODEL TRACKING
            # =============================================================================
            if abs(avg_wd) < best_wasserstein_dist:
                best_wasserstein_dist = abs(avg_wd)
                print(f"ğŸ† New best model! Wasserstein Distance: {best_wasserstein_dist:.6f}")
                best_checkpoint = save_checkpoint_enhanced(
                    dataset_key, epoch + 1, generator, critic, ema_generator,
                    optimizer_G, optimizer_D, scheduler_G, scheduler_D,
                    training_stats, "best"
                )

                metrics_logger.log_event("best_model_updated", f"New best model found", {
                    "epoch": epoch + 1,
                    "wasserstein_distance": best_wasserstein_dist,
                    "checkpoint_path": best_checkpoint,
                    "session_id": current_stats["current_session_id"]
                })

            # Update epoch progress bar
            epoch_pbar.set_postfix({
                'D_Loss': f'{avg_d_loss:.4f}',
                'G_Loss': f'{avg_g_loss:.4f}',
                'W_Dist': f'{avg_wd:.4f}',
                'GradNorm': f'{avg_grad_norm:.3f}',
                'EMA_Q': f'{ema_generator.quality_score:.3f}',
                'Session': f'{current_stats["current_session_id"]}',
                'TotalSteps': f'{current_stats["total_steps_completed"]}'
            })

            print("=" * 80)

            # Update checkpoint manager with current state after each epoch
            checkpoint_manager.update_current_state(epoch + 1, training_stats)

    except KeyboardInterrupt:
        print(f"\nğŸš¨ Training interrupted by user - logging final state...")
        metrics_logger.log_event("training_interrupted", "Training stopped by user (Ctrl+C)")
        pass
    except Exception as e:
        print(f"\nâŒ Training error occurred...")

        # Try to finalize metrics on error
        if metrics_logger:
            try:
                print("ğŸ“Š Attempting to finalize metrics after error...")
                metrics_logger.finalize_training()
                print("âœ… Metrics finalized after error!")
            except:
                print("âŒ Failed to finalize metrics after error")

        # Mark abnormal exit and re-raise
        checkpoint_manager._abnormal_exit = True
        raise e

    # =============================================================================
    # TRAINING COMPLETION
    # =============================================================================
    epoch_pbar.close()

    # Save final checkpoint
    print(f"\nğŸ’¾ Saving final checkpoint...")
    final_checkpoint = save_checkpoint_enhanced(
        dataset_key, epoch + 1, generator, critic, ema_generator,
        optimizer_G, optimizer_D, scheduler_G, scheduler_D,
        training_stats, "final"
    )

    # Log final checkpoint
    current_stats = metrics_logger.get_current_stats()
    metrics_logger.log_event("final_checkpoint_saved", "Final training checkpoint saved", {
        "checkpoint_path": final_checkpoint,
        "session_id": current_stats["current_session_id"]
    })

    # FINALIZE COMPOSITE METRICS LOGGING
    print(f"\nğŸ“Š FINALIZING COMPOSITE METRICS LOGGING...")
    metrics_logger.finalize_training()

    # Training completion summary
    total_time = time.time() - start_time
    final_stats = metrics_logger.get_current_stats()

    print(f"\nğŸ‰ ENHANCED TRAINING COMPLETED WITH COMPOSITE METRICS LOGGING!")
    print("="*80)
    print(f"â±ï¸ Total Training Time: {total_time/60:.1f} minutes")
    print(f"ğŸ† Best Wasserstein Distance: {best_wasserstein_dist:.6f}")
    print(f"ğŸ’œ Final EMA Quality Score: {ema_generator.quality_score:.4f}")
    print(f"ğŸ“ Final Average Gradient Norm: {avg_grad_norm:.6f} (target: ~1.0)")
    print(f"ğŸ“Š Total Batches Processed: {(epoch + 1 - start_epoch) * len(train_loader):,}")
    print(f"âš¡ Average Speed: {progress_tracker.get_avg_batch_time():.3f}s/batch")
    print(f"ğŸ–¥ï¸  Training Device: {device}")
    if start_epoch > 0:
        print(f"ğŸ”„ Successfully resumed from epoch {start_epoch}")

    # Display composite metrics logging summary
    print(f"\nğŸ“„ COMPOSITE METRICS LOGGING SUMMARY:")
    print(f"   ğŸ“Š Total step metrics logged: {final_stats['total_steps_completed']}")
    print(f"   ğŸ“… Total epoch summaries: {final_stats['total_epochs_completed']}")
    print(f"   ğŸ”„ Training sessions: {final_stats['total_sessions']}")
    print(f"   ğŸ—“ï¸  Training events: {len(metrics_logger.training_data['training_events'])}")
    print(f"   ğŸ“ Log files (composite across all sessions):")
    print(f"      â€¢ Step metrics: {metrics_logger.step_metrics_file.name}")
    print(f"      â€¢ Epoch summaries: {metrics_logger.epoch_summaries_file.name}")
    print(f"      â€¢ Complete log: {metrics_logger.full_training_log.name}")

    # Display session summary
    print(f"\nğŸ”„ {metrics_logger.get_session_summary()}")

    print("="*80)

    writer.close()
    return ema_generator, critic

# Enhanced image generation and saving
def save_enhanced_generated_images(epoch, fixed_noise, fixed_labels, ema_generator, config, dataset_key):
    """Save enhanced generated images with EMA - Works for both MNIST and CIFAR-10"""
    class_names = get_class_names(dataset_key)

    with torch.no_grad():
        # Generate with both regular and EMA generator
        regular_imgs = ema_generator.generator(fixed_noise, fixed_labels)
        ema_imgs = ema_generator.forward_with_ema(fixed_noise, fixed_labels)

        # Denormalize from [-1, 1] to [0, 1]
        regular_imgs = regular_imgs.cpu() * 0.5 + 0.5
        ema_imgs = ema_imgs.cpu() * 0.5 + 0.5

        # Clamp values to ensure they're in [0, 1] range
        regular_imgs = torch.clamp(regular_imgs, 0, 1)
        ema_imgs = torch.clamp(ema_imgs, 0, 1)

        # Create comparison grid
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

        # Regular generator samples
        regular_grid = torchvision.utils.make_grid(regular_imgs[:16], nrow=4, padding=2, normalize=False)

        if config.channels == 1:  # MNIST (Grayscale)
            # Handle different possible shapes from make_grid
            if regular_grid.dim() == 3:
                if regular_grid.size(0) == 3:
                    # make_grid created 3-channel image, convert to grayscale
                    regular_grid = regular_grid.mean(dim=0)
                elif regular_grid.size(0) == 1:
                    # make_grid created 1-channel image, squeeze
                    regular_grid = regular_grid.squeeze(0)
            ax1.imshow(regular_grid, cmap='gray', vmin=0, vmax=1)
        else:  # CIFAR-10 (RGB)
            # Convert from CHW to HWC format for matplotlib
            if regular_grid.dim() == 3 and regular_grid.size(0) == 3:
                regular_grid = regular_grid.permute(1, 2, 0)
            ax1.imshow(regular_grid)

        ax1.set_title(f'Regular Generator - Epoch {epoch}', fontsize=14, fontweight='bold')
        ax1.axis('off')

        # EMA generator samples
        ema_grid = torchvision.utils.make_grid(ema_imgs[:16], nrow=4, padding=2, normalize=False)

        if config.channels == 1:  # MNIST (Grayscale)
            # Handle different possible shapes from make_grid
            if ema_grid.dim() == 3:
                if ema_grid.size(0) == 3:
                    # make_grid created 3-channel image, convert to grayscale
                    ema_grid = ema_grid.mean(dim=0)
                elif ema_grid.size(0) == 1:
                    # make_grid created 1-channel image, squeeze
                    ema_grid = ema_grid.squeeze(0)
            ax2.imshow(ema_grid, cmap='gray', vmin=0, vmax=1)
        else:  # CIFAR-10 (RGB)
            # Convert from CHW to HWC format for matplotlib
            if ema_grid.dim() == 3 and ema_grid.size(0) == 3:
                ema_grid = ema_grid.permute(1, 2, 0)
            ax2.imshow(ema_grid)

        ax2.set_title(f'EMA Generator - Epoch {epoch}', fontsize=14, fontweight='bold')
        ax2.axis('off')

        # Add dataset info to the title
        dataset_info = f"{config.name} Dataset - {config.channels} Channel{'s' if config.channels > 1 else ''}"
        plt.suptitle(f'{dataset_info} Enhanced GAN Comparison - Epoch {epoch}', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # Save the comparison image
        save_path = f'./outputs/{dataset_key}_enhanced_comparison_epoch_{epoch}.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"   ğŸ’¾ Saved comparison: {save_path}")
        plt.close()

        # Save detailed EMA samples in a larger grid
        fig, axes = plt.subplots(8, 8, figsize=(16, 16))

        for i, ax in enumerate(axes.flat):
            if i < len(ema_imgs):
                img = ema_imgs[i]
                label_idx = fixed_labels[i].item()

                if config.channels == 1:  # MNIST (Grayscale)
                    ax.imshow(img.squeeze(), cmap='gray', vmin=0, vmax=1)
                else:  # CIFAR-10 (RGB)
                    # Convert from CHW to HWC format
                    img = img.permute(1, 2, 0)
                    ax.imshow(img)

                ax.set_title(f'{class_names[label_idx]}', fontsize=8, fontweight='bold')
                ax.axis('off')
            else:
                # Hide unused subplots
                ax.axis('off')

        plt.suptitle(f'{config.name} EMA Generated Images - Epoch {epoch}', fontsize=16, fontweight='bold')
        plt.tight_layout()

        # Save the detailed EMA samples
        save_path = f'./outputs/{dataset_key}_enhanced_ema_epoch_{epoch}.png'
        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')
        print(f"   ğŸ’¾ Saved EMA samples: {save_path}")
        plt.close()

        print(f"   âœ… Successfully generated and saved images for epoch {epoch}")

def generate_enhanced_specific_classes(ema_generator, config, dataset_key, classes, num_each=8):
    """Generate specific classes with enhanced quality"""
    class_names = get_class_names(dataset_key)

    # Set generator to eval mode for inference
    ema_generator.generator.eval()

    with torch.no_grad():
        all_imgs = []
        all_labels = []

        for class_idx in classes:
            z = torch.randn(num_each, 100).to(device)  # latent_dim = 100
            labels = torch.full((num_each,), class_idx).to(device)
            fake_imgs = ema_generator.forward_with_ema(z, labels)
            all_imgs.append(fake_imgs.cpu())
            all_labels.extend([class_idx] * num_each)

        all_imgs = torch.cat(all_imgs)
        all_imgs = all_imgs * 0.5 + 0.5

        # Plot with enhanced layout
        rows = len(classes)
        fig, axes = plt.subplots(rows, num_each, figsize=(num_each*2.5, rows*2.5))
        if rows == 1:
            axes = axes.reshape(1, -1)

        idx = 0
        for i in range(rows):
            for j in range(num_each):
                if config.channels == 1:
                    axes[i, j].imshow(all_imgs[idx].squeeze(), cmap='gray')
                else:
                    axes[i, j].imshow(all_imgs[idx].permute(1, 2, 0))
                axes[i, j].set_title(f'{class_names[all_labels[idx]]}', fontsize=10)
                axes[i, j].axis('off')
                idx += 1

        plt.suptitle(f'{config.name} - Enhanced Generated Specific Classes (EMA)', fontsize=16)
        plt.tight_layout()
        plt.show()

    # Restore training mode
    ema_generator.generator.train()

def enhanced_interpolate_latent_space(ema_generator, config, dataset_key, class1, class2, steps=10):
    """Enhanced latent space interpolation with EMA"""
    class_names = get_class_names(dataset_key)

    # Set generator to eval mode for inference
    ema_generator.generator.eval()

    with torch.no_grad():
        z1 = torch.randn(1, 100).to(device)  # latent_dim = 100
        z2 = torch.randn(1, 100).to(device)

        interpolated_imgs = []
        for i in range(steps):
            alpha = i / (steps - 1)
            z_interp = (1 - alpha) * z1 + alpha * z2

            # Generate images for both classes using EMA
            img1 = ema_generator.forward_with_ema(z_interp, torch.tensor([class1]).to(device))
            img2 = ema_generator.forward_with_ema(z_interp, torch.tensor([class2]).to(device))

            interpolated_imgs.append((img1.cpu(), img2.cpu()))

        # Enhanced plot
        fig, axes = plt.subplots(2, steps, figsize=(steps*2.5, 6))
        for i, (img1, img2) in enumerate(interpolated_imgs):
            img1 = img1 * 0.5 + 0.5
            img2 = img2 * 0.5 + 0.5

            if config.channels == 1:
                axes[0, i].imshow(img1.squeeze(), cmap='gray')
                axes[1, i].imshow(img2.squeeze(), cmap='gray')
            else:
                axes[0, i].imshow(img1.squeeze().permute(1, 2, 0))
                axes[1, i].imshow(img2.squeeze().permute(1, 2, 0))

            axes[0, i].set_title(f'{class_names[class1]}', fontsize=10)
            axes[1, i].set_title(f'{class_names[class2]}', fontsize=10)
            axes[0, i].axis('off')
            axes[1, i].axis('off')

        plt.suptitle(f'{config.name} Enhanced Latent Interpolation: {class_names[class1]} â†” {class_names[class2]}',
                     fontsize=16)
        plt.tight_layout()
        plt.show()

    # Restore training mode if it was in training mode
    ema_generator.generator.train()

# =============================================================================
# ENHANCED MAIN EXECUTION WITH CHECKPOINT SUPPORT
# =============================================================================

# Modified main function to integrate all changes
def main_with_checkpoint_support_enhanced():
    """Enhanced main function with composite metrics logging for seamless resume"""

    print("=" * 100)
    print("ğŸ¨ ENHANCED MULTI-DATASET DCGAN WITH COMPOSITE METRICS LOGGING")
    print("=" * 100)
    print("ğŸš€ Advanced Features:")
    print("1. âœ… WGAN-GP Loss with Gradient Penalty")
    print("2. âœ… EMA (Exponential Moving Average)")
    print("3. âœ… Enhanced Generator/Critic Architecture")
    print("4. âœ… Spectral Normalization")
    print("5. âœ… Progressive Learning Rate Scheduling")
    print("6. âœ… Real-time Enhanced Monitoring")
    print("7. âœ… Live Progress Bars & Terminal Streaming")
    print("8. âœ… Comprehensive Training Analytics")
    print("9. âœ… Multi-Dataset Support (MNIST & CIFAR-10)")
    print("10. âœ… Checkpoint Resume Capability")
    print("11. âœ… AUTO-SAVE EVERY 5 EPOCHS")
    print("12. âœ… GRACEFUL INTERRUPT HANDLING (Ctrl+C)")
    print("13. âœ… EMERGENCY ERROR RECOVERY")
    print("14. âœ… ğŸ†• COMPOSITE METRICS LOGGING")
    print("15. âœ… ğŸ†• SEAMLESS RESUME LOGGING")
    print("16. âœ… ğŸ†• MULTI-SESSION TRACKING")
    print("17. âœ… ğŸ†• CONTINUOUS LOG APPENDING")
    print("=" * 100)

    # =============================================================================
    # USER INPUT AND CONFIGURATION
    # =============================================================================

    # Get user's dataset choice
    dataset_key = get_dataset_choice()
    config = DATASETS[dataset_key]

    # Ask for experiment name
    print(f"\nğŸ”¬ EXPERIMENT CONFIGURATION:")
    experiment_name = input(f"Enter experiment name (optional, press Enter to skip): ").strip()
    if not experiment_name:
        experiment_name = None
        print("âœ… Using default experiment naming")
    else:
        print(f"âœ… Experiment name: {experiment_name}")

    # Check for existing logs
    print(f"\nğŸ” CHECKING FOR EXISTING LOGS...")
    temp_logger = CompositeEnhancedMetricsLogger(dataset_key, experiment_name)
    if temp_logger.existing_log_found:
        print(f"âœ… Found existing training logs!")
        print(f"   ğŸ“Š Previous steps: {len(temp_logger.training_data['step_metrics'])}")
        print(f"   ğŸ“… Previous epochs: {len(temp_logger.training_data['epoch_summaries'])}")
        print(f"   ğŸ”„ Training sessions: {len(temp_logger.training_data['training_sessions'])}")
        print(f"   ğŸ“ Log files: {temp_logger.base_experiment_id}_*.json")
    else:
        print(f"ğŸ†• No existing logs found - will create new experiment")

    # Ask if user wants to resume from checkpoint
    print(f"\nğŸ”„ CHECKPOINT OPTIONS FOR {config.name}:")
    print("1. ğŸ“‹ List all available checkpoints")
    print("2. ğŸ”„ Resume from checkpoint (composite logging will continue existing logs)")
    print("3. ğŸ†• Start fresh training (new logs)")

    while True:
        try:
            choice = input(f"\nSelect option (1-3): ").strip()
            if choice == '1':
                list_all_checkpoints()
                continue
            elif choice == '2':
                resume_from_checkpoint = True
                break
            elif choice == '3':
                resume_from_checkpoint = False
                break
            else:
                print("Please enter 1, 2, or 3")
        except KeyboardInterrupt:
            print("\nExiting...")
            exit()

    # Ask for number of epochs
    while True:
        try:
            epochs_input = input(f"\nNumber of training epochs (default: 50): ").strip()
            num_epochs = int(epochs_input) if epochs_input else 50
            if num_epochs > 0:
                break
            print("Please enter a positive number")
        except ValueError:
            print("Please enter a positive number")
        except KeyboardInterrupt:
            print("\nExiting...")
            exit()

    # =============================================================================
    # CONFIGURATION SUMMARY
    # =============================================================================

    print(f"\nâœ… Training Configuration:")
    print(f"   ğŸ¯ Dataset: {config.name}")
    print(f"   ğŸ”¬ Experiment: {experiment_name or 'Default naming'}")
    print(f"   ğŸ“… Epochs: {num_epochs}")
    print(f"   ğŸ”„ Resume: {'Yes (composite logging will continue existing logs)' if resume_from_checkpoint else 'Fresh start'}")
    print(f"   ğŸ“Š Metrics Logging: Composite (seamless across multiple sessions)")

    if not resume_from_checkpoint:
        print("ğŸ†• Starting fresh training with composite metrics logging...")
    else:
        print("ğŸ”„ Resuming training - composite logger will automatically continue existing logs...")

    # =============================================================================
    # SETUP AND PREPARATION
    # =============================================================================

    # Display enhancement details
    display_enhancement_details(dataset_key)

    print(f"\nğŸš€ Starting enhanced automated workflow with composite metrics logging...")

    # Create enhanced directory structure
    directories = [
        './outputs', './models', './training_logs',
        f'./outputs/{dataset_key}', f'./models/{dataset_key}',
        f'./outputs/{dataset_key}/enhanced', f'./models/{dataset_key}/enhanced',
        f'./models/{dataset_key}/emergency'
    ]

    for directory in directories:
        os.makedirs(directory, exist_ok=True)

    print(f"ğŸ“ Created enhanced directory structure for {config.name}")
    print(f"ğŸ“Š Composite metrics logging directory: ./training_logs/")

    # Load dataset
    print(f"\nğŸ“¥ Loading {config.name} dataset...")
    transform = get_transforms(dataset_key)
    train_dataset = get_dataset(dataset_key, transform)

    print(f"âœ… Dataset loaded successfully!")
    print(f"   ğŸ“Š Training samples: {len(train_dataset):,}")

    # =============================================================================
    # START TRAINING
    # =============================================================================

    # Start enhanced training with composite metrics logging
    print(f"\nğŸ¯ Beginning enhanced training with composite metrics logging...")
    print(f"âš ï¸  All training metrics will be seamlessly appended to existing logs")
    print(f"ğŸ“Š Multi-session tracking with continuous log appending")
    print(f"ğŸ’¡ Press Ctrl+C anytime for graceful shutdown")

    # Countdown to training start
    for i in range(3, 0, -1):
        print(f"ğŸš€ Starting training in {i}...", end='\r', flush=True)
        time.sleep(1)
    print("ğŸš€ Starting training now! " + " " * 20)

    # Start enhanced training with composite metrics logging
    ema_generator, critic = train_enhanced_gan_with_resume_modified(
        dataset_key, config, resume_from_checkpoint, num_epochs, experiment_name
    )

    # =============================================================================
    # POST-TRAINING ACTIVITIES
    # =============================================================================

    print(f"\nğŸ¨ Generating enhanced final examples for {config.name}...")

    # Generate samples for all classes
    print("ğŸ­ Creating class-specific samples...")
    all_classes = list(range(config.num_classes))
    generate_enhanced_specific_classes(ema_generator, config, dataset_key, all_classes, num_each=6)

    # Show enhanced interpolation
    print(f"\nğŸŒˆ Creating enhanced latent space interpolations...")
    if dataset_key == 'mnist':
        interpolations = [(3, 8, "3â†’8"), (0, 6, "0â†’6"), (4, 9, "4â†’9")]
    else:  # cifar10
        interpolations = [(3, 5, "catâ†’dog"), (0, 1, "planeâ†’car"), (2, 7, "birdâ†’horse")]

    for class1, class2, desc in interpolations:
        enhanced_interpolate_latent_space(ema_generator, config, dataset_key, class1, class2, steps=10)

    # =============================================================================
    # FINAL COMPREHENSIVE SUMMARY
    # =============================================================================

    print(f"\n" + "="*100)
    print("ğŸ‰ ENHANCED AUTOMATED WORKFLOW WITH COMPOSITE METRICS LOGGING COMPLETED!")
    print("="*100)

    summary_items = [
        "ğŸ¯ Dataset Processing",
        "ğŸ”„ Advanced Checkpoint Management",
        "ğŸ¤– Model Training with Auto-Save",
        "ğŸ›¡ï¸  Graceful Interrupt Handling",
        "ğŸ“Š Composite Metrics Logging",
        "ğŸ”„ Seamless Resume Logging",
        "ğŸ¨ Sample Generation",
        "ğŸŒˆ Interpolation Creation",
        "ğŸ“‹ Multi-Session Analytics Export",
        "ğŸ’¾ Complete State Preservation"
    ]

    print("ğŸ“‹ WORKFLOW COMPLETION SUMMARY:")
    for item in summary_items:
        print(f"   âœ… {item}")

    print(f"\nğŸ“Š FINAL TRAINING STATISTICS:")
    print(f"   ğŸ¯ Dataset: {config.name}")
    print(f"   ğŸ“ˆ Final EMA Quality Score: {ema_generator.quality_score:.4f}")
    print(f"   ğŸ–¥ï¸  Training Device: {device}")
    print(f"   ğŸ”¢ Total Parameters: {sum(p.numel() for p in ema_generator.generator.parameters()):,}")

    print(f"\nğŸ“ All enhanced outputs saved to:")
    print(f"   ğŸ–¼ï¸  Enhanced Images: ./outputs/{dataset_key}/enhanced/")
    print(f"   ğŸ¤– Enhanced Models: ./models/{dataset_key}/enhanced/")
    print(f"   ğŸ“¦ Regular Checkpoints: ./models/{dataset_key}_enhanced_epoch_*.pth")
    print(f"   ğŸš¨ Emergency Checkpoints: ./models/{dataset_key}/emergency/")
    print(f"   ğŸ“Š ğŸ“Š COMPOSITE METRICS LOGS: ./training_logs/")
    if TENSORBOARD_AVAILABLE:
        print(f"   ğŸ“ˆ TensorBoard Logs: ./runs/{dataset_key}_enhanced_gan*/")

    print(f"\nğŸ“Š ğŸ“Š COMPOSITE METRICS LOGGING FILES:")
    print(f"   ğŸ“„ Step-by-step metrics: {dataset_key}_*_step_metrics.json")
    print(f"   ğŸ“‹ Epoch summaries: {dataset_key}_*_epoch_summaries.json")
    print(f"   ğŸ“– Complete training log: {dataset_key}_*_complete_training_log.json")
    print(f"   ğŸ”„ Multi-session support: Seamlessly appends across resume sessions")
    print(f"   ğŸ’¡ Use these JSON files for detailed analysis and visualization")

    print(f"\nğŸš€ Enhanced Features Successfully Implemented:")
    enhancements = [
        "WGAN-GP Loss - Better convergence and stability",
        "EMA Generator - Improved sample quality",
        "Enhanced Architecture - Better gradient flow",
        "Spectral Normalization - Stable discriminator training",
        "Progressive LR Scheduling - Adaptive learning",
        "Advanced Monitoring - Real-time health tracking",
        "Live Progress Bars - Enhanced user experience",
        "Terminal Streaming - Detailed step analysis",
        "Checkpoint Resume - Continue from any saved state",
        "Auto-Save Every 5 Epochs - Frequent progress saves",
        "Graceful Interrupt (Ctrl+C) - Safe shutdown",
        "Emergency Error Recovery - Crash-proof training",
        "ğŸ†• Composite Metrics Logging - Multi-session support",
        "ğŸ†• Seamless Resume Logging - Continues existing logs",
        "ğŸ†• Session Tracking - Complete training history",
        "ğŸ†• Continuous Data Appending - No data loss on resume"
    ]

    for i, enhancement in enumerate(enhancements, 1):
        print(f"   âœ… {i:2d}. {enhancement}")

    print(f"\nğŸ’¡ Enhanced Next Steps:")
    next_steps = [
        f"Load final model: ./models/{dataset_key}/enhanced/final_enhanced_model.pth",
        f"Analyze composite training metrics: Check ./training_logs/ for JSON files",
        f"Visualize multi-session progress: Use composite analysis tools",
        f"Study session progression: Review complete_training_log.json",
        f"Compare session performance: Analyze session-specific data",
        f"Resume from any checkpoint: All logs will be continuous",
        f"Emergency recovery: Check ./models/{dataset_key}/emergency/ if needed"
    ]

    for step in next_steps:
        print(f"   ğŸ’¡ {step}")

    print(f"\nğŸ“Š ğŸ“Š COMPOSITE METRICS ANALYSIS EXAMPLES:")
    print(f"   ğŸ“ˆ Python: from composite_enhanced_metrics_logger import analyze_composite_training_metrics")
    print(f"   ğŸ“Š Analysis: analyze_composite_training_metrics('./training_logs/{dataset_key}_*_complete_training_log.json')")
    print(f"   ğŸ¨ Multi-session plots: Session boundaries and progression visualization")
    print(f"   ğŸ“‹ Session comparison: Compare performance across training sessions")

    if TENSORBOARD_AVAILABLE:
        print(f"\nğŸ“ˆ View enhanced training metrics:")
        print(f"   tensorboard --logdir ./runs/")
        print(f"   Then open: http://localhost:6006")

    print("="*100)
    print("ğŸŠ Thank you for using Enhanced Multi-Dataset DCGAN with Composite Metrics Logging!")
    print("ğŸ“Š All your training data across multiple sessions is now seamlessly logged!")
    print("ğŸ”„ Resume training anytime - your logs will continue automatically!")

    try:
        print("\nâ³ Keeping plots alive... (Press Ctrl+C to exit)")
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Graceful exit completed! Your composite metrics are saved!")
        plt.close('all')

# =============================================================================
# COMMAND LINE INTERFACE WITH COMPOSITE LOGGING SUPPORT
# =============================================================================

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        command = sys.argv[1].lower()

        if command == "analyze" and len(sys.argv) > 2:
            log_file_path = sys.argv[2]
            # Use the composite analysis function
            analyze_composite_training_metrics(log_file_path)

        elif command == "list":
            list_all_checkpoints()

        elif command == "resume" and len(sys.argv) > 2:
            dataset_key = sys.argv[2].lower()
            if dataset_key in ['mnist', 'cifar10']:
                checkpoint_path = quick_resume_latest(dataset_key)
                if checkpoint_path:
                    print(f"ğŸš€ Quick resuming from: {os.path.basename(checkpoint_path)}")
                    print(f"ğŸ“Š Composite logging will automatically continue existing logs")
                    main_with_checkpoint_support_enhanced()
                else:
                    print("âŒ No checkpoints found for quick resume")
            else:
                print("âŒ Invalid dataset. Use 'mnist' or 'cifar10'")

        elif command == "sessions" and len(sys.argv) > 2:
            # New command to show session information
            dataset_key = sys.argv[2].lower()
            if dataset_key in ['mnist', 'cifar10']:
                temp_logger = CompositeEnhancedMetricsLogger(dataset_key)
                if temp_logger.existing_log_found:
                    print(f"\nğŸ”„ TRAINING SESSIONS FOR {dataset_key.upper()}:")
                    print("=" * 50)
                    print(temp_logger.get_session_summary())

                    # Show log file sizes
                    if temp_logger.step_metrics_file.exists():
                        size_mb = temp_logger.step_metrics_file.stat().st_size / (1024*1024)
                        print(f"ğŸ“„ Step metrics file: {size_mb:.1f} MB")
                    if temp_logger.epoch_summaries_file.exists():
                        size_mb = temp_logger.epoch_summaries_file.stat().st_size / (1024*1024)
                        print(f"ğŸ“‹ Epoch summaries file: {size_mb:.1f} MB")
                    if temp_logger.full_training_log.exists():
                        size_mb = temp_logger.full_training_log.stat().st_size / (1024*1024)
                        print(f"ğŸ“– Complete log file: {size_mb:.1f} MB")
                else:
                    print(f"âŒ No training sessions found for {dataset_key}")
            else:
                print("âŒ Invalid dataset. Use 'mnist' or 'cifar10'")

        elif command == "help":
            print("\nğŸ¨ Enhanced DCGAN with Composite Metrics Logging - Command Line Help")
            print("=" * 80)
            print("Usage:")
            print("  python script.py                              # Interactive mode")
            print("  python script.py analyze path/to/log.json     # Analyze composite training metrics")
            print("  python script.py list                         # List all checkpoints")
            print("  python script.py resume mnist                 # Quick resume latest MNIST")
            print("  python script.py resume cifar10               # Quick resume latest CIFAR-10")
            print("  python script.py sessions mnist               # Show training sessions for MNIST")
            print("  python script.py sessions cifar10             # Show training sessions for CIFAR-10")
            print("  python script.py help                         # Show this help")
            print("\nğŸ“Š ğŸ“Š NEW: Composite Metrics Logging Features:")
            print("  â€¢ Seamless resume logging across multiple training sessions")
            print("  â€¢ Automatic detection and continuation of existing logs")
            print("  â€¢ Multi-session tracking with session boundaries")
            print("  â€¢ Continuous data appending without data loss")
            print("  â€¢ Session-specific analytics and comparisons")
            print("  â€¢ Complete training history preservation")
            print("\nğŸ“ Composite Metrics Files Generated:")
            print("  â€¢ {dataset}_*_step_metrics.json               # All step data across sessions")
            print("  â€¢ {dataset}_*_epoch_summaries.json            # Epoch aggregations across sessions")
            print("  â€¢ {dataset}_*_complete_training_log.json      # Full session log with history")
            print("\nğŸ”„ Resume Behavior:")
            print("  â€¢ When resuming training, logs automatically continue")
            print("  â€¢ Session boundaries are tracked and marked")
            print("  â€¢ No duplicate or lost data across resume sessions")
            print("  â€¢ Complete training timeline preserved")
            print("\nğŸ”¬ Composite Analysis Examples:")
            print("  python script.py analyze ./training_logs/mnist_*_complete_training_log.json")
            print("  python script.py sessions mnist  # Show all training sessions")
            print("  # This will create multi-session visualizations with session boundaries")
            print("=" * 80)
        else:
            print("âŒ Unknown command. Use 'help' for usage information.")
    else:
        # Interactive mode with composite metrics logging
        main_with_checkpoint_support_enhanced()